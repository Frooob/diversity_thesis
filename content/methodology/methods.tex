\section{Data Collection}


\subsection{Subjects}
The data were collected from 5 healthy volunteers aged between 23 and 33 years (4 males and 1 female). All subjects provided informed consent before the experiment, with the study protocol having been reviewed and approved by the Ethics Committee of the Graduate School of Informatics at Kyoto University (approval no.: KUIS{-}EAR{-}2017{-}002). The subjects had normal or corrected-to-normal vision.

\subsection{Experimental Setup}
% Description of Deeprecon Dataset (quote Horikawa/Kamitani)
The experimental design for the current study was the same as that of Shen et al.\ \cite{shenDeepImageReconstruction2019} However, only the data from the image presentation experiment are used in this study. The public dataset of Shen et al.\ \cite{ds001506:1.3.1} contains three of the five subjects that are used in the current study.  Two types of image presentation experiments were performed: training and testing. All stimuli were rear projected onto a screen in an MRI scanner bore with a luminance-calibrated liquid crystal display projector. The stimulus images were displayed at the screen center with a size of 12° $\times$ 12° of visual angle on a gray background. We asked subjects to fixate on the center of the images cued by a circle of 0.3° $\times$ 0.3° of visual angle. Each subject used a custom-molded bite bar and/or personalized headcase from CaseForge Inc.\ to reduce head motion during fMRI data collection. Multiple scanning sessions were performed to collect data for each subject. Multiple scanning sessions were performed to collect data for each subject. Each consecutive session took a maximum of 2 hours, with each run taking 6 to 8 min. The subjects were free to rest adequately between runs or to terminate the experiment at any time. Each image was flashed for 8 seconds at a frequency of 2 Hz, after which the average brain activation during the 8 seconds was calculated. This procedure was repeated 5 times for each image. The experimental design is described in detail in the original paper by Shen et al.\cite{shenDeepImageReconstruction2019}. There were three different categories of images shown to the subjects: natural training images, natural test images and artificial shapes test images. The selection of natural images was the same as that used in the experiment by Horikawa et al.\ \cite{horikawaGenericDecodingSeen2017}. The dataset consists of 1200 images that were extracted from the ImageNet database\cite{dengImageNetLargescaleHierarchical2009} for the training dataset and 50 images for the test dataset. It was ensured that the overlap between the categories of the training and test dataset was minimized (thus the Deeprecon dataset has a considerably lower overlap between the training and test data than, for example, the popular NSD dataset\cite{allenMassive7TFMRI2022,shirakawaSpuriousReconstructionBrain2024}). The Artificial Shapes test dataset consists of 40 images in which 5 different artificial shapes (the same shapes of the study by Miyawaki et al.\ \cite{miyawakiVisualImageReconstruction2008}) are depicted in 4 different colors on a uniform gray background. Graphic~\ref{fig:datasets_train_art} shows a sample of the Train/Test and Artificial Shapes images. Since the brain-diffuser algorithm requires descriptions (captions) of the images in addition to the images, Shirakawa et al.\ \cite{shirakawaSpuriousReconstructionBrain2024} used crowd-sourcing to collect 5 short captions for each image in the training and test datasets for the natural images. In addition, 5 captions were also created for each image in the test dataset of artificial shapes for this work.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/datasets_train_art_images.jpeg}
    \caption{A nice image}\label{fig:datasets_train_art}
\end{figure}

\subsection{MRI Data}
% Hier kann ich evtl die Beschreibung aus dem Kamitani Lab nutzen
\subsubsection{MRI data acquisition}
The fMRI data was collected using a 3.0-T Siemens MAGNETOM Verio scanner at the Kyoto University Institute for the Future of Human Society (formerly, Kokoro Research Center). An interleaved T2*-weighted gradient-echo echo-planar imaging scan was performed to acquire functional images covering the entire brain [repetition time (TR), 2000 ms; echo time (TE), 43 ms; flip angle, 80°; field of view (FOV), 192 $\times$ 192 mm2; voxel size, 2 $\times$ 2 $\times$ 2 mm3; slice gap, 0 mm; number of slices, 76; and multiband factor, 4]. T1-weighted (T1w) magnetization-prepared rapid acquisition gradient-echo fine-structural images of the entire head were also acquired [TR, 2250 ms; TE, 3.06 ms; inversion time (TI), 900 ms; flip angle, 9°; FOV, 256 $\times$ 256 mm2; and voxel size, 1.0 $\times$ 1.0 $\times$ 1.0 mm3].

\subsubsection{MRI data preprocessing}
MRI data preprocessing was performed through the pipeline of FMRIPREP (version 1.2.1). For the functional data of each run, a BOLD reference image was first estimated using a custom methodology of FMRIPREP.\@ Then, data were motion-corrected using MCFLIRT from FSL (version 5.0.9) and slice time-corrected using 3dTshift from AFNI (version 16.2.07), based on this BOLD reference image. Next, the corresponding T1w image was coregistered using boundary-based registration implemented by bbregister from FreeSurfer (version 6.0.1). The coregistered BOLD time-series were then resampled onto their original space (2 $\times$ 2 $\times$ 2 mm3 voxels) with antsApplyTransforms from ANTs (version 2.1.0) using Lanczos interpolation. After obtaining the resampled BOLD time series, the time series was first shifted by 4 s (two volumes) to compensate for hemodynamic delays, and then nuisance parameters from each voxel’s time series of each run were regressed out, including a constant baseline, a linear trend, and temporal components proportional to the six motion parameters calculated during the motion correction procedure (three rotations and three translations). Single-trial data samples were created by reducing extreme values (beyond ±3 SD for each run) of the time series and averaged within each 8-s trial (four volumes).

\subsubsection{Brain regions of interest}
According to standard retinotopy experiments\cite{engelFMRIHumanVisual1994,serenoBordersMultipleVisual1995}, V1, V2, V3, and V4 were delineated. The LOC, FFA, and PPA were identified using conventional functional localizers\cite{kanwisherFusiformFaceArea1997,epsteinCorticalRepresentationLocal1998,kourtziCorticalRegionsInvolved2000}. The higher visual cortex (HVC) region was defined by manually delineating a contiguous region that covered the LOC, FFA, and PPA on the flattened cortical surfaces. The VC was defined by combining V1 to V4 and the HVC.\@ For all the upcoming experiments, the whole VC was used for the reconstruction. After pre-processing the number of Voxels in the VC that can be used for the reconstruction task ranged from 13,135 to 16,667.

% {'S1': 13596, 'S2': 14597, 'S3': 13135, 'S4': 16067, 'S5': 13149, 'S6': 15316}

% Images were collected from an online image database ImageNet31 (2011, fall release), an image database where images are grouped according to the hierarchy in WordNet38. We selected 200 representative object categories (synsets) as stimuli in the visual image presentation experiment. After excluding images with a width or height o100 pixels or aspect ratio 41.5 or o2/3, all remaining images in ImageNet were cropped to the centre.

% Description of fmri-data 
%% Number of subjects
%% Number of runs per image
%% Experimental Design how the participants are shown the images
%% ROI Experiments
%% Chosen ROIs

\section{Reconstruction-Algorithms}

% How does the whole reconstruction pipeline look like? Both for ICNN and for brain-diffuser
In this work, the two algorithms brain-diffuser\cite{ozcelikNaturalSceneReconstruction2023} and ICNN\cite{shenDeepImageReconstruction2019}, that are described in more detail above, are used. The parameters for the algorithms that are used for this work are stated below. The entire codebase, which this work is based on, is publicly available on github\cite{mildenbergerKamitaniLabBrain_diffuser}, the associated README describes how to work with the codebase. 

\subsection{Brain-Diffuser}
% Brain-diffuser
Unless stated otherwise, the default settings described in the original publication are used for the brain-diffuser algorithm. For this purpose, the publicly available codebase of the author\cite{ozcelikOzcelikfuBraindiffuser2025} was used and adapted for the purposes of this work so that the Deeprecon data set can also be used.
First, the mean value of the measured brain activity of the 5 image presentations was calculated for each of the 1200 images in the training data set. This data serves as input for both the brain-diffuser algorithm and the ICNN algorithm. It should be noted that a better performance in the reconstruction could possibly result if the 5 presentations of each stimulus were included individually in the training data (i.e.\ a total of 6000 training samples would be available). However, since the main aim of this work is to investigate the relative difference between different test conditions and not to achieve the highest possible absolute performance, it was decided to calculate the mean value of the activation in order to save computing resources.

%% Regression
\subsubsection{Brain-Diffuser Decoder}
As is usual in reconstruction algorithms, the brain-diffuser algorithm first trains translators that map the brain activity into a latent space, which can then be used for reconstruction. There are three different translators that are trained for the brain-diffuser: one that maps the brain activity to the latent space of a VDVAE, one for clip text features and one for the clip vision features. The activations of all voxels recorded during the presentation of an image serve as predictor values.  

The criteria to be predicted are the embeddings of the stimuli in the respective latent spaces of the three components. These true features must first be extracted from the simulus material. For the VDVAE, the model of Child et al.\ \cite{childVeryDeepVAEs2020} is used, which was pre-trained with the ImageNet data set (64 $\times$ 64 pixels). As with the original brain-diffuser, only the first 31 layers of the total 75 layers of the VDVAE are used here, as adding further layers would not improve the prediction, but would unnecessarily increase the size of the translator module. The 31 VDVAE layers of the embedded training images were concatenated so that the Translator has to learn to predict a 91168-dimensional vector. 
Since CLIP\cite{radfordLearningTransferableVisual2021} is a multimodal model, it can be used to extract both the true features from the presented images (clipvision) and the associated captions (cliptext). For this work, a pre-trained CLIP network is used which is based on the Transformer architecture (ViT-L/14). For clipvision, the images are embedded in a space with 257 $\times$ 768 values (256 image patches and a final vector for the entire image). For cliptext, the captions are embedded in a space with 77 $\times$ 768 values (76 tokens and a final vector for the entire caption). As 5 different captions were generated for each image, the average value of the latent space from all 5 captions is used as the true feature for cliptext.

% Ridge Regression

For the brain-diffuser algorithm, the linear ridge regression\cite{hoerlRidgeRegressionBiased1970} is used for all translators. The ridge regression extends the linear regression by adding a penalty term, which is controlled via the coefficient $\lambda$ and regularizes (i.e.\ shrinks) the estimated coefficients. The higher $\lambda$, the more the parameters are shrunk. Especially in cases where a high dependency between individual predictors is to be expected, more robust parameters can be estimated in that way. Since the activation of neighboring voxels is strongly correlated, ridge regression is therefore helpful in our case.

The ridge regression minimizes the following objective function:
\[
\min_{\beta} \sum_{i=1}^{n} {(Y_i - X_i \beta)}^2 + \lambda \sum_{j=1}^{p} \beta_j^2
\]
where:
\begin{itemize}
    \item \(Y_i\) is the extracted feature-vector (VDAVE, cliptext or clipvision) for the \(i\)-train image or caption,
    \item \(X_i\) is the vector of recorded voxels in response to the \(i\)-th picture,
    \item \(\beta\) is the vector of coefficients to be estimated,
    \item \(\lambda\) is the regularization parameter controlling the strength of the penalty.
\end{itemize}

For the prediction of the latent space of the VDAVE, $\lambda_{VDVAE}=50000$ is set, for cliptext $\lambda_{cliptext}=100000$ and for clipvision $\lambda_{clipvision}=60000$. 

% \begin{equation}
%     \hat{\beta} = \underset{\beta}{\arg\min} \ \frac{1}{n} \sum_{i=1}^{n} (y_i - X_i \beta)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
% \end{equation}
        
% where:
% \begin{itemize}
%     \item \(\hat{\beta}\) is the estimated vector of coefficients,
%     \item \(y_i\) is the observed response for the \(i\)-th observation,
%     \item \(X_i\) is the vector of predictor values for the \(i\)-th observation,
%     \item \(\beta\) is the vector of coefficients to be estimated,
%     \item \(\lambda\) is the regularization parameter controlling the strength of the penalty.
% \end{itemize}

\subsubsection{Brain-Diffuser Reconstruction}
% % Reconstruction
After the brain activity for the test datasets has been mapped to the latent spaces of VDVAE, cliptext and clipvision using the Translators, these decoded features can be fed into the generator in the second step, which reconstructs the images.

% Versatile Diffusion
The Versatile Diffusion Algorithm\cite{xuVersatileDiffusionText2024} is used as the reconstruction module in the brain diffuser. The model used here was pre-trained on the Laion2B-en\cite{schuhmannLAION400MOpenDataset2021} dataset. During the reconstruction itself, no parameters are trained, only the pre-trained parameters of the models are used. First, the generator part of the VDVAE is used to generate a first (blurry) approximation of the final reconstructed image from the predicted features. The VDVAE generates images with a size of 64 $\times$ 64 pixels, which are first scaled up to a size of 512 $\times$ 512 to serve as a prior for the Versatile Diffusion Generator. The image generated by the VDVAE adds noise to 37 of the 50 forward diffusion steps. In the backward-denoising process, the generated image is conditioned by both the predicted clipvision (relative strength 0.6) and cliptext (relative strength 0.4) features. The generated images have a size of 512 $\times$ 512 pixels. 

% ICNN
\subsection{ICNN}
Similarly to the brian-diffuser, the ICNN\cite{shenDeepImageReconstruction2019} algorithm also consists of a translator and a reconstruction module. 
\subsubsection{ICNN Decoder}
The averaged activations of all voxels are again the predictors for the translator. The criteria are the features of a VGG19\cite{simonyanVeryDeepConvolutional2014} model, with which the stimuli (resized to 224$\times$224 pixels) are encoded. The VGG19 implementation of Pytorch was used, whereby the weights of the caffe implementation\cite{ModelZoo} (pre-trained on ImageNet) were used to create the same conditions as in the original implementation by Shen et al.\ \cite{shenDeepImageReconstruction2019}. The features from the 19 layers of the VGG were then reshaped into one-dimensional vectors. The size of the respective vectors for all the 19 layers can be found in the appendix. As with the brain diffuser, a ridge regression is then also calculated, which predicts the features from the voxel data, the ridge parameter is set to $\lambda=50000$.

\subsubsection{ICNN Reconstruction}
Once the brain activity of the test data sets has been mapped to the feature space of the VGG19 layer using the learned regression, the reconstruction of the images with the ICNN algorithm\cite{shenDeepImageReconstruction2019} begins. The implementation of the bdpy package\cite{KamitaniLabBdpy2024} is used for this. The SGD optimizer is used and the number of iterations in which the reconstructed image is adapted to the predicted features is set to 500. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/baseline_qual_true_recon_test.JPEG}
    \caption{A nice image}\label{fig:baselinetruerecon}
\end{figure}
To get an impression of the theoretical upper limit for the reconstruction performance of the two algorithms, the true features can be used for reconstruction. Here, features extracted directly from the stimulus material are fed into the reconstruction module. This means that no brain data is used for the true feature reconstruction. True feature reconstruction therefore offers the opportunity to see how good the reconstruction would be if the latent features could be perfectly predicted from the brain data. Figure~\ref{fig:baselinetruerecon} shows the true feature reconstruction for 10 of the 50 images of the test data set with natural images, in Figure~\ref{fig:baselinetruerecon_art} the true feature reconstruction is displayed for 10 of the 40 images of the artificial shapes data set. The first line shows the images that were presented to the test subjects during the experiment. In addition to the final result of the brain-diffuser algorithm (shown in the third row), the intermediate result of the VDVAE is also shown in the second row. It can be seen that the VDVAE algorithm shows a very good reconstruction performance for all images and is usually only a slightly blurred image of the ground truth. This is to be expected, as the VDVAE is precisely designed to reconstruct original images from a reduced feature space with as few errors as possible. The final result of the Versatile Diffusion of the brain-diffuser algorithm also shows good performance in principle, but it becomes apparent that details from the image are sometimes displayed incorrectly or are hallucinated. For example, the bat looks more like a bird, the fish swims in the wrong direction, additional writing is visible on the sphere or an additional circle has appeared around the blue cross in the artificial shape. The iCNN algorithm is closer to the result of the VDVAE.\ Most of the images can be displayed correctly for the main part. In particular, finer details and the coloring of backgrounds (clearly visible in the artificial shapes) are sometimes incorrectly reconstructed by the ICNN.\ In summary, the true feature reconstruction shows that both algorithms are basically capable of reconstructing images from a feature space, even if both algorithms have some weaknesses.
% True-Feature Reconstruction


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/baseline_qual_true_recon_art.JPEG}
    \caption{A nice image}\label{fig:baselinetruerecon_art}
\end{figure}

% Small Evaluation of the True-Feature Reconstruction

\section{Evaluation Metrics}
%% Regression Criteria
% Profile Correlation
% Pattern Correlation
% Pairwise Identification Accuracy

It is possible to evaluate both stages of the reconstruction algorithms (the decoding and the reconstruction). The results of the regressions are used to evaluate the decoding part of the algorithms and the final reconstructed images are examined to evaluate the reconstruction part of the algorithms.

\subsection{Measuring Decoding Performance}
To evaluate the decoding performance, different criteria can be used. Formally, the result of the regression can be described as follows. Let:
\begin{itemize}
    \item \( \hat{Y} \) be the predicted features matrix of shape \( n \times m \).
    \item \( Y \) be the true features matrix of shape \( n \times m \).
    \item \( n \) be the number of samples.
    \item \( m \) be the number of features.
\end{itemize}

The profile correlation and pattern correlation can now be used to assess how similar the features predicted from the brain activity and the true features are\cite{horikawaGenericDecodingSeen2017}. 

\[
\text{ProfileCorrelation}(i) = \text{corr}\left( \mathbf{Y}_{\text{true}, i},\ \mathbf{Y}_{\text{pred}, i} \right) \quad \text{for } i = 1, 2, \ldots, m
\]

\[
\text{PatternCorrelation}(j) = \text{corr}\left( \mathbf{Y}_{\text{true}}^{(j)},\ \mathbf{Y}_{\text{pred}}^{(j)} \right) \quad \text{for } j = 1, 2, \ldots, n
\]

Profile Correlation focuses more strongly on how well individual prediction units (e.g.\ a low-level layer in VGG19) can be predicted. Pattern Correlation focuses on how well the entire feature pattern can be predicted for a single image. 
In addition to Profile and Pattern Correlation, the Pairwise Identification Accuracy can also be calculated. The Pairwise Identification Accuracy is a metric frequently used to determine the feature prediction in reconstruction algorithms\cite{shirakawaSpuriousReconstructionBrain2024}. 
The pairwise identification accuracy measures whether a predicted feature vector is more similar to the true feature than the other predicted vectors. For a Feature Vector $i$, it is defined as:

% für alle
% \[
% \text{PairwiseIDAcc} = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{1}{n-1} \sum_{\substack{j=1 \\ j \neq i}}^{n} \mathbb{I} \left( \left\| \mathbf{Y}_{\text{pred},i} - \mathbf{Y}_{\text{true},i} \right\| < \left\| \mathbf{Y}_{\text{pred},j} - \mathbf{Y}_{\text{true},i} \right\| \right) \right)
% \]


\[ % Only for image i
\text{PairwiseIDAcc}_i = \frac{1}{n-1} \sum_{\substack{j=1 \\ j \neq i}}^{n} \mathbb{I} \left(  \text{dist}\left(\mathbf{Y}_{\text{pred},i}, \mathbf{Y}_{\text{true},i}\right)  < \text{dist}\left(\mathbf{Y}_{\text{pred},j}, \mathbf{Y}_{\text{true},i} \right) \right)
\]

\begin{itemize}
    \item \( \mathbf{Y}_{\text{true},i} \) represents the true feature vector for the \( i \)-th sample.
    \item \( \mathbf{Y}_{\text{pred},i} \) represents the predicted feature vector for the \( i \)-th sample.
    \item \( \text{sim}\left( \cdot, \cdot \right) \) denotes an arbitrary distance metric for vectors.
    \item \( \mathbb{I}(\cdot) \) is the indicator function, which equals 1 if the condition inside is true and 0 otherwise.
\end{itemize}

The distance metric can be freely chosen, however in this work, the cosine distance is computed as distance metric, since it isn't as much affected by the number of dimensions as the euclidean distance. The cosine distance between to vectors \textbf{a} and \textbf{b} is calculated as follows:

\[
\text{cosine\_dist}(\mathbf{a}, \mathbf{b}) = 1 - \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \, \|\mathbf{b}\|}
\]

If a feature vector cannot be predicted from brain activity, one would expect the pairwise identification accuracy to be 0.5, since it would be a matter of chance whether any other vector would be more or less similar to the true feature than the actually predicted feature vector. Due to the better interpretability of the Pairwise Identification Accuracy compared to the Pattern and Profile Correlation, the latter is reported and interpreted. The values for Pattern and Profile Correlation can be found in the appendix, unless they are directly necessary to make results more explicit (as for example when the Pairwise Identification Accuracy is not able to show the differences between individual experimental conditions due to ceiling effects).

In order to compute the Pairwise Identification Accuracy, for the different translators of the ICNN and the brain-diffuser, the corresponding multidimensional feature vectors are reshaped to a one-dimensional vector to be able to compute the vector similarity between layers (for example the convolutional layers of the VGG19 are multidimensional)

%% Recsontruction Criteria
% Pixel-Correlation (Low-level)
% Dreamsim (Mid-Level)
% Clipvision (High-level)

\subsection{Measuring Reconstruction Performance}

The final results of the reconstruction algorithms can in turn be analysed both qualitatively and quantitatively. Different metrics can be used to compare the ground truth images and the reconstructed images on a quantitative level\cite{ozcelikNaturalSceneReconstruction2023}. On the one hand, the metrics can focus on the low-level features of an image (e.g.\ structure and colour); on the other hand, there are metrics that compare the high-level features of the images (i.e.\ the semantic meaning). For an appropriate judgement on the overall quality of a reconstruction, it is important to consider both the low-level and high-level features (a reconstructed image that has the correct outlines but completely ignores the semantic concepts is just as bad as an image in which the meaning has been correctly captured but the position and position of the objects in the image are incorrectly represented).

In this work, the PixelCorrelation is used as a low-level metric\cite{shenDeepImageReconstruction2019,ozcelikNaturalSceneReconstruction2023}. In this method, the reconstructed images and the ground truth image are first reshaped into a one-dimensional vector and the correlation between the two is then calculated. The expected value for random reconstructions would be $r=0$.

To measure the high-level differences between reconstructed and ground truth images, they are first embedded in a semantic feature space using a ClipVision encoder\cite{radfordLearningTransferableVisual2021}. The correlation between the embedded features of the ground truth images and the reconstruction images is then compared and, as with pairwise identification accuracy, it is determined how often the congruent pairs of reconstructed image and ground truth have a higher correlation of clipvision features than the incongruent pairs between any other reconstructed image and the ground truth. As with the Identification Accuracy, the random value here is 0.5. The two metrics were used to match the author's metrics from brain-diffuse\cite{ozcelikNaturalSceneReconstruction2023} to ensure comparability with these results. The metric will be called clip-accuracy from now on.

In addition to the low-level and high-level differences, a third quantiative metric is used in this work to test the reconstruction performance. Since the natural human judgement about the similarity of images is based on several visual attributes that integrate low-level features with high-level features\cite{sundaramWhenDoesPerceptual2024}, Dreamsim\cite{fuDreamSimLearningNew2023} was developed to get closer to the natural judgement about the similarity of images. DreamSim combines various image features and has been fine-tuned with human judgement to better estimate the ``mid-level'' similarity between images. To calculate the similarity between two images, the cosine-similarity (i.e.\ 1\@ $-$\@ cosine distance) between the DreamSim embedding vector of the ground truth and the predicted image is calculated. In a random reconstruction, the cosine similarity would be 0 (the feature vectors would be orthogonal to each other).

Since even the DreamSim metric is not perfect for comparing the similarity of images, and since judgments of quality may vary from person to person, the differences are also described qualitatively. Individual examples of the actual reconstructed images are shown and compared to the ground truth images for each test condition tested in this paper.  As not all reconstructed images can be shown in this paper, additional reconstructed images are presented in the appendix.

\section{Baseline Results}
In order to validate the reconstruction algorithms used in this work, the results of the two algorithms are presented below. No other changes were made to the input datasets or the method, so the results can be used as a baseline for comparison with the further results of the following experiments.

\subsection{Decoding Results}

Figure~\ref{fig:baselinetranslatortest} shows the results for the translator with the natural test images and Figure~\ref{fig:baselinetranslatorart} shows the results for the translator with the artificial test images. The mean values over all 50 (natural test images) or 40 (artificial images) test samples are shown. The error bars are the standard errors. In addition, the results aggregated over all 5 subjects for both types of test images are listed in Table~\ref{tab:baseline_translator}, with the standard deviation in parentheses.
For the natural images, all mean values for all participants are clearly above the random probability of 0.5. Thus, for both the brain-diffuser and ICNN, the translators have the ability to predict the true features of the test images to some extent. The performance is also relatively constant across the different subjects, with the significantly larger differences being found between the different translator modules. In particular, for the cliptext features, the identification accuracy is very high at 0.892 on average and is already close to the upper limit of 1.0. A slightly different picture emerges when the artificial shapes are used as test images and thus the translator has to generalise outside its training domain. All individual values are lower than with the natural test images. ICNN (from 0.831 to 0.782) and VDVAE (from 0.761 to 0.665) show only a comparatively small drop in performance. For the two clip translators, the difference to the natural test images is significantly greater (clipvision falls from 0.777 to 0.54 and cliptext falls from 0.892 to 0.558). The identification accuracy for the clip translator is only just above the chance probability of 0.5 for the artificial test images.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/baseline_translator_test.png}
    \caption{A nice image}\label{fig:baselinetranslatortest}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/baseline_translator_art.png}
    \caption{A nice image}\label{fig:baselinetranslatorart}
\end{figure}

\begin{table}[ht]
    \centering
\input{plots/baseline_translator_table.tex}
\caption{A nice table.}\label{tab:baseline_translator}
\end{table}


\subsection{Reconstruction Results}
Ten of the reconstructed images are shown for the natural test images in Figure~\ref{fig:baselinerecontestqual} and for the artificial shapes in Figure~\ref{fig:baselinereconartqual}. As with the true-feature reconstruction, both the intermediate result of the VDVAE and the overall result are shown here for the brain-diffuser algorithm. The VDVAE and ICNN tend to produce blurred outlines, while the brain-diffuser algorithm usually creates photorealistic objects. For the natural test images, the VDVAE and ICNN usually decode the outlines approximately correctly. For example, the ball produces a round object in front of a neutral background and the goldfish is reconstructed as a red, elongated object in front of a dark background. The VDVAE produced artefacts for the aeroplane, which could be the result of clipping in the prediction (the predicted values of the features are too high or too low and distort the pixel value). The VDVAE and ICNN are also largely able to reconstruct the correct shapes for the artificial shapes. Only the shape X cannot be reconstructed very well. All in all, the results of the VDVAE and ICNN are comparable. The full brain-diffuser algorithm is designed to add semantic information to the results of the VDVAE and incorporate this into a photorealistic image with the versatile diffusion process. In the case of natural test images, it looks as if this is at least partially successful: the images that represent living beings (bat, llama, goldfish or orca) also look as if something alive is to be represented in the reconstructed version (even if the exact animal cannot be recognised). Likewise, the inanimate objects (e.g.\ the hat, the surgical instrument or the umbrella) are also reconstructed in a comparatively neutral way. Even the snowmobile looks like a vehicle (with handlebars and wheels) after reconstruction. Only in the case of the aircraft is the interpretation difficult, but this may also be due to the faulty result of the VDVAE.\@ As the results of the translator already indicated, the final brain-diffuser result for the artificial shapes is far from perfect. Here, too, photorealistic images are usually generated. The shapes are often well adopted by the VDVAE and then converted into a photorealistic image with false details hallucinated by the diffusion algorithm (for example, in the image with the purple cross on a person can be seen). 

The plots in Figure~\ref{fig:baselinerecontestquant} show the results of the quantitative evaluation of the image reconstruction for the natural test images and in Figure~\ref{fig:baselinereconartquan} for the artificial shapes for the individual subjects. As with the translators, the error bars are the standard errors. The three previously described measurement parameters, PixelCorrelation, dreamsim-distance and clip-accuracy, are shown for each of the ICNN and brain-diffuser (bd) algorithms. In Table~\ref{tab:baseline_reconstruction}, the values are again averaged over all subjects (the standard deviation in parentheses). For the natural test images, there is no apparent difference between ICNN and brain-diffuser in terms of dreamsim-distance. In terms of clip accuracy, however, the reconstructions of the brain-diffuser show consistently higher performance than the ICNN.\@ The difference in the pixel correlation is not as visible in the graphic (the clip-accuracy is more sensitive to small differences than the direct PixelCorrelation), but it appears that the ICNN consistently achieves higher results in this low-level metric than the brain-diffuser. As with the translator, the results for the artificial shapes are worse than for the natural images. For the clip accuracy, there is no longer any difference between the two algorithms, and the results are very close to the random probability of 0.5. The ICNN now performs better than the brain-diffuser in terms of both the dreamsim-distance and the pixel correlation. 


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/baseline_qual_recon_test.JPEG}
    \caption{A nice image}\label{fig:baselinerecontestqual}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/baseline_qual_recon_art.JPEG}
    \caption{A nice image}\label{fig:baselinereconartqual}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/baseline_reconstruction_test.png}
    \caption{A nice image}\label{fig:baselinerecontestquant}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/baseline_reconstruction_art.png}
    \caption{A nice image}\label{fig:baselinereconartquan}
\end{figure}

\begin{table}[ht]
    \centering
\input{plots/baseline_reconstruction_table.tex}
\caption{A nice table.}\label{tab:baseline_reconstruction}
\end{table}

\subsection{Conclusion}
The effectiveness of the two algorithms, brain-diffuser and ICNN, could be validated with the Deeprecon data set. It was shown that the decoder modules of the algorithms were able to predict the features from the brain activity well above the random probability. Furthermore, it was shown that the reconstructed images also show strong similarities to the ground-truth images, so it is possible to a certain degree to reconstruct seen images from brain activity using the mentioned algorithms. The reconstructed images also look qualitatively similar to those in the relevant literature (see [shenDeepImageReconstruction2019, shirakawaSpuriousReconstructionBrain2024, ozcelikNaturalSceneReconstruction2023]).
In contrast to the original study, the brain-diffuser was evaluated with the Deeprecon dataset and not the NSD dataset. As already described, there is a partial overlap of semantic concepts between the training and the test dataset in the NSD dataset. Accordingly, the results of the brain-diffuser in our validation are worse than for the original author\cite{ozcelikNaturalSceneReconstruction2023}. With our data, the brain-diffuser algorithm achieves a PixelCorrelation of 0.201 (0.305 for the original authors) and a clip-accuracy of 0.796 (0.925 for the original authors). In particular, the high difference in clip accuracy is to be expected, since the semantic overlap between training and test data in the NSD dataset enables the semantic categories to be learned. Thus, the clip translators do not need to learn a regression on the complete possible clip space, but only needs to perform a classification (generalisation over unknown classes is not fully possible). 
Nevertheless, our results with the Deeprecon data set showed that at least parts of the semantic meaning of the images could be captured (e.g.\ living vs.\ inanimate objects). At least for the natural test data, it has been shown that the ICNN algorithm is better able to reconstruct the low-level structure of an image (difference of PixelCorrelation), while the brain-diffuser is better able to recognise high-level concepts (difference of clip-accuracy). 
The artificial shapes were consistently  reconstructed worse than the natural test images. This indicates the additional difficulty of out-of-distribution generalisation. Compared to brain-diffuser, the ICNN algorithm has lost significantly less in its reconstruction quality. This can be explained by the fact that ICNN is less dependent on learned categories than brain-diffuser and thus generalises more easily. The brain-diffuser shows the significantly poorer generalisation described by Shirakawa et.\ al.\cite{shirakawaSpuriousReconstructionBrain2024} The main cause for this is described as output dimension collapse: the translator modules have not been trained with sufficiently diverse data and therefore map to the semantic space known from the training, in which potential (previously unknown) dimensions in the test data set cannot be mapped. To improve the generalisation of the algorithms, it is therefore necessary to have more diverse data in the training set to enable regression into dimensions not present in the training. In the following, we will thus investigate in different experiments what influence the diversity of the training data has on reconstruction and out-of-distribution generalisation, and how the diversity of existing data sets can presumably be increased. 