\chapter{Background}

Alle wichtigen Teile die nötig sind um die Arbeit im Detail zu verstehen werden im Folgenden erläutert. 

\section{Brain Decoding And Image Reconstruction}


% \cite{grill-spectorHUMANVISUALCORTEX2004} Es ist schon lange bekannt, dass es im visuellen System Neurone gibt, welche von unterschiedlichen Reizen (zum Beispiel der Ausrichtung) von Objekten reagieren. Mithilfe von fMRI können die Regionen auch noninvasiv genauer untersucht werden. Zwei Hauptprinzipien: hierarchische Verarbeitung und funktionale Spezialisierung. 
% Hierarchisch, dass von V1 bis V4 immer komplexere Merkmale encoded werden. 
% Spezialisierung, verschiedene Wege die das where oder what pathway (also parallele hierarchische Sequenzen). 
% \cite{kourtziCorticalRegionsInvolved2000} Regionen, welche auf unterschiedliche Formen reagieren
% \cite{kanwisherFusiformFaceArea1997} die FFA, die vor allem auf Gesichter reagiert
% \cite{epsteinCorticalRepresentationLocal1998} die PPA, die auf Szenen reagiert aber nur wenig auf einzelne Objekte und gar nicht auf Gesichter. 

% Das Problem an dem ganzen ist, dass die Gehirne unterschiedlich funktionieren. Viel Forschung ist bezogen auf einzelne Regionen im Gehirn (wo also die V1 generell ist). Um genauere Informationen über die Stimuli zu erlangen ist es aber nötig, feinere Unterschiede zu treffen. Vom region based zu so fein granular based wie möglich (was bei fmri studien voxel sind).

% All das sind allgemeine Studien, welche das Encoding untersucht haben (also, wie sind Informationen im Kopf gespeichert). Es gibt den Unterschied zum Decoding \cite{naselarisEncodingDecodingFMRI2011}
% Encoding uses stimuli to predict brain activity, decoding uses brain activity to predict features of the stimulus. Both of them can be used for different usecases. In our study we want to reconstruct visual stimuli, thus we need to use decoding models. These are usually done with linear classifiers (because it's easy to train those with limited training data.) In the brain, there could be nonlinear interactions however, where the activation of voxels might be interdependent. Training classifiers that would uncover these would however need lots of data. 

% Decoding haben kamitani und Tong gemacht. \cite{kamitaniDecodingVisualSubjective2005} Es wurde mit fmri Aktivität im frühen visuellen System die orientation aus 8 möglichkeiten von edges vorhergesagt. Auch wenn die orientation columns wahrscheinlich kleiner sind, als die voxel, kann durch ensemble approaches doch die Information rausgelesen werden. Dabei waren für die orientation selectivity insbesondere Neurone in V1 und V2 wichtig. 


% \cite{miyawakiVisualImageReconstruction2008} machen einen ersten Schrit in die Richtung von genereller Bildrekonstruktion, bei dem Muster in 10x10 patches (also 2 hoch 100 mögliche States) gut vorhergesagt werden können. Dieses Paper geht also einen Schritt über reine Klassifikation hinaus. Einzelne lokale Decoder werden trainiert, die jeweils Teile des Gesamtbildes beschreiben können. Somit kann auch mit wenigen Trainingssamples schon eine hohe Generalisierung erreicht werden. Auch hier wurde noch mehr low-level information aus V1 und V2 gearbeitet. Informationen von höheren Regionen wie V3 und V4 hatten in diesem Modell noch keinen großen impact (v1 hatte die meisten Voxel die significant waren).


% Das Problem dass aus dieser Forschung folgte, war dass nur low-level Information aus den frühen Regionen genutzt werden konnte, damit also möglich ist zu rekonstruieren, wie die Umrisse von etwas aussehen, aber kein Inhalt über die semantische Bedeutung decoded wird. Horikawa Et al nutzen also einen Ansatz um higher level Regions auch in das decoding einzubinden und dabei nicht auf Klassifizierung angewiesen zu sein, sondern bestenfalls einen generellen Decoder zu erstellen. Deshalb verfolgen sie einen zero-shot Ansatz mit dem sie generic object decoding betreiben (dabei können beliebige objektkategorien aus der Gehirnaktivität gelesen werden). Mithilfe von sparse linear regression haben sie die feature vectors von CNNs unterschiedlicher layer vorhergesagt. Sie konnten zeigen, dass die hierarchische Verarbeitung im Gehirn auch auf die hierarchische Verarbeitung in einem CNN abgebildet werden kann. So können mit der Aktivität in tiefen Gehirnregionen (wie V1, V2) die low-level Features eines CNN vorhergesagt werden und mit hohen Gehirnregionen (V4, LOC, PPA, FFA) die high-level Features vom CNN. 
% \cite{horikawaGenericDecodingSeen2017}
 

It has long been known that the visual system contains neurons that respond to various stimuli, such as object orientation\cite{grill-spectorHUMANVISUALCORTEX2004}. Functional magnetic resonance imaging (fMRI) has enabled noninvasive examination of these neural regions in greater detail. The visual cortex follows two main principles: hierarchical processing and functional specialization. Hierarchical processing indicates that increasingly complex features are encoded progressively from primary visual area V1 up to area V4. Functional specialization refers to parallel hierarchical pathways, commonly known as the `where' and `what' pathways, which process spatial information and object identification, respectively\cite{grill-spectorHUMANVISUALCORTEX2004}. Specific regions within the cortex have been identified to respond preferentially to different stimulus types; for instance, the fusiform face area (FFA) is specialized for faces\cite{kanwisherFusiformFaceArea1997}, whereas the parahippocampal place area (PPA) predominantly responds to scenes rather than individual objects or faces \cite{epsteinCorticalRepresentationLocal1998}. Further, cortical regions responding specifically to shape features have also been described\cite{kourtziCorticalRegionsInvolved2000}. However, a key challenge arises from individual differences in brain organization and function. Many studies focus on broad cortical modules (e.g. V1), but detailed insights into stimuli (e.g. the region within v1 that codes for horizontal edges) require finer resolution. In order get a more granular view on brain regions, the voxels that the fmri measures can be analysed. Although the voxel size is still not granular enough to find single neurons that code for example for a specific orientation of stimuli, the `average' activation of the neurons within a voxel might lean towards a specific orientation of a seen stimulus\cite{kamitaniDecodingVisualSubjective2005} and thus the voxels can contain this very granular information in an indirect way. When investigating brain regions, one can differentiate between encoding and decoding models. Encoding models try to predict brain activity depending on a given stimulus, decoding does the exact opposite: depending on measured brain activity, characteristics of a stimulus are supposed to be predicted\cite{naselarisEncodingDecodingFMRI2011}. For reconstructing visual stimuli from measured brain activity, decoding models are essential. Decoding models typically employ linear classifiers due to their effectiveness in cases where the amount of data is limited. Although neural interactions within the brain may be nonlinear, revealing these interdependencies through more complex models requires extensive datasets, which is usually not feasible in fMRI experiments (both due to the high recording effort and the granularity of the recorded data). Early influential decoding work by Kamitani and Tong\cite{kamitaniDecodingVisualSubjective2005} demonstrated that the orientation of edges (from eight possible orientations) could be predicted from fMRI activity from the activity early visual areas (V1 and V2). Despite orientation columns in the visual areas likely being smaller than individual voxels, ensemble decoding methods successfully extracted meaningful orientation information, that enabled the prediction of the orientation of seen stimuli. Advancing further, Miyawaki et al.\cite{miyawakiVisualImageReconstruction2008} introduced a method for image reconstruction, successfully predicting visual patterns composed of 10$\times$10 pixel patches (resulting in $2^{100}$ possible states). They trained multiple local decoders with slightly different features (for example single pixels or $2\time1$ pixel rectangles) that were then used together in an ensemble approach to reconstruct the shown stimuli. Like previous studies, Miyawaki et al.\ predominantly relied on low-level visual information from V1 and V2, as higher-level areas (V3 and V4) had minimal influence on the reconstruction. 
A limitation arising from this body of work is the predominant use of low-level visual information from early cortical regions, thus restricting reconstructions primarily to low-level features (like contours and textures) without capturing much of the semantic meaning. Addressing this limitation, Horikawa et al.\cite{horikawaGenericDecodingSeen2017} developed an approach that was able to incorporate the influence of higher cortical regions into the decoder. They were able to create a generic object decoder, which enabled the prediction of diverse object categories from brain activity without relying on having a definite set of objects that was learned in the training data. Using sparse linear regression, they predicted feature vectors from different layers of convolutional neural networks (CNNs). This approach demonstrated a correspondence between the hierarchical processing within the human brain and CNN hierarchies: low-level CNN features were predicted from early visual regions (V1, V2), whereas high-level CNN features correlated with activity in higher visual regions (V4, LOC, PPA, FFA).

Now that features of deep learning models can be predicted at different levels, it is possible to integrate higher brain structures into the decoding process. In this way, a general reconstruction of visual stimuli can be pursued, which is not only based on low-level image features, but also, for example, on semantically decoded semantic categories (e.g. in \cite{shenDeepImageReconstruction2019,ozcelikNaturalSceneReconstruction2023}). Shirakawa et al.\cite{shirakawaSpuriousReconstructionBrain2024} describe the general workflow used by modern reconstruction algorithms. This pipeline is illustrated in Figure~\ref{fig:recon_pipeline}. Image Reconstruction essentially consists of two steps: first, a decoder (which can also be called a translator, both terms are used interchangeably in this work) is used to map the brain data onto latent features. These latent features may, as in Horikawa et al.\cite{horikawaGenericDecodingSeen2017}, contain aggregated information about the low-level and high-level visual information. Typically, a linear model is used for the translator, which can learn to predict the features robustly and with little data from brain activity. The latent features are then converted into an image in a second step using a generator module, which can use different techniques such as diffusion models\cite{xuVersatileDiffusionText2024,ozcelikNaturalSceneReconstruction2023} or generative adversarial networks (GANs). These generative models are usually too complex to be trained with the small amount of recorded brain activity. Therefore, pre-trained models are used here to generate images from the previously predicted latent features.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/01_background_shirakawa_recon_pipeline.png}
    \caption[General Reconstruction Pipeline]{Visualization of the general Reconstruction Pipeline taken from Shirakawa et al.\cite{shirakawaSpuriousReconstructionBrain2024}}\label{fig:recon_pipeline}
\end{figure}

One algorithm using this approach is the iterative Convolutional Network algorithm (iCNN), first described by Shen et al.\cite{shenDeepImageReconstruction2019}. They adopted the method introduced by Horikawa et al.\cite{horikawaGenericDecodingSeen2017} to predict hierarchical features of a deep neural network based on brain activity and subsequently used these features to reconstruct images that the participants have seen. Specifically, Shen et al.\ used a VGG-19 network that was pre-trained on ImageNet, to obtain features from the training images. Then a sparse linear regression was trained to predict these features from the brain activity. Then, for two different test datasets, the trained decoder predicted the test features from the brain activity. Backpropagation is used to adjust the pixels in multiple steps to minimise the loss between the predicted features and the features, that the pixels produce in the vgg-19 network. In addition, an optional deep generator network was introduced to act as an image prior to improve the naturalness of the reconstructed images. Using this approach, Shen et al.\ successfully reconstructed both natural and artificially generated test images, effectively exploiting information from higher cortical structures. The iCNN has been used successfully to study the influence of attention on ambiguous stimuli. Horikawa et al.\cite{horikawaAttentionModulatesNeural2022} showed two superimposed images in an experiment. Subjects were asked to focus on one of the images, and the image on which attention was focused could be successfully reconstructed. This shows that the iCNN can also be used to map attention-modulated top-down processes and does not only depend on the actual visual properties of an image. Cheng et al.\cite{chengReconstructingVisualIllusory2023} used images that created optical illusions so that people perceived lines that were not actually present. These perceived (non-existent) features of the images could also be successfully reconstructed using the algorithm.



Neben dem iCNN, welcher die Rekonstruktion über eine iterative Anpassung im Pixel Space vornimmt, gibt es noch eine zweite Gruppe an Rekonstruktionsalgorithmen, welche die Rekonstruktion über diffusion basierte Image creation durchführen\cite{takagiHighResolutionImageReconstruction,ozcelikNaturalSceneReconstruction2023, scottiMindEye2SharedSubjectModels2024, scottiReconstructingMindsEye2023}. 


% Bei denen weiß ich nicht was ich mit ihnen machen soll:
% \cite{kamitaniSpatialSmoothingHurts2010}
% \cite{yamashitaSparseEstimationAutomatically2008}
% \cite{chengReconstructingVisualIllusory2023}

\section{Data Diversity}
- Wir wollen einen Bildgenerator der universal ist, also Bilder mit ganz unterschiedlichen Charakteristika rekonstruieren kann.
- Generelle Erläuterung über Data Diversity in machine learning und OOS generalization
- Irgendwo auch schonmal irgendwie image naturalness droppen (also dass Diversität sich auch auf die naturalness beziehen kann)
- evtl: Image Diversity ist gar nicht so leicht zu definieren

- Datendiversität gibts auf unterschiedlichen Ebenen (das muss irgendwie mit drin sein für den Rest der Arbeit)
- Bei der Diversity kann man auch darauf eingehen, dass der visual pathway zwei verschiedene Wege folgt, in 20, 31, 66 aus \cite{chenExploringNaturalnessAIGenerated2023}
    - Genauso ist es sehr wichtig, dass eingeführt wird in das Konzept, dass es low-level und high-level Diversity geben kann, sodass die folgenden Kapitel (insbesondere dropout) darauf aufbauen können
    - Das macht ja zum Beispiel auch der ICNN Algorithmus, der die natürliche Hierarchisierung von VGG19 für diese Einteilung nutzt
    - Horikawa und so machen gucken auch schon, wie sich die lowe level Features durch unterschiedliche Gehirnstrukturen vorhersagen lassen

Diversity in Image Reconstruction 
    Output Dimension collapse\cite{shirakawaSpuriousReconstructionBrain2024}


\section{Problem Statement}
Can we reduce the output dimension collapse?
    Or at least understand the phenomenon better
Can we improve results by adding diversity in some ways
    Or find the parts in the training data, that account for the generalization