\chapter{Discussion}
% https://www.scribbr.com/dissertation/discussion/

% Summary: A brief recap of your key results
% Interpretations: What do your results mean?
% Implications: Why do your results matter?
% Limitations: What can’t your results tell us?
% Recommendations: Avenues for further studies or 

In this work, three experiments were performed to investigate the influence of diversity on image reconstruction from brain activity. The two algorithms iCNN by Shen et al.~\cite{shenDeepImageReconstruction2019} and in particular the Brain-Diffuser by Ozcelic et al.~\cite{ozcelikNaturalSceneReconstruction2023} were investigated in more detail. The specific results of the individual experiments have already been discussed. At this point, we will discuss the experiments and the resulting findings in the broader context of data diversity and output dimension collapse. First, the individual results are briefly summarised to then open up the discussion for the broad context. In the first experiment, an attempt was made to create subsamples of the training data that maximised diversity at different levels (low-level to high-level features) and tested against a random selection of the training data. For the iCNN in particular, a diversity-based selection of training data slightly improved the reconstruction for the natural test images, but the reconstruction of the artificial shapes was slightly worsened by the diversity-based subselection, indicating a deterioration of the OOD generalisation. A more detailed investigation, using more monotonous or heterogeneous subsamples for training, revealed important findings: on the one hand, the brain diffuser may be very strongly influenced by the monotonous images (or its performance in reconstructing the artificial shapes was largely based on the monotonous test images). On the other hand, the iCNN appeared to have a slight bias towards reconstructing images with single objects in the foreground and little detail in the background, such as the artificial shapes. In the second experiment, the influence of the CLIP text embeddings on the brain diffuser reconstruction was analysed. AI was used to create additional captions for the training images, focusing on either low-level or high-level features of the images. These captions were used to train the CLIP text translator and subsequently create reconstructions with the brain diffuser. Particularly at high levels of mixing, i.e.\ when the CLIP Text features had a stronger influence on the reconstruction, it was shown that a focus on the low-level features ensured a better reconstruction of the artificial images. At the same time, however, the performance of the natural test images was reduced. Finding a setting that improves both OOD generalisation (in the form of artificial images) and the performance of congruent test images therefore seems difficult. However, these results at least imply that focusing on low-level vs.\ high-level features during training may also favour corresponding reconstructions. In the third experiment, the influence of the CLIP vision embeddings on the reconstruction of the brain diffuser was further investigated. Backpropagation was used to craft perturbations in the training images, which are barely visible to the naked eye, but strongly alter the meaning of the CLIP vision embeddings extracted from them. The information from the captions could thus be added to the images in the form of `semantic noise'. The CLIP Vision translator was trained on these perturbed images, and reconstructions were again produced using the Brain Diffuser. It was found that the reconstructions could not be improved for the natural test images by focusing on the semantic meaning, nor could they be improved for the artificial shapes by reducing the semantic meaning of the images. In fact, performance deteriorated in both conditions. This raised the question of whether a perturbation in which the modified images had to be similar to the originals was necessary at all, and whether modifying the CLIP vision features directly might be a more targeted and effective way of modifying the semantic meaning of the images.

% In dieser Arbeit wurden drei Experimente durchgeführt, die den Einfluss von Diversität auf die Rekonstruktion von Bildern aus Gehirnaktivität untersucht haben. Dabei wurden die zwei Algorithmen iCNN von Shen et al.~\cite{shenDeepImageReconstruction2019} und insbesondere der Brain-Diffuser von Ozcelic et al.~\cite{ozcelikNaturalSceneReconstruction2023} näher untersucht. Die spezifischen Ergebnisse der einzelnen Experimente wurden bereits diskutiert. An dieser Stelle soll nun eine Diskussion der Experimente und der daraus folgenden Erkenntnisse im breiteren Kontext der Datendiversität und des output dimension collapse stattfinden. Zunächst seien dazu die einzelnen Ergebnisse kurz zusammengefasst beschrieben. 


% Im ersten Experiment wurde versucht Subsamples der Trainingsdaten zu erstellen, welche die Diversität auf verschiedenen Ebenen (low-level Features bis high-level Features) maximiert haben und gegen eine zufällige Auswahl der Trainingsdaten getestet. Insbesondere mit dem iCNN konnte mit einer diversitätsbasierten Auswahl der Trainingsdaten die Rekonstruktion für natürliche Testbilder leicht verbessert werden, die Rekonstruktion von den artificial Shapes hingegen wurde durch die diversitätsbasierte Subselection aber hingegen leicht verschlechtert, was auf eine Verschlechterung der OOD Generalisierung hindeutet. In einer näheren Untersuchung, in der weitere subsamples für das Training genutzt wurden die besonders monoton bzw. heterogen waren wurden wichtige Erkenntnisse sichtbar: einerseits lässt sich der Brain-Diffuser sehr stark von den monotonen Bildern beeinflussen (bzw. seine Leistung, die Artificial Shapes zu rekonstruieren basierte maßgeblich auf den monotone Testbildern). Andererseits stellte sich heraus, dass der iCNN einen leichten bias zu haben scheint, Bilder zu rekonstruieren, die wie die artificial shapes einzelne Objekte im Vordergrund und wenig Details im Hintergrund haben. 

% Im zweiten Experiment wurde der Einfluss der CLIP Text embeddings auf die Rekonstruktion des Brain-Diffusers untersucht. Mithilfe von KI wurden zusätzliche Captions der Trainingsbilder erstellt, die sich entweder auf low-level oder high-level features der Bilder fokussierten. Mithilfe dieser captions wurden CLIP Text translator trainiert und mit dem Brain-Diffuser Rekonstruktionen erstellt. Insbesondere bei hohem mixing, also stärkerem Einfluss der CLIP Text features bei der Rekonstruktion zeigte sich, dass ein Unterschied im Fokus auf die low-level Features für eine bessere Rekonstruktion der artificial images sorgte. Im gleichen Atemzuge wurde aber die Performance für die natural test images verringert. Ein setting zu finden, bei dem also sowohl die OOD Generalisierung (in Form der artificial images) als auch die Performance bei kongruenten Testbildern verbessert wird scheint also schwer zu sein. Diese Ergebnisse bedeuten aber zumindest, dass der Fokus auf low-level vs high-level features bei dem training auch korrespondierende Rekonstruktionen begünstigen kann.

% Im dritten Experiment wurde der Einfluss der CLIP Vision embeddings auf die Rekonstruktiond es Brain-Diffusers weiter untersucht. Mithilfe von backpropagation wurden gezielt perturbations in den Trainingsbildern erzeugt, die für das bloße Auge kaum sichtbar sind, aber die Bedeutung der daraus extrahierten CLIP Vision embeddings stark verändert haben. Die Informationen aus den Captions konnten so in Form von `semantic noise' den Bildern hinzugefügt werden. Mit diesen perturbed Bildern wurde der CLIP Vision translator trainiert und wieder mit dem Brain-Diffuser Rekonstruktionen erstellt. Es zeigte sich, dass die Rekonstruktionen weder für die natural test images verbessert werden konnte, wenn ein Fokus auf die semantische Bedeutung gelegt wurde, noch dass sie für die artificial shapes verbessert werden konnte, wenn die semantische Bedeutung der Bilder verringert wurde. Für beide Bedingungen wurde die performance sogar eher verschlechtert. Daraufhin stellte sich die Frage, ob eine perturbation bei der die modifizierten Bilder den originalen überhaupt ähnlich sein müssen überhaupt notwendig ist und nicht eventuell eine Modifikation der CLIP Vision features direkt eine zielgerichtetere und effektivere Möglichkeit sein könnte, die semantische Bedeutung der Bilder zu modifizierten.
The results in this paper largely support those of Shirakawa et al.~\cite{shirakawaSpuriousReconstructionBrain2024}. It could be reproduced that the diffusion-based brain diffuser does not perform particularly  with artificial shapes, and that the hallucinations of the diffusion process become visible strongly. As Shirakawa et al.\ also showed, the iCNN performed better for this test dataset. These results suggest that the reconstruction algorithm also has a strong influence on the generalisation ability of the reconstructions. Regarding the output dimension collapse, the experiments with the monotonic dropout and the low-level labels yielded interesting results: almost all of the performance of the brain diffuser with the artificial shapes seems to be due to the monotonic training images. This can be interpreted to mean that the `monotone background' dimension in the training data is essential for these test images. This supports the results of the simulation study by Shirakawa et al.\ in the sense that training data in the right dimensions is needed to enable general reconstructions. The results with the low-level labels can be interpreted in the same way; a focus on low-level labels can also support the low-level reconstructions (artificial shapes). However, in both cases, due to the fact that no new data could be collected for this work, the performance on the natural test data decreased at the same time as the performance on artificial shapes increased. In order to gain further insight into whether the overall performance for all test data can be improved with the findings of this work, it would be important to create a new training data set that is designed specifically by regarding relevant image dimensions. Unfortunately, the question of which dimensions are fundamentally important for learning the reconstruction of the entire possible image space remains unanswered. The low-level vs.\ high-level and monotonic vs.\ heterogeneous dimensions analysed in this study are by no means the only possible dimensions that could be used. Other dimensions like colors, object categories, object locations, rotation and others that could possibly not even be described by natural language might also be suited better as the fundamental dimensions that would span the image space. In regard to OOD generalisation, the results of this study suggest that with the current translation method, it would presumably be difficult to generalise to dimensions that do not appear in the training data at all. The result of the diversity-based dropout is particularly interesting. In some cases, the iCNN performed better with a diversity-based subsample than a random subsample. This is a small indication that greater data diversity is able to improve generalisation (and thus presumably also helps with OOD generalisation). On the other hand, the fact that diversity-based dropout had no effect compared to the random dropout baseline for the brain diffuser suggests that true OOD generalisation is difficult with this algorithm. Only by explicitly adding training images with monotonous backgrounds (which are closer to the distribution of the artificial shapes than the heterogeneous images) could the artificial shapes be better reconstructed. This suggests that the effect of training images close to the distribution of the artificial shapes was significantly greater in this case than simply diversifying the training images.

In addition to the results directly related to the research question of output dimension collapse and the OOD generaliser, this work also produced other results that are interesting in the overall context of image reconstruction from brain activity. The different performance criteria of the reconstructions could be examined in more detail in the three experiments under different occasions. PixelCorrelation sometimes produced questionable results, e.g.\ when increasing the CLIP text mixing parameter also led to an improvement in PixelCorrelation for shuffled captions (i.e.\ a strengthening of an actual negative influence). In another case, the perturbations slightly worsened the reconstruction results (measured with Dreamsim, CLIP accuracy and checked by qualitative evaluation of the reconstructed images), but PixelCorrelation failed to detect any difference between the reconstructed images. On the other hand, CLIP accuracy was very poor at detecting differences in the reconstruction of artificial shapes and was only slightly above random level (which is plausible since there is little semantic information for the CLIP embeddings in the artificial shapes). In the course of this work, dreamsim performed well as a mid-level metric in the sense that it reliably detected changes in the reconstruction quality of both natural test images and artificial shapes.

% Zu meinen Ergebnissen:
% - PixelCorrelation ist nicht so gut (haben wir an vielen Ergebnissen schon irgendwie gesehen)
%     - Zum Beispiel dass durch die Erhöhung von falschem Mixing die Ergebnisse verbessert wurden
%     - Die random Reduction hat fast keinen Effekt gehabt
%     - Clip-accuracy war wiederum sehr schlecht bei den artificial shapes

- Decoding von Brain-Diffuser ist auch irgendwo fraglich, weil das so wenig sensibel auf die random reduction war
    - Schauen, wieso Brain-Diffuser überhaupt irgendwas von den Artificial Shapes rekonstruieren kann
    - Die random Reduction hat fast keinen Effekt gehabt
    - Insbesondere halt CLIP embeddings (Vision ist ja quasi 0 bei denen)
- Der Einfluss der CLIP Text Embeddings beim Brain-Diffuser war ja auch ein Witz

Der Bias vom iCNN

Generell:
- https://x.com/PTenigma/status/1893459033379807586
- Erklärung dafür, dass der Brain-Diffuser in clip so viel besser ist, ist vielleicht einfach die image-naturalness
- Man könnte auch noch den tatsächlichen test einfügen in die versatile diffusion
- Den Clip Space der Artificial shapes untersuchen, gibt es eventuell Bilder, die sich gar nicht erst in CLIP einbetten lassen?

Andere Datensätze:
- Things, NSD, jetzt neu auch NSD artificial images 

https://arxiv.org/pdf/2503.06286

Biologisch gesehen:
- Schauen, auf welchen gehirnebenen die Treatments von uns jeweils einen Effekt hatten und wo nicht.

Konkrete Veränderung des setups das wir haben:

- Andere "fertige" End2End-Algorithmen (mindeye etc.)
    - Man könnte auch komplette captions generieren lassen und nicht nur die Features wie bei unibrain \cite{maiUniBrainUnifyImage2023}

- Andere Decoder Algorithmen 
    - Statt der ridge regression, durch Methoden in dieser Arbeit könnten teilweise die Menge der Daten deutlich erhöht werden, wodurch möglicherweise auch etwas anderes als eine (Ridge) Regression verwendet werden könnte

- Andere Reconstruction Algorithmen:
    - Um die rare concepts zu verbessern sagen \cite{samuelGeneratingImagesRare2024} dass man sich vor allem auf den diffusion input fokussieren muss
        - man kann noch einen Schritt weiter gehen und sagen, man solle sich ausschließlich auf den Input konzentrieren und mehr mit VDVAEs machen (oder ähnlichen Algorithmen die nicht so viel halluzinieren)
    - Andere Image generation Algorithms (innerhalb des Brain-Diffuser Algorithm, also versatile diffusion austauschen)
    - Theoretisch sind viele Contexts möglich bei dem versatile diffusion approach, mit einer weiterentwicklung könnte man so dann auch je nach image das regeneriert werden soll ganz unterschiedlichen kontext geben (das schreibe ich auch schon teilweise beim aicap Experiment)
    - Generell kann man ja schauen, wie (ja wie nur, ich hab den Satz nicht fertiggeschrieben...)
    - iCNN mit clip ginge ja theoretisch auch
    - Oder Autoregressive models wie jetzt jüngst von OpenAI gezeigt wurde


Eventuell abschließen damit, dass die visuelle Verarbeitung so komplex und top-down reguliert ist, dass es wahrscheinlich bei der Bildrekonstruktion nicht eine einzige Wahrheit gibt. Klar, man kann versuchen ein Bild was jemand sieht 1zu1 wieder darzustellen, das wird aber der Verarbeitung im Kopf nicht unbedingt gerecht. Es können mehrere Unterschiedliche Repräsentationen gleichzeitig geschehen und damit unterschiedliche Bilder gleich gut rekonstruiert werden. 


\section{Conclusion}
% https://www.scribbr.com/dissertation/write-conclusion/
Hier nochmal die Zusammenfassung dessen was ich gemacht habe. 


% Clearly state the answer to your main research question
% Summarize and reflect on your research process
% Make recommendations for future work on your thesis or dissertation topic
% Show what new knowledge you have contributed to your field
% Wrap up your thesis or dissertation
