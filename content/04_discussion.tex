\chapter{Discussion}
% https://www.scribbr.com/dissertation/discussion/

% Summary: A brief recap of your key results
% Interpretations: What do your results mean?
% Implications: Why do your results matter?
% Limitations: What can’t your results tell us?
% Recommendations: Avenues for further studies or 

In this work, three experiments were performed to investigate the influence of diversity on image reconstruction from brain activity. The two algorithms iCNN by Shen et al.~\cite{shenDeepImageReconstruction2019} and in particular the Brain-Diffuser by Ozcelic et al.~\cite{ozcelikNaturalSceneReconstruction2023} were investigated in more detail. The specific results of the individual experiments have already been discussed. At this point, we will discuss the experiments and the resulting findings in the broader context of data diversity and output dimension collapse. First, the individual results are briefly summarised to then open up the discussion for the broad context. In the first experiment, an attempt was made to create subsamples of the training data that maximised diversity at different levels (low-level to high-level features) and tested against a random selection of the training data. For the iCNN in particular, a diversity-based selection of training data slightly improved the reconstruction for the natural test images, but the reconstruction of the artificial shapes was slightly worsened by the diversity-based subselection, indicating a deterioration of the OOD generalisation. A more detailed investigation, using more monotonous or heterogeneous subsamples for training, revealed important findings: on the one hand, the brain diffuser may be very strongly influenced by the monotonous images (or its performance in reconstructing the artificial shapes was largely based on the monotonous test images). On the other hand, the iCNN appeared to have a slight bias towards reconstructing images with single objects in the foreground and little detail in the background, such as the artificial shapes. In the second experiment, the influence of the CLIP text embeddings on the brain diffuser reconstruction was analysed. AI was used to create additional captions for the training images, focusing on either low-level or high-level features of the images. These captions were used to train the CLIP text translator and subsequently create reconstructions with the brain diffuser. Particularly at high levels of mixing, i.e.\ when the CLIP Text features had a stronger influence on the reconstruction, it was shown that a focus on the low-level features ensured a better reconstruction of the artificial images. At the same time, however, the performance of the natural test images was reduced. Finding a setting that improves both OOD generalisation (in the form of artificial images) and the performance of congruent test images therefore seems difficult. However, these results at least imply that focusing on low-level vs.\ high-level features during training may also favour corresponding reconstructions. In the third experiment, the influence of the CLIP vision embeddings on the reconstruction of the brain diffuser was further investigated. Backpropagation was used to craft perturbations in the training images, which are barely visible to the naked eye, but strongly alter the meaning of the CLIP vision embeddings extracted from them. The information from the captions could thus be added to the images in the form of `semantic noise'. The CLIP Vision translator was trained on these perturbed images, and reconstructions were again produced using the Brain Diffuser. It was found that the reconstructions could not be improved for the natural test images by focusing on the semantic meaning, nor could they be improved for the artificial shapes by reducing the semantic meaning of the images. In fact, performance deteriorated in both conditions. This raised the question of whether a perturbation in which the modified images had to be similar to the originals was necessary at all, and whether modifying the CLIP vision features directly might be a more targeted and effective way of modifying the semantic meaning of the images.

The results in this paper largely support those of Shirakawa et al.~\cite{shirakawaSpuriousReconstructionBrain2024}. It could be reproduced that the diffusion-based brain diffuser does not perform particularly  with artificial shapes, and that the hallucinations of the diffusion process become visible strongly. As Shirakawa et al.\ also showed, the iCNN performed better for this test dataset. These results suggest that the reconstruction algorithm also has a strong influence on the generalisation ability of the reconstructions. Regarding the output dimension collapse, the experiments with the monotonic dropout and the low-level labels yielded interesting results: almost all of the performance of the brain diffuser with the artificial shapes seems to be due to the monotonic training images. This can be interpreted to mean that the `monotone background' dimension in the training data is essential for these test images. This supports the results of the simulation study by Shirakawa et al.\ in the sense that training data in the right dimensions is needed to enable general reconstructions. The results with the low-level labels can be interpreted in the same way; a focus on low-level labels can also support the low-level reconstructions (artificial shapes). However, in both cases, due to the fact that no new data could be collected for this work, the performance on the natural test data decreased at the same time as the performance on artificial shapes increased. In order to gain further insight into whether the overall performance for all test data can be improved with the findings of this work, it would be important to create a new training data set that is designed specifically by regarding relevant image dimensions. Unfortunately, the question of which dimensions are fundamentally important for learning the reconstruction of the entire possible image space remains unanswered. The low-level vs.\ high-level and monotonic vs.\ heterogeneous dimensions analysed in this study are by no means the only possible dimensions that could be used. Other dimensions like colors, object categories, object locations, rotation and others that could possibly not even be described by natural language might also be suited better as the fundamental dimensions that would span the image space. In regard to OOD generalisation, the results of this study suggest that with the current translation method, it would presumably be difficult to generalise to dimensions that do not appear in the training data at all. The result of the diversity-based dropout is particularly interesting. In some cases, the iCNN performed better with a diversity-based subsample than a random subsample. This is a small indication that greater data diversity is able to improve generalisation (and thus presumably also helps with OOD generalisation). On the other hand, the fact that diversity-based dropout had no effect compared to the random dropout baseline for the brain diffuser suggests that true OOD generalisation is difficult with this algorithm. Only by explicitly adding training images with monotonous backgrounds (which are closer to the distribution of the artificial shapes than the heterogeneous images) could the artificial shapes be better reconstructed. This suggests that the effect of training images close to the distribution of the artificial shapes was significantly greater in this case than simply diversifying the training images.

In addition to the results directly related to the research question of output dimension collapse and the OOD generaliser, this work also produced other results that are interesting in the overall context of image reconstruction from brain activity. The different performance criteria of the reconstructions could be examined in more detail in the three experiments under different occasions. PixelCorrelation sometimes produced questionable results, e.g.\ when increasing the CLIP text mixing parameter also led to an improvement in PixelCorrelation for shuffled captions (i.e.\ a strengthening of an actual negative influence). In another case, the perturbations slightly worsened the reconstruction results (measured with Dreamsim, CLIP accuracy and checked by qualitative evaluation of the reconstructed images), but PixelCorrelation failed to detect any difference between the reconstructed images. On the other hand, CLIP accuracy was very poor at detecting differences in the reconstruction of artificial shapes and was only slightly above random level (which is plausible since there is little semantic information for the CLIP embeddings in the artificial shapes). In the course of this work, dreamsim performed well as a mid-level metric in the sense that it reliably detected changes in the reconstruction quality of both natural test images and artificial shapes.

In addition to the evaluation criteria, the work also provided further insights into the reconstruction algorithms used. In particular, the Brain Diffuser was examined in detail in the three experiments. The fact that shuffled captions had no effect on the results of the Brain Diffuser (with the default mixing of 0.4) raises initial questions about how well the prediction of semantic features actually works with the Brain Diffuser. The identification accuracy of the CLIP text features was very high for the natural test images, and yet the reconstruction was worse at high mixing (measured with the CLIP-accuracy). This is an indication that the semantic information could not be translated very well, despite the high identification accuracy. It stands to reason that the good prediction of semantic categories (e.g. the exact animal in an image) that Ozcelik et al.~\cite{ozcelikNaturalSceneReconstruction2023} found was mainly due to the overlap of training and test data in the NSD dataset (as Shirakawa et al.~\cite{shirakawaSpuriousReconstructionBrain2024} were also able to show). Qualitatively, only very rough semantic categories (such as `living object' or `vehicle') could be recognised in the Deeprecon dataset which has a significantly lower train/test overlap concerning semantic categories. Also, the fact that the performance of predicting artificial shapes using CLIP Vision features was completely independent of the size of the training dataset (see results for random subsampling) casts doubt on whether images such as the artificial shapes can be properly encoded using CLIP Vision features at all. All in all, the results of this work do not present the generalisation capabilities of the brain diffuser in the most favourable light. As a consequence of these results, it would be appropriate to examine how the performance of the complete Brain Diffusers (using versatile diffusion) differs from its intermediate results of the VDVAE, and whether the Brain-Diffuser could be simply performing a reconstruction with the VDVAE + natural looking hallucination with very limited semantic information (at least if there is not a big overlap in train and test data). The fact, that the the Brain-Diffuser showed better baseline results for the natural test images (measured by CLIP-accuracy) than the iCNN could therefore also be explained by the fact, that the diffusion based hallucinations look more natural to the CLIP Vision encoder than the results of the iCNN.@ Therefore the CLIP-accuracy would favor the Brain-Diffuser only because of the fact, that the created images look more natural, but not because it's better at predicting the actual semantic content. Also, the iCNN needs to be critically examined, as it seems to be particularly good at predicting salient objects in the center of an image. Whether this bias is a consequence of the ImageNet~\cite{dengImageNetLargescaleHierarchical2009} dataset used to pre-train the iCNN (i.e.\ it is hardly possible to perform different reconstructions with it), or possibly even a logical consequence of the nature of human attention, needs further investigation.


\section{Future directions}

Some open questions remain for the future. In particular, whether there really is a set of fundamental dimensions that, as shown in as simulation study by Shirakawa et al.~\cite{shirakawaSpuriousReconstructionBrain2024}, is able to greatly improve the generalisation of brain activity translation (and hence reconstruction) with linear increase in training data. This raises the question of what exactly these fundamental dimensions might be. In this work, the dimensions low-level vs high-level information were mainly investigated. However, this need not be the only approach. Further approaches in this direction could try to use the concept of intrinsic dimensionality~\cite{bennettIntrinsicDimensionalitySignal1969} as the minimum number of dimensions needed to represent all data. For example, initial research into intrinsic dimensionality showed that all ImageNet images could be covered by about 20 dimensions~\cite{popeIntrinsicDimensionImages2021}. In further studies, such a method could be used to select training data so that a subset of images is used that most closely reflects the intrinsic dimensions of a large dataset such as ImageNet. If new data can be collected with the MRI, one would also not be limited to working with subsets of training data, but can also show subjects entirely new images. For example, with a core-set approach~\cite{nguyenDatasetDistillationInfinitely2021,wangDatasetDistillation2018}, the intrinsic dimensionality of the training dataset could be preserved with a drastically reduced dataset containing artificially created images. Such approaches would still be limited in that they would only represent the intrinsic dimensionality of a finite dataset (i.e. no generalisation over the entire possible image space would be sought), but modern automated data mining methods make it possible to generate extremely large base datasets whose distillation can already cover a large part of all possible images. However, it is not enough to focus only on the images (or in other words the features that are to be predicted) in the training dataset. It is equally important to focus also on the diversity of brain activity. This is because there is no guarantee that maximising diversity at the feature level will also maximise diversity in brain activity. Just because an image is a perfect representative of one of the intrinsic dimensions of the image dataset after image distillation does not necessarily mean that this image will trigger the corresponding activity in the brain. For example, an image that is a perfect representative of the class 'dog' at the feature level could end up looking very different from a dog for a human and therefore not trigger the activity in the brain that seeing a dog would normally trigger. For this reason, it would be important for future research to keep an eye on brain activity and see if the findings from intrinsic dimensions and data distillation would also transfer to brain activity. In a future experiment, both levels could also be combined by looking at  a possible trade-off in a dataset that has high diversity at both the image level and the brain activity level. Such a training dataset could be especially suited for subsequent reconstruction. It would also not necessarily be necessary to record new brain data for this study and could be validated using previously recorded data.

% Wie kommen wir näher dem Problem, die Dimensionen zu finden, welche wir brauchen, um den output dimension collapse zu verhindern?
% - Welche sind die fundamentalen Achsen?
% - \cite{bennettIntrinsicDimensionalitySignal1969} The intrinsic dimensionality of a collection of signals is defined to be equal to the number of free parameters required in a hypothetical signal generator capable of producing a close approximation to each signal in the collection. 
% - Kann zum Beispiel gemessen werden mit der Menge an PCA Komponenten die man braucht um 90\% der Varianz zu erfüllen
% - \cite{popeIntrinsicDimensionImages2021} (intrinsic dimensionality of images, imagenet about 20)
    % - Man könnte mit Dataset Distillation versuchen Bilder herzustellen, die die intrinsic dimension komplett ausfüllen würden
% -  Dataset Distillation, keeping the intrinsic dimension
    % - Den Nguyen muss man noch ein Bisschen genauer untersuchen
% - Für diese Methoden braucht man dann erstmal einen Datensatz, der alle möglichen vorstellbaren Bilder beinhalten müsste, das an sich ist schon relativ unwahrscheinlich, aber sehr sehr große Bilddatensätze die man distillieren kann müssten sich ja finden lassen. 
% - Vielleicht bringt das ja was für die Reconstruction lul
% - Wer braucht schon OOD Generalisierung, wenn man theoretisch alle Dimensionen mit drin hatt
% - Das wird eine Frage sein, ob die Distillierung sich auch ohne weiteres so überträgt auf die Daten die von Menschen aufgenommen werden
% - Nur weil ein Bild im Image space der theoretisch beste Vertreter für einen Hund ist, heißt das ja nicht, dass dadurch die Gehirnaktivität dementsprechend erregt wird
% - Also wäre es sehr spannend zu schauen, ob man die Erkenntnisse aus der image datset distillation auch auf Menschen übertragen kann
% - Eine andere Herangehensweise wäre also diese beiden Disziplinen miteinander zu verbinden, also zu schauen, mit welchem Subdatensatz man die Aktivierung im Gehirn möglichst diversifizieren kann
% - Das könnte man sogar machen ohne neue Daten zu erheben. Also wäre es ein Ansatz der im Gegensatz zu dieser Arbeit nicht vom Input ausgeht sondern vom output



Andere Datensätze:
- Things\cite{hebartTHINGSdataMultimodalCollection2023}, NSD\cite{allenMassive7TFMRI2022}, jetzt neu auch NSD  artificial images \cite{gifford7TFMRIDataset2025}

- Andere Decoder Algorithmen 
    - Statt der ridge regression, durch Methoden in dieser Arbeit könnten teilweise die Menge der Daten deutlich erhöht werden, wodurch möglicherweise auch etwas anderes als eine (Ridge) Regression verwendet werden könnte
    - Den Clip Space der Artificial shapes untersuchen, gibt es eventuell Bilder, die sich gar nicht erst in CLIP einbetten lassen?

- Andere Reconstruction Algorithmen:
    - Um die rare concepts zu verbessern sagen \cite{samuelGeneratingImagesRare2024} dass man sich vor allem auf den diffusion input fokussieren muss
        - man kann noch einen Schritt weiter gehen und sagen, man solle sich ausschließlich auf den Input konzentrieren und mehr mit VDVAEs machen (oder ähnlichen Algorithmen die nicht so viel halluzinieren)
    - Andere Image generation Algorithms (innerhalb des Brain-Diffuser Algorithm, also versatile diffusion austauschen)
    - Theoretisch sind viele Contexts möglich bei dem versatile diffusion approach, mit einer weiterentwicklung könnte man so dann auch je nach image das regeneriert werden soll ganz unterschiedlichen kontext geben (das schreibe ich auch schon teilweise beim aicap Experiment)
    - Den Clip Space der Artificial shapes untersuchen, gibt es eventuell Bilder, die sich gar nicht erst in CLIP einbetten lassen?
    - iCNN mit clip ginge ja theoretisch auch
    - Oder Autoregressive models wie jetzt jüngst von OpenAI gezeigt wurde

- Andere "fertige" End2End-Algorithmen (mindeye etc.)
    - Man könnte auch komplette captions generieren lassen und nicht nur die Features wie bei unibrain \cite{maiUniBrainUnifyImage2023}

Biologisch gesehen:
    - Schauen, auf welchen gehirnebenen die Treatments von uns jeweils einen Effekt hatten und wo nicht.

    
Eventuell abschließen damit, dass die visuelle Verarbeitung so komplex und top-down reguliert ist, dass es wahrscheinlich bei der Bildrekonstruktion nicht eine einzige Wahrheit gibt. Klar, man kann versuchen ein Bild was jemand sieht 1zu1 wieder darzustellen, das wird aber der Verarbeitung im Kopf nicht unbedingt gerecht. Es können mehrere Unterschiedliche Repräsentationen gleichzeitig geschehen und damit unterschiedliche Bilder gleich gut rekonstruiert werden. 


\section{Conclusion}
% https://www.scribbr.com/dissertation/write-conclusion/
Hier nochmal die Zusammenfassung dessen was ich gemacht habe. 


% Clearly state the answer to your main research question
% Summarize and reflect on your research process
% Make recommendations for future work on your thesis or dissertation topic
% Show what new knowledge you have contributed to your field
% Wrap up your thesis or dissertation
