\chapter{Discussion}


Zu meinen Ergebnissen:
- PixelCorrelation ist nicht so gut (haben wir an vielen Ergebnissen schon irgendwie gesehen)
    - Zum Beispiel dass durch die Erhöhung von falschem Mixing die Ergebnisse verbessert wurden
    - Die random Reduction hat fast keinen Effekt gehabt


den dimension collapse in dem monotone dropout ziemlich gut nochmal gesehen, es wird nur noch auf das abgebildet, was das modell schon kann
aber auch ein Bisschen catastrophic forgetting. 
Oder?
Also die Gesamtleistung wird ja dann nicht besser oder doch?
Also verglichen mit der main baseline frag ich mich gerade.
Sollte man vielleicht noch hinzufügen, also den Vergleich von monotone subsampling mit 100 baselin

- Decoding von brain-diffuser ist auch irgendwo fraglich, weil das so wenig sensibel auf die random reduction war

Generell:
- https://x.com/PTenigma/status/1893459033379807586
- Erklärung dafür, dass der brain-diffuser in clip so viel besser ist, ist vielleicht einfach die image-naturalness
- Man könnte auch noch den tatsächlichen test einfügen in die versatile diffusion

Andere Datensätze:
- Things, NSD, jetzt neu auch NSD artificial images https://arxiv.org/pdf/2503.06286

Biologisch gesehen:
- Schauen, auf welchen gehirnebenen die Treatments von uns jeweils einen Effekt hatten und wo nicht.

Konkrete Veränderung des setups das wir haben:

- Andere "fertige" End2End-Algorithmen (mindeye etc.)
    - Man könnte auch komplette captions generieren lassen und nicht nur die Features wie bei unibrain \cite{maiUniBrainUnifyImage2023}

- Andere Decoder Algorithmen 
    - Statt der ridge regression, durch Methoden in dieser Arbeit könnten teilweise die Menge der Daten deutlich erhöht werden, wodurch möglicherweise auch etwas anderes als eine (Ridge) Regression verwendet werden könnte

- Andere Reconstruction Algorithmen:
    - Um die rare concepts zu verbessern sagen \cite{samuelGeneratingImagesRare2024} dass man sich vor allem auf den diffusion input fokussieren muss
        - man kann noch einen Schritt weiter gehen und sagen, man solle sich ausschließlich auf den Input konzentrieren und mehr mit VDVAEs machen (oder ähnlichen Algorithmen die nicht so viel halluzinieren)
    - Andere Image generation Algorithms (innerhalb des brain-diffuser Algorithm, also versatile diffusion austauschen)
    - Theoretisch sind viele Contexts möglich bei dem versatile diffusion approach, mit einer weiterentwicklung könnte man so dann auch je nach image das regeneriert werden soll ganz unterschiedlichen kontext geben (das schreibe ich auch schon teilweise beim aicap Experiment)
    - Generell kann man ja schauen, wie (ja wie nur, ich hab den Satz nicht fertiggeschrieben...)
    - ICNN mit clip ginge ja theoretisch auch

