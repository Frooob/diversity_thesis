\chapter{Discussion}
% https://www.scribbr.com/dissertation/discussion/

% Summary: A brief recap of your key results
% Interpretations: What do your results mean?
% Implications: Why do your results matter?
% Limitations: What canâ€™t your results tell us?
% Recommendations: Avenues for further studies or 

In this work, three experiments were performed to investigate the influence of diversity on image reconstruction from brain activity. The two algorithms iCNN by Shen et al.~\cite{shenDeepImageReconstruction2019} and in particular the Brain-Diffuser by Ozcelik et al.~\cite{ozcelikNaturalSceneReconstruction2023} were investigated in more detail. First, the individual results are briefly summarised to then open up the discussion for the broad context \rThree{of data diversity and output dimension collapse}. In the first experiment, an attempt was made to create subsamples of the training data that maximised diversity at different levels (low-level to high-level features) and tested against a random selection of the training data. For the iCNN in particular, a diversity-based selection of training data slightly improved the reconstruction for the natural test images, but the reconstruction of the artificial shapes was slightly worsened by the diversity-based subselection, indicating a deterioration of the OOD generalisation. A more detailed investigation, using more monotonous or heterogeneous subsamples for training, revealed important findings: on the one hand, the Brain-Diffuser may be very strongly influenced by the monotonous images (or its performance in reconstructing the artificial shapes was largely based on the monotonous test images). On the other hand, the iCNN appeared to have a slight bias towards reconstructing images with single objects in the foreground and little detail in the background, such as the artificial shapes. In the second experiment, the influence of the CLIP text embeddings on the Brain-Diffuser reconstruction was analysed. AI was used to create additional captions for the training images, focusing on either low-level or high-level features of the images. These captions were used to train the CLIP text translator and subsequently create reconstructions with the Brain-Diffuser. Particularly at high levels of mixing, i.e.\ when the CLIP Text features had a stronger influence on the reconstruction, it was shown that a focus on the low-level features ensured a \rThree{more accurate} reconstruction of the artificial images. At the same time, however, the performance of the natural test images was reduced. Finding a setting that improves both OOD generalisation (in the form of artificial images) and the performance of congruent test images therefore seems difficult. \rTwo{These results} imply that focusing on low-level vs.\ high-level features during training may also favour corresponding reconstructions. In the third experiment, the influence of the CLIP vision embeddings on the reconstruction of the Brain-Diffuser was further investigated. Backpropagation was used to craft perturbations in the training images, which are barely visible to the naked eye, but strongly alter the meaning of the CLIP vision embeddings extracted from them. The information from the captions could thus be added to the images in the form of `semantic noise'. The CLIP Vision translator was trained on these perturbed images, and reconstructions were again produced using the Brain-Diffuser. It was found that the reconstructions could not be improved for the natural test images by focusing on the semantic meaning, nor could they be improved for the artificial shapes by reducing the semantic meaning of the images. In fact, performance deteriorated in both conditions. This raised the question of whether a perturbation in which the modified images had to be similar to the originals was necessary at all, and whether modifying the CLIP vision features directly might be a more targeted and effective way of modifying the semantic meaning of the images.

The results in this paper largely support those of Shirakawa et al.~\cite{shirakawaSpuriousReconstructionBrain2024}. It could be reproduced that the diffusion-based Brain-Diffuser does not perform particularly  with artificial shapes, and that the hallucinations of the diffusion process become visible strongly. As Shirakawa et al.\ also showed, the iCNN performed better for this test dataset. These results suggest that the reconstruction algorithm also has a strong influence on the generalisation ability of the reconstructions. Regarding the output dimension collapse, the experiments with the monotonic dropout and the low-level labels yielded interesting results: almost all of the performance of the Brain-Diffuser with the artificial shapes seems to be due to the monotonic training images. This can be interpreted to mean that the `monotone background' dimension in the training data is essential for these test images. This supports the results of the simulation study by Shirakawa et al.\ in the sense that training data in the right dimensions is needed to enable general reconstructions. The results with the low-level labels can be interpreted in the same way; a focus on low-level labels can also support the low-level reconstructions (artificial shapes). However, in both cases, due to the fact that no new data could be collected for this work, the performance on the natural test data decreased at the same time as the performance on artificial shapes increased. In order to gain further insight into whether the overall performance for all test data can be improved with the findings of this work, it is important to create a new training data set that is designed specifically by regarding relevant image dimensions. Unfortunately, the question of which dimensions are fundamentally important for learning the reconstruction of the entire possible image space remains unanswered. The low-level vs.\ high-level and monotonic vs.\ heterogeneous dimensions analysed in this study are by no means the only possible dimensions that could be used. Other dimensions like colors, object categories, object locations, rotation and others that could possibly not even be described by natural language might also be suited \rThree{more appropriately} as the fundamental dimensions that span the image space. In regard to OOD generalisation, the results of this study suggest that with the current translation method, it would presumably be difficult to generalise to dimensions that do not appear in the training data at all. The result of the diversity-based dropout is particularly interesting. In some cases, the iCNN performed better with a diversity-based subsample than a random subsample. This is a small indication that greater data diversity is able to improve generalisation (and thus presumably also helps with OOD generalisation). On the other hand, the fact that diversity-based dropout had no effect compared to the random dropout baseline for the Brain-Diffuser suggests that true OOD generalisation is difficult with this algorithm. Only by explicitly adding training images with monotonous backgrounds (which are closer to the distribution of the artificial shapes than the heterogeneous images) could the artificial shapes be reconstructed with higher accuracy. This suggests that the effect of training images close to the distribution of the artificial shapes was significantly greater in this case than simply diversifying the training images.

In addition to the results directly related to the research question of output dimension collapse and the OOD generaliser, this work also produced other results that are interesting in the overall context of image reconstruction from brain activity. The different performance criteria of the reconstructions could be examined in more detail in the three experiments under different occasions. PixelCorrelation sometimes produced questionable results, e.g.\ when increasing the CLIP text mixing parameter also led to an improvement in PixelCorrelation for shuffled captions (i.e.\ a strengthening of an actual negative influence). In another case, the perturbations slightly worsened the reconstruction results (measured with DreamSim, CLIP accuracy and checked by qualitative evaluation of the reconstructed images), but PixelCorrelation failed to detect any difference between the reconstructed images. On the other hand, CLIP accuracy was very poor at detecting differences in the reconstruction of artificial shapes and was only slightly above random level (which is plausible since there is little semantic information for the CLIP embeddings in the artificial shapes). In the course of this work, DreamSim performed well as a mid-level metric in the sense that it reliably detected changes in the reconstruction quality of both natural test images and artificial shapes.

In addition to the evaluation criteria, the work also provided further insights into the reconstruction algorithms used. In particular, the Brain-Diffuser was examined in detail in the three experiments. The fact that shuffled captions had no effect on the results of the Brain-Diffuser (with the default mixing of 0.4) raises initial questions about how well the prediction of semantic features actually works with the Brain-Diffuser. The identification accuracy of the CLIP text features was very high for the natural test images, and yet the reconstruction was worse at high mixing (measured with the CLIP-accuracy). This is an indication that the semantic information could not be translated very well, despite the high identification accuracy. It stands to reason that the good prediction of semantic categories (e.g. the exact animal in an image) that Ozcelik et al.~\cite{ozcelikNaturalSceneReconstruction2023} found was mainly due to the overlap of training and test data in the NSD dataset (as Shirakawa et al.~\cite{shirakawaSpuriousReconstructionBrain2024} were also able to show). Qualitatively, only very rough semantic categories (such as `living object' or `vehicle') could be recognised in the Deeprecon dataset which has a significantly lower train/test overlap concerning semantic categories. Also, the fact that the performance of predicting artificial shapes using CLIP Vision features was completely independent of the size of the training dataset (see results for random subsampling) casts doubt on whether images such as the artificial shapes can be properly encoded using CLIP Vision features at all. All in all, the results of this work do not present the generalisation capabilities of the Brain-Diffuser in the most favourable light. 


\rThree{It should be further investigated, which parts of the Brain-Diffuser account for the shown reconstruction performance.} Thus it can be examined how the performance of the complete Brain-Diffusers (using versatile diffusion) differs from its intermediate results of the VDVAE, and whether the Brain-Diffuser could be simply performing a reconstruction with the VDVAE + natural looking hallucination with very limited semantic information (at least if there is not a big overlap in train and test data). The fact, that the the Brain-Diffuser showed better baseline results for the natural test images (measured by CLIP-accuracy) than the iCNN could therefore also be explained by the fact, that the diffusion based hallucinations look more natural to the CLIP Vision encoder than the results of the iCNN.\@ Therefore the CLIP-accuracy favors the Brain-Diffuser only because of the fact, that the created images look more natural, but not because it is better at predicting the actual semantic content. Also, the iCNN needs to be critically examined, as it seems to be particularly suited for predicting salient objects in the center of an image. Whether this bias is a consequence of the ImageNet~\cite{dengImageNetLargescaleHierarchical2009} dataset used to pre-train the iCNN (i.e.\ it is hardly possible to perform different reconstructions with it), or possibly even a logical consequence of the nature of human attention, needs further investigation.


\section{Future directions}

Some open questions remain for the future. In particular, whether there really is a set of fundamental dimensions that, as shown in as simulation study by Shirakawa et al.~\cite{shirakawaSpuriousReconstructionBrain2024}, is able to greatly improve the generalisation of brain activity translation (and hence reconstruction) with linear increase in training data. This raises the question of what exactly these fundamental dimensions might be. In this work, the dimensions low-level vs high-level information were mainly investigated. \rTwo{This} need not be the only approach. Further approaches in this direction could try to use the concept of intrinsic dimensionality~\cite{bennettIntrinsicDimensionalitySignal1969} as the minimum number of dimensions needed to represent all data. For example, initial research into intrinsic dimensionality showed that all ImageNet images could be covered by about 20 dimensions~\cite{popeIntrinsicDimensionImages2021}. In further studies, such a method could be used to select training data so that a subset of images is used that most closely reflects the intrinsic dimensions of a large dataset such as ImageNet. If new data can be collected with the MRI, one would also not be limited to working with subsets of training data, but can also show subjects entirely new images. For example, with a core-set approach~\cite{nguyenDatasetDistillationInfinitely2021,wangDatasetDistillation2018}, the intrinsic dimensionality of the training dataset could be preserved with a drastically reduced dataset containing artificially created images. Such approaches will still be limited in that they only represent the intrinsic dimensionality of a finite dataset (i.e. no generalisation over the entire possible image space can be sought). \rThree{This would not necessarily if the dataset is big enough:} Modern automated data mining methods make it possible to generate extremely large base datasets whose distillation can already cover a large part of all possible images. 

However, it is not enough to focus only on the images (or in other words the features that are to be predicted) in the training dataset. It is equally important to focus also on the diversity of brain activity. This is because there is no guarantee that maximising diversity at the feature level will also maximise diversity in brain activity. Just because an image is a perfect representative of one of the intrinsic dimensions of the image dataset after image distillation does not necessarily mean that this image will trigger the corresponding activity in the brain. For example, an image that is a perfect representative of the class `dog' at the feature level could end up looking very different from a dog for a human and therefore not trigger the activity in the brain that seeing a dog normally triggers. For this reason, it would be important for future research to investigate the brain activity and see if the findings from intrinsic dimensions and data distillation also transfer to brain activity. In a future experiment, both levels could also be combined by looking at  a possible trade-off in a dataset that has high diversity at both the image level and the brain activity level. Such a training dataset could be especially suited for subsequent reconstruction. It is also not necessarily to record new brain data for this study and could be validated using previously recorded data.

In addition to the approaches that  address diversity and generalisation, there are other areas where further insights are needed. On the one hand, the results of this work could be replicated with other datasets, such as THINGS~\cite{hebartTHINGSdataMultimodalCollection2023}, NSD~\cite{allenMassive7TFMRI2022}. As shown by Shirakawa et al.~\cite{shirakawaSpuriousReconstructionBrain2024}, alternative splitting or holdout of clusters in the training dataset can also be used for investigating OOD generalisation for such datasets with high redundancy between training and test data. In addition, a new version of the NSD dataset~\cite{gifford7TFMRIDataset2025} has recently been published which deliberately contains OOD test samples that are conceptually similar to the artificial shapes of the Deeprecon dataset. In this work, the activations of all recorded voxels in the visual system were always used for reconstructions. To further analyse the results, it would be interesting to separate the activity according to different brain regions (as in \cite{horikawaGenericDecodingSeen2017}). In particular, it could be investigated whether the procedures that focus on low-level features (e.g. low-level AI captions) are also more likely to be represented by low-level brain structures (and vice versa for high-level structures).It is also possible to partially adapt the existing reconstruction method or replace it completely with other algorithms. Instead of linear ridge regression, other translation algorithms could be used. In particular, if a dataset with a larger number of training samples is used and fewer features need to be predicted (e.g. in the form of a different reconstruction algorithm), it might be possible to use non-linear translators and thus include interactions between individual voxels (or underlying neurons) in the translation. In addition to the translator, the reconstruction method can also be adapted. Samuel et al.~\cite{samuelGeneratingImagesRare2024} show that rare concepts can be reconstructed better with diffusion algorithms, especially if the diffusion seed is well chosen; these findings could possibly be applied to brain activity reconstruction. Following this logic, the VDVAE input of the versatile diffusion algorithm could be given more weight by the Brain-Diffuser when it detects that the image to be reconstructed is not well embedded in the CLIP space (e.g. when low naturalness is detected). The versatile diffusion part can also be further modified so that several different contexts can be included in the diffusion (e.g. a low-level context trained with low-level labels and a high-level context from high-level labels). Also, as Shen et al.~\cite{shenDeepImageReconstruction2019} showed, diffusion is not absolutely necessary for reconstruction. An iterative algorithm such as iCNN could also be applied directly to the predicted features from the CLIP space to reconstruct the images. In addition, auto-regressive transformers such as Janus-Pro~\cite{chenJanusproUnifiedMultimodal2025} also look like a promising method for performing image reconstruction. In addition to the Brain-Diffuser and iCNN, there are other end2end algorithms that translate brain activity into reconstructed images. Mindeye2~\cite{scottiMindEye2SharedSubjectModels2024}, which also allows training data to be shared between subjects for fine-tuning, looks particularly promising.


\section{Conclusion}
% https://www.scribbr.com/dissertation/write-conclusion/
% Clearly state the answer to your main research question
% Summarize and reflect on your research process
% Make recommendations for future work on your thesis or dissertation topic
% Show what new knowledge you have contributed to your field
% Wrap up your thesis or dissertation


This work has addressed the question of how the diversity of training data affects the decoding of brain activity for image reconstruction. The work is largely based on the findings of Shirakawa et al.~\cite{shirakawaSpuriousReconstructionBrain2024}, who found that well-known methods for image reconstruction from brain activity, such as Brain-Diffuser~\cite{ozcelikNaturalSceneReconstruction2023}, have great difficulties in reconstructing concepts that are missing in the training data. Shirakawa et al.\ postulated that high diversity in the training data may help with this generalisation. In this work, images were reconstructed from fMRI activity (recorded in a previous study)  and evaluated in an empirical study with three different experiments. In each of the three experiments, different methods were used to vary the diversity of the training data. Particular emphasis was placed on distinguishing between low-level feature diversity in the training data (i.e.\ different colours and shapes in the training images) and high-level feature diversity (i.e.\ different semantic concepts). Both the algorithms iCNN from Shen et al.~\cite{shenDeepImageReconstruction2019} and Brain-Diffuser~\cite{ozcelikNaturalSceneReconstruction2023} are used in the experiments.


The results of Shirakawa et al.\ were largely confirmed. In particular, we could confirm that the Brain-Diffuser showed only poor reconstruction performance on out-of-distribution (OOD) samples. The other results of the experiments in this study were somewhat heterogeneous. It was shown that a translator trained with a focus on low-level features in the training data was able to achieve better results for OOD samples with little semantic content (in the artificial shapes test dataset from Horikawa et al.~\cite{horikawaGenericDecodingSeen2017}) and at the same time, focusing on high-level features in the training data led to more accurate reconstruction of natural test data with semantic meaning (e.g., natural objects such as animals, plants, or vehicles). It was also shown to some extent that the performance of reconstruction can be improved by specifically maximising diversity in the training data, but the question of whether generalisation to OOD samples can be specifically improved by diverse training data remained unresolved due to heterogeneous results. A stronger effect than purely maximising the diversity of the training data seems to be achieved when similar concepts are present in the training and test data (e.g. monotonous training images for artificial shape reconstruction). The reconstruction was most successful when the possible dimensions of the test data (e.g. homogeneous vs. heterogeneous images in our study) are present in the training data. This is also consistent with the findings of Shirakawa et al.\ who postulated, that in order to generalize to many different image concepts, it is necessary to include training data, that spans the dimensions of the test data.

\rTwo{An} important question that remains for the future is what these dimensions should be according to which the training data must be selected to enable general image reconstruction. To answer this question, future research could select a training dataset by maximising the intrinsic dimensionality~\cite{popeIntrinsicDimensionImages2021}. In addition, insights from dataset distillation~\cite{wangDatasetDistillation2018} could be used to create (comparatively small) training datasets that maximise the intrinsic dimensionality diversity of the entire (very large, preferably universal) image dataset. However, it should be noted that this approach only refers to the diversity of the features in the training data set, while the predictor side (the brain activity) is still left out. In the future, it will also be important to ensure that maximising diversity at the feature level also generates diverse activation in the brain activity, in order to be able to read as many brain states as possible and get a universal understanding of how brain activity can be encoded more clearly.
