@article{1056489,
  title = {Least Squares Quantization in {{PCM}}},
  author = {Lloyd, S.},
  year = {1982},
  journal = {IEEE Transactions on Information Theory},
  volume = {28},
  number = {2},
  pages = {129--137},
  doi = {10.1109/TIT.1982.1056489}
}

@article{ahmedRecentReviewImage2015,
  title = {Recent Review on Image Clustering},
  author = {Ahmed, Nasir},
  year = {2015},
  month = nov,
  journal = {IET Image Processing},
  volume = {9},
  number = {11},
  pages = {1020--1032},
  issn = {1751-9667, 1751-9667},
  doi = {10.1049/iet-ipr.2014.0885},
  urldate = {2024-11-26},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {/Users/matt/Zotero/storage/UHHBU5PA/Ahmed - 2015 - Recent review on image clustering.pdf}
}

@article{allenMassive7TFMRI2022,
  title = {A Massive {{7T fMRI}} Dataset to Bridge Cognitive Neuroscience and Artificial Intelligence},
  author = {Allen, Emily J. and {St-Yves}, Ghislain and Wu, Yihan and Breedlove, Jesse L. and Prince, Jacob S. and Dowdle, Logan T. and Nau, Matthias and Caron, Brad and Pestilli, Franco and Charest, Ian and Hutchinson, J. Benjamin and Naselaris, Thomas and Kay, Kendrick},
  year = {2022},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {25},
  number = {1},
  pages = {116--126},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-021-00962-x},
  urldate = {2025-01-21},
  langid = {english},
  file = {/Users/matt/Zotero/storage/VRE8CBJ4/Allen et al. - 2022 - A massive 7T fMRI dataset to bridge cognitive neur.pdf}
}

@incollection{ariaVivoElectrophysiology2020,
  title = {In Vivo Electrophysiology},
  booktitle = {Electrophysiology {{Measurements}} for {{Studying Neural Interfaces}}},
  author = {Aria, Mohammad M.},
  year = {2020},
  pages = {91--104},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-12-817070-0.00004-X},
  urldate = {2024-02-20},
  isbn = {978-0-12-817070-0},
  langid = {english},
  file = {/Users/matt/Zotero/storage/CSIWGD3K/Aria - 2020 - In vivo electrophysiology.pdf}
}

@article{baiSurveyAutomaticImage2018,
  title = {A Survey on Automatic Image Caption Generation},
  author = {Bai, Shuang and An, Shan},
  year = {2018},
  month = oct,
  journal = {Neurocomputing},
  volume = {311},
  pages = {291--304},
  issn = {09252312},
  doi = {10.1016/j.neucom.2018.05.080},
  urldate = {2025-03-04},
  langid = {english},
  file = {/Users/matt/Zotero/storage/ABQMC8E5/Bai and An - 2018 - A survey on automatic image caption generation.pdf}
}

@article{bergerWirelessRecordingUnrestrained2020,
  title = {Wireless Recording from Unrestrained Monkeys Reveals Motor Goal Encoding beyond Immediate Reach in Frontoparietal Cortex},
  author = {Berger, Michael and Agha, Naubahar Shahryar and Gail, Alexander},
  editor = {Gold, Joshua I and Pesaran, Bijan and Santacruz, Samantha R},
  year = {2020},
  month = may,
  journal = {eLife},
  volume = {9},
  pages = {e51322},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.51322},
  urldate = {2024-02-02},
  abstract = {System neuroscience of motor cognition regarding the space beyond immediate reach mandates free, yet experimentally controlled movements. We present an experimental environment (Reach Cage) and a versatile visuo-haptic interaction system (MaCaQuE) for investigating goal-directed whole-body movements of unrestrained monkeys. Two rhesus monkeys conducted instructed walk-and-reach movements towards targets flexibly positioned in the cage. We tracked 3D multi-joint arm and head movements using markerless motion capture. Movements show small trial-to-trial variability despite being unrestrained. We wirelessly recorded 192 broad-band neural signals from three cortical sensorimotor areas simultaneously. Single unit activity is selective for different reach and walk-and-reach movements. Walk-and-reach targets could be decoded from premotor and parietal but not motor cortical activity during movement planning. The Reach Cage allows systems-level sensorimotor neuroscience studies with full-body movements in a configurable 3D spatial setting with unrestrained monkeys. We conclude that the primate frontoparietal network encodes reach goals beyond immediate reach during movement planning.},
  keywords = {arm movements,motion capture,motor cortex,parietal cortex,premotor cortex,wireless neurophysiology},
  file = {/Users/matt/Zotero/storage/MGMIS7NY/Berger et al. - 2020 - Wireless recording from unrestrained monkeys revea.pdf}
}

@misc{birodkarSemanticRedundanciesImageClassification2019,
  title = {Semantic {{Redundancies}} in {{Image-Classification Datasets}}: {{The}} 10\% {{You Don}}'t {{Need}}},
  shorttitle = {Semantic {{Redundancies}} in {{Image-Classification Datasets}}},
  author = {Birodkar, Vighnesh and Mobahi, Hossein and Bengio, Samy},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1901.11409},
  urldate = {2024-11-18},
  abstract = {Large datasets have been crucial to the success of deep learning models in the recent years, which keep performing better as they are trained with more labelled data. While there have been sustained efforts to make these models more data-efficient, the potential benefit of understanding the data itself, is largely untapped. Specifically, focusing on object recognition tasks, we wonder if for common benchmark datasets we can do better than random subsets of the data and find a subset that can generalize on par with the full dataset when trained on. To our knowledge, this is the first result that can find notable redundancies in CIFAR-10 and ImageNet datasets (at least 10\%). Interestingly, we observe semantic correlations between required and redundant images. We hope that our findings can motivate further research into identifying additional redundancies and exploiting them for more efficient training or data-collection.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/Users/matt/Zotero/storage/77RLGCST/Birodkar et al. - 2019 - Semantic Redundancies in Image-Classification Data.pdf}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-31073-2},
  langid = {english},
  lccn = {Q327 .B52 2006},
  keywords = {Machine learning,Pattern perception},
  file = {/Users/matt/Zotero/storage/V6QBMMNE/Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@inproceedings{buccinoIndependentComponentAnalysis2018,
  title = {Independent {{Component Analysis}} for {{Fully Automated Multi-Electrode Array Spike Sorting}}},
  booktitle = {2018 40th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  author = {Buccino, Alessio P. and Hagen, Espen and Einevoll, Gaute T. and H{\"a}fliger, Philipp D. and Cauwenberghs, Gert},
  year = {2018},
  month = jul,
  pages = {2627--2630},
  issn = {1558-4615},
  doi = {10.1109/EMBC.2018.8512788},
  urldate = {2024-02-22},
  abstract = {In neural electrophysiology, spike sorting allows to separate different neurons from extracellularly measured recordings. It is an essential processing step in order to understand neural activity and it is an unsupervised problem in nature, since no ground truth information is available. There are several available spike sorting packages, but many of them require a manual intervention to curate the results, which makes the process time consuming and hard to reproduce. Here, we focus on high-density Multi-Electrode Array (MEA) recordings and we present a fully automated pipeline based on Independent Component Analysis (ICA). While ICA has been previously investigated for spike sorting, it has never been compared with fully automated state-of-the-art algorithms. We use realistic simulated datasets to compare the spike sorting performance in terms of complexity, signal-to-noise ratio, and recording duration. We show that an ICA-based fully automated spike sorting approach can be a viable alternative approach due to its precision and robustness, but it needs to be optimized for time constraints and requires sufficient density of electrodes to cover active neurons in the proximity of the MEA.},
  keywords = {Biological system modeling,Complexity theory,Neurons,Noise level,Sensitivity,Sorting},
  file = {/Users/matt/Zotero/storage/INSG8BWI/Buccino et al. - 2018 - Independent Component Analysis for Fully Automated.pdf;/Users/matt/Zotero/storage/RMJ6QJ2P/8512788.html}
}

@article{buccinoSpikeInterfaceUnifiedFramework2020,
  title = {{{SpikeInterface}}, a Unified Framework for Spike Sorting},
  author = {Buccino, Alessio P and Hurwitz, Cole L and Garcia, Samuel and Magland, Jeremy and Siegle, Joshua H and Hurwitz, Roger and Hennig, Matthias H},
  editor = {Colgin, Laura L and Gr{\"u}n, Sonja and Kloosterman, Fabian},
  year = {2020},
  month = nov,
  journal = {eLife},
  volume = {9},
  pages = {e61834},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.61834},
  urldate = {2024-03-04},
  abstract = {Much development has been directed toward improving the performance and automation of spike sorting. This continuous development, while essential, has contributed to an over-saturation of new, incompatible tools that hinders rigorous benchmarking and complicates reproducible analysis. To address these limitations, we developed SpikeInterface, a Python framework designed to unify preexisting spike sorting technologies into a single codebase and to facilitate straightforward comparison and adoption of different approaches. With a few lines of code, researchers can reproducibly run, compare, and benchmark most modern spike sorting algorithms; pre-process, post-process, and visualize extracellular datasets; validate, curate, and export sorting outputs; and more. In this paper, we provide an overview of SpikeInterface and, with applications to real and simulated datasets, demonstrate how it can be utilized to reduce the burden of manual curation and to more comprehensively benchmark automated spike sorters.},
  keywords = {extracellular recordings,open-source software,python,reproducibility,spike sorting},
  file = {/Users/matt/Zotero/storage/JGGWX9DH/Buccino et al. - 2020 - SpikeInterface, a unified framework for spike sort.pdf}
}

@article{buccinoSpikeSortingNew2022,
  title = {Spike Sorting: New Trends and Challenges of the Era of High-Density Probes},
  shorttitle = {Spike Sorting},
  author = {Buccino, Alessio P. and Garcia, Samuel and Yger, Pierre},
  year = {2022},
  month = may,
  journal = {Progress in Biomedical Engineering},
  volume = {4},
  number = {2},
  pages = {022005},
  publisher = {IOP Publishing},
  issn = {2516-1091},
  doi = {10.1088/2516-1091/ac6b96},
  urldate = {2024-02-22},
  abstract = {Recording from a large neuronal population of neurons is a crucial challenge to unravel how information is processed by the brain. In this review, we highlight the recent advances made in the field of `spike sorting', which is arguably a very essential processing step to extract neuronal activity from extracellular recordings. More specifically, we target the challenges faced by newly manufactured high-density multi-electrode array devices (HD-MEA), e.g. Neuropixels probes. Among them, we cover in depth the prominent problem of drifts (movements of the neurons with respect to the recording devices) and the current solutions to circumscribe it. In addition, we also review recent contributions making use of deep learning approaches for spike sorting, highlighting their advantages and disadvantages. Next, we highlight efforts and advances in unifying, validating, and benchmarking spike sorting tools. Finally, we discuss the spike sorting field in terms of its open and unsolved challenges, specifically regarding scalability and reproducibility. We conclude by providing our personal view on the future of spike sorting, calling for a community-based development and validation of spike sorting algorithms and fully automated, cloud-based spike sorting solutions for the neuroscience community.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/FC256QH8/Buccino et al. - 2022 - Spike sorting new trends and challenges of the er.pdf}
}

@article{canatarOutofDistributionGeneralizationKernel,
  title = {Out-of-{{Distribution Generalization}} in {{Kernel Regression}}},
  author = {Canatar, Abdulkadir and Bordelon, Blake and Pehlevan, Cengiz},
  abstract = {In real word applications, the data generating process for training a machine learning model often differs from what the model encounters in the test stage. Understanding how and whether machine learning models generalize under such distributional shifts remains a theoretical challenge. Here, we study generalization in kernel regression when the training and test distributions are different using the replica method from statistical physics. We derive an analytical formula for the out-of-distribution generalization error applicable to any kernel and real datasets. We identify an overlap matrix that quantifies the mismatch between distributions for a given kernel as a key determinant of generalization performance under distribution shift. Using our analytical expressions we elucidate various generalization phenomena including possible improvement in generalization when there is a mismatch. We develop procedures for optimizing training and test distributions for a given data budget to find best and worst case generalizations under the shift. We present applications of our theory to real and synthetic datasets and for many kernels. We compare results of our theory applied to Neural Tangent Kernel with simulations of wide networks and show agreement. We analyze linear regression in further depth.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/QM83882D/Canatar et al. - Out-of-Distribution Generalization in Kernel Regre.pdf}
}

@article{canatarSpectralBiasTaskmodel2021,
  title = {Spectral Bias and Task-Model Alignment Explain Generalization in Kernel Regression and Infinitely Wide Neural Networks},
  author = {Canatar, Abdulkadir and Bordelon, Blake and Pehlevan, Cengiz},
  year = {2021},
  month = may,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {2914},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-23103-1},
  urldate = {2024-06-07},
  abstract = {Abstract             A theoretical understanding of generalization remains an open problem for many machine learning models, including deep networks where overparameterization leads to better performance, contradicting the conventional wisdom from classical statistics. Here, we investigate generalization error for kernel regression, which, besides being a popular machine learning method, also describes certain infinitely overparameterized neural networks. We use techniques from statistical mechanics to derive an analytical expression for generalization error applicable to any kernel and data distribution. We present applications of our theory to real and synthetic datasets, and for many kernels including those that arise from training deep networks in the infinite-width limit. We elucidate an inductive bias of kernel regression to explain data with simple functions, characterize whether a kernel is compatible with a learning task, and show that more data may impair generalization when noisy or not expressible by the kernel, leading to non-monotonic learning curves with possibly many peaks.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/3XF74247/Canatar et al. - 2021 - Spectral bias and task-model alignment explain gen.pdf}
}

@inproceedings{chang2017deep,
  title = {Deep Adaptive Image Clustering},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Chang, Jianlong and Wang, Lingfeng and Meng, Gaofeng and Xiang, Shiming and Pan, Chunhong},
  year = {2017},
  pages = {5879--5887}
}

@misc{chenExploringNaturalnessAIGenerated2023,
  title = {Exploring the {{Naturalness}} of {{AI-Generated Images}}},
  author = {Chen, Zijian and Sun, Wei and Wu, Haoning and Zhang, Zicheng and Jia, Jun and Ji, Zhongpeng and Sun, Fengyu and Jui, Shangling and Min, Xiongkuo and Zhai, Guangtao and Zhang, Wenjun},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2312.05476},
  urldate = {2025-03-04},
  abstract = {The proliferation of Artificial Intelligence-Generated Images (AGIs) has greatly expanded the Image Naturalness Assessment (INA) problem. Different from early definitions that mainly focus on tone-mapped images with limited distortions (e.g., exposure, contrast, and color reproduction), INA on AI-generated images is especially challenging as it has more diverse contents and could be affected by factors from multiple perspectives, including low-level technical distortions and high-level rationality distortions. In this paper, we take the first step to benchmark and assess the visual naturalness of AI-generated images. First, we construct the AI-Generated Image Naturalness (AGIN) database by conducting a large-scale subjective study to collect human opinions on the overall naturalness as well as perceptions from technical and rationality perspectives. AGIN verifies that naturalness is universally and disparately affected by technical and rationality distortions. Second, we propose the Joint Objective Image Naturalness evaluaTor (JOINT), to automatically predict the naturalness of AGIs that aligns human ratings. Specifically, JOINT imitates human reasoning in naturalness evaluation by jointly learning both technical and rationality features. We demonstrate that JOINT significantly outperforms baselines for providing more subjectively consistent results on naturalness assessment.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/matt/Zotero/storage/4778JGY8/Chen et al. - 2023 - Exploring the Naturalness of AI-Generated Images.pdf}
}

@misc{chenExploringNaturalnessAIGenerated2023a,
  title = {Exploring the {{Naturalness}} of {{AI-Generated Images}}},
  author = {Chen, Zijian and Sun, Wei and Wu, Haoning and Zhang, Zicheng and Jia, Jun and Ji, Zhongpeng and Sun, Fengyu and Jui, Shangling and Min, Xiongkuo and Zhai, Guangtao and Zhang, Wenjun},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2312.05476},
  urldate = {2025-03-05},
  abstract = {The proliferation of Artificial Intelligence-Generated Images (AGIs) has greatly expanded the Image Naturalness Assessment (INA) problem. Different from early definitions that mainly focus on tone-mapped images with limited distortions (e.g., exposure, contrast, and color reproduction), INA on AI-generated images is especially challenging as it has more diverse contents and could be affected by factors from multiple perspectives, including low-level technical distortions and high-level rationality distortions. In this paper, we take the first step to benchmark and assess the visual naturalness of AI-generated images. First, we construct the AI-Generated Image Naturalness (AGIN) database by conducting a large-scale subjective study to collect human opinions on the overall naturalness as well as perceptions from technical and rationality perspectives. AGIN verifies that naturalness is universally and disparately affected by technical and rationality distortions. Second, we propose the Joint Objective Image Naturalness evaluaTor (JOINT), to automatically predict the naturalness of AGIs that aligns human ratings. Specifically, JOINT imitates human reasoning in naturalness evaluation by jointly learning both technical and rationality features. We demonstrate that JOINT significantly outperforms baselines for providing more subjectively consistent results on naturalness assessment.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{chengReconstructingVisualIllusory2023,
  title = {Reconstructing Visual Illusory Experiences from Human Brain Activity},
  author = {Cheng, Fan L. and Horikawa, Tomoyasu and Majima, Kei and Tanaka, Misato and Abdelhack, Mohamed and Aoki, Shuntaro C. and Hirano, Jin and Kamitani, Yukiyasu},
  year = {2023},
  month = nov,
  journal = {Science Advances},
  volume = {9},
  number = {46},
  pages = {eadj3906},
  issn = {2375-2548},
  doi = {10.1126/sciadv.adj3906},
  urldate = {2025-03-12},
  abstract = {Visual illusions provide valuable insights into the brain's interpretation of the world given sensory inputs. However, the precise manner in which brain activity translates into illusory experiences remains largely unknown. Here, we leverage a brain decoding technique combined with deep neural network (DNN) representations to reconstruct illusory percepts as images from brain activity. The reconstruction model was trained on natural images to establish a link between brain activity and perceptual features and then tested on two types of illusions: illusory lines and neon color spreading. Reconstructions revealed lines and colors consistent with illusory experiences, which varied across the source visual cortical areas. This framework offers a way to materialize subjective experiences, shedding light on the brain's internal representations of the world.           ,              Images generated from brain activity mirror illusory experiences, revealing neural representations behind subjective perception.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/4MF37QDE/Cheng et al. - 2023 - Reconstructing visual illusory experiences from hu.pdf}
}

@misc{childVeryDeepVAEs2020,
  title = {Very {{Deep VAEs Generalize Autoregressive Models}} and {{Can Outperform Them}} on {{Images}}},
  author = {Child, Rewon},
  year = {2020},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2011.10650},
  urldate = {2025-01-22},
  abstract = {We present a hierarchical VAE that, for the first time, generates samples quickly while outperforming the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@misc{chittaTrainingDataSubset2020,
  title = {Training {{Data Subset Search}} with {{Ensemble Active Learning}}},
  author = {Chitta, Kashyap and Alvarez, Jose M. and Haussmann, Elmar and Farabet, Clement},
  year = {2020},
  month = nov,
  number = {arXiv:1905.12737},
  eprint = {1905.12737},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-15},
  abstract = {Deep Neural Networks (DNNs) often rely on very large datasets for training. Given the large size of such datasets, it is conceivable that they contain certain samples that either do not contribute or negatively impact the DNN's performance. If there is a large number of such samples, subsampling the training dataset in a way that removes them could provide an effective solution to both improve performance and reduce training time. In this paper, we propose an approach called Active Dataset Subsampling (ADS), to identify favorable subsets within a dataset for training using ensemble based uncertainty estimation. When applied to three image classification benchmarks (CIFAR-10, CIFAR-100 and ImageNet) we find that there are low uncertainty subsets, which can be as large as 50\% of the full dataset, that negatively impact performance. These subsets are identified and removed with ADS. We demonstrate that datasets obtained using ADS with a lightweight ResNet18 ensemble remain effective when used to train deeper models like ResNet-101. Our results provide strong empirical evidence that using all the available data for training can hurt performance on large scale vision tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/matt/Zotero/storage/3U7UW5SJ/Chitta et al. - 2020 - Training Data Subset Search with Ensemble Active L.pdf}
}

@article{chungFullyAutomatedApproach2017,
  title = {A {{Fully Automated Approach}} to {{Spike Sorting}}},
  author = {Chung, Jason E. and Magland, Jeremy F. and Barnett, Alex H. and Tolosa, Vanessa M. and Tooker, Angela C. and Lee, Kye Y. and Shah, Kedar G. and Felix, Sarah H. and Frank, Loren M. and Greengard, Leslie F.},
  year = {2017},
  month = sep,
  journal = {Neuron},
  volume = {95},
  number = {6},
  pages = {1381-1394.e6},
  issn = {08966273},
  doi = {10.1016/j.neuron.2017.08.030},
  urldate = {2024-02-22},
  abstract = {Understanding the detailed dynamics of neuronal networks will require the simultaneous measurement of spike trains from hundreds of neurons (or more). Currently, approaches to extracting spike times and labels from raw data are time consuming, lack standardization, and involve manual intervention, making it difficult to maintain data provenance and assess the quality of scientific results. Here, we describe an automated clustering approach and associated software package that addresses these problems and provides novel cluster quality metrics. We show that our approach has accuracy comparable to or exceeding that achieved using manual or semi-manual techniques with desktop central processing unit (CPU) runtimes faster than acquisition time for up to hundreds of electrodes. Moreover, a single choice of parameters in the algorithm is effective for a variety of electrode geometries and across multiple brain regions. This algorithm has the potential to enable reproducible and automated spike sorting of larger scale recordings than is currently possible.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/FBTQPNF5/Chung et al. - 2017 - A Fully Automated Approach to Spike Sorting.pdf}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
  year = {2009},
  month = jun,
  pages = {248--255},
  publisher = {IEEE},
  address = {Miami, FL},
  doi = {10.1109/CVPR.2009.5206848},
  urldate = {2025-01-21},
  isbn = {978-1-4244-3992-8}
}

@article{diggelmannAutomaticSpikeSorting2018,
  title = {Automatic Spike Sorting for High-Density Microelectrode Arrays},
  author = {Diggelmann, Roland and Fiscella, Michele and Hierlemann, Andreas and Franke, Felix},
  year = {2018},
  month = dec,
  journal = {Journal of Neurophysiology},
  volume = {120},
  number = {6},
  pages = {3155--3171},
  publisher = {American Physiological Society},
  issn = {0022-3077},
  doi = {10.1152/jn.00803.2017},
  urldate = {2024-02-22},
  abstract = {High-density microelectrode arrays can be used to record extracellular action potentials from hundreds to thousands of neurons simultaneously. Efficient spike sorters must be developed to cope with such large data volumes. Most existing spike sorting methods for single electrodes or small multielectrodes, however, suffer from the ``curse of dimensionality'' and cannot be directly applied to recordings with hundreds of electrodes. This holds particularly true for the standard reference spike sorting algorithm, principal component analysis-based feature extraction, followed by k-means or expectation maximization clustering, against which most spike sorters are evaluated. We present a spike sorting algorithm that circumvents the dimensionality problem by sorting local groups of electrodes independently with classical spike sorting approaches. It is scalable to any number of recording electrodes and well suited for parallel computing. The combination of data prewhitening before the principal component analysis-based extraction and a parameter-free clustering algorithm obviated the need for parameter adjustments. We evaluated its performance using surrogate data in which we systematically varied spike amplitudes and spike rates and that were generated by inserting template spikes into the voltage traces of real recordings. In a direct comparison, our algorithm could compete with existing state-of-the-art spike sorters in terms of sensitivity and precision, while parameter adjustment or manual cluster curation was not required. NEW \& NOTEWORTHY We present an automatic spike sorting algorithm that combines three strategies to scale classical spike sorting techniques for high-density microelectrode arrays: 1) splitting the recording electrodes into small groups and sorting them independently; 2) clustering a subset of spikes and classifying the rest to limit computation time; and 3) prewhitening the spike waveforms to enable the use of parameter-free clustering. Finally, we combined these strategies into an automatic spike sorter that is competitive with state-of-the-art spike sorters.},
  keywords = {HD-MEA surrogate data,high-density microelectrode array,prewhitening,spike sorting},
  file = {/Users/matt/Zotero/storage/L5PDWP6Q/Diggelmann et al. - 2018 - Automatic spike sorting for high-density microelec.pdf}
}

@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.11929},
  urldate = {2025-02-28},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/5ZL2MQHJ/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf;/Users/matt/Zotero/storage/EU4B83VK/2010.html}
}

@misc{ds001506:1.3.1,
  title = {"{{Deep}} Image Reconstruction"},
  author = {Shen, Guohua and Horikawa, Tomoyasu and Majima, Kei and Kamitani, Yukiyasu},
  year = {2020},
  publisher = {OpenNeuro},
  doi = {10.18112/openneuro.ds001506.v1.3.1}
}

@inproceedings{durdovTrainingDatasetPruning2024,
  title = {Training {{Dataset Pruning Algorithm}} with {{Evaluation}} on {{Medical Datasets}}},
  booktitle = {2024 {{International Conference}} on {{Software}}, {{Telecommunications}} and {{Computer Networks}} ({{SoftCOM}})},
  author = {Durdov, Bo{\v z}o and Prvan, Marina and {\v C}oko, Duje and Musi{\'c}, Josip},
  year = {2024},
  month = sep,
  pages = {1--8},
  publisher = {IEEE},
  address = {Split, Croatia},
  doi = {10.23919/SoftCOM62040.2024.10721725},
  urldate = {2024-11-15},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-953-290-138-2},
  file = {/Users/matt/Zotero/storage/7PS9SDJZ/Durdov et al. - 2024 - Training Dataset Pruning Algorithm with Evaluation.pdf}
}

@article{engelFMRIHumanVisual1994,
  title = {{{fMRI}} of Human Visual Cortex},
  author = {Engel, Stephen A. and Rumelhart, David E. and Wandell, Brian A. and Lee, Adrian T. and Glover, Gary H. and Chichilnisky, Eduardo-Jose and Shadlen, Michael N.},
  year = {1994},
  month = jun,
  journal = {Nature},
  volume = {369},
  number = {6481},
  pages = {525--525},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/369525a0},
  urldate = {2025-01-25},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/matt/Zotero/storage/IW3EQ74J/Engel et al. - 1994 - fMRI of human visual cortex.pdf}
}

@article{epsteinCorticalRepresentationLocal1998,
  title = {A Cortical Representation of the Local Visual Environment},
  author = {Epstein, Russell and Kanwisher, Nancy},
  year = {1998},
  month = apr,
  journal = {Nature},
  volume = {392},
  number = {6676},
  pages = {598--601},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/33402},
  urldate = {2025-01-25},
  copyright = {http://www.springer.com/tdm},
  langid = {english}
}

@article{ferreaBCImatMatlabbasedFramework2022,
  title = {{{BCImat}}: A {{Matlab-based}} Framework for {{IntracorticalBrain-Computer Interfaces}} and Their Simulation with an Artificialspiking Neural Network},
  shorttitle = {{{BCImat}}},
  author = {Ferrea, Enrico and Morel, Pierre and Gail, Alexander},
  year = {2022},
  month = jul,
  journal = {Journal of Open Source Software},
  volume = {7},
  number = {75},
  pages = {3956},
  issn = {2475-9066},
  doi = {10.21105/joss.03956},
  urldate = {2024-02-19},
  abstract = {Recent advances in intracortical Brain-Computer Interface (BCI) technology allowed motor disabled patients to partially regain lost motor functions (Aflalo et al., 2015; Ajiboye et al., 2017; Collinger et al., 2013; Hochberg et al., 2012). In these patients, intact neural activity is extracted from motor-related areas of the cerebral cortex via intracortical implanted electrodes and interpreted by a machine-learning algorithm to control a prosthetic device, thereby bypassing dysfunctional corticospinal projections that resulted, for example, from spinal cord lesions. BCIs of this type have been and still are being developed mostly in non-human primate animal models. Additionally, they are also used for basic neuroscientific studies in animals to establish a specific experimentally-controlled transformation between the brain area under investigation and a specific behavior (Koralek et al., 2012; Sadtler et al., 2014), thereby imposing a direct and controllable causal link between brain activity and behavior. The software introduced here allows true online BCI control of a computer cursor based on physiological signals. Importantly, it also allows realistic real-time neural data simulations from artificial spiking neural network (SNN). With this, all algorithms and the control architecture can be tested in silico identical to the physiological experiment.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/VTSMWFNU/Ferrea et al. - 2022 - BCImat a Matlab-based framework for Intracortical.pdf}
}

@article{ferreaStatisticalDeterminantsVisuomotor2022,
  title = {Statistical Determinants of Visuomotor Adaptation along Different Dimensions during Naturalistic {{3D}} Reaches},
  author = {Ferrea, E. and Franke, J. and Morel, P. and Gail, A.},
  year = {2022},
  month = jun,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {10198},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-13866-y},
  urldate = {2024-02-19},
  abstract = {Neurorehabilitation in patients suffering from motor deficits relies on relearning or re-adapting motor skills. Yet our understanding of motor learning is based mostly on results from one or two-dimensional experimental paradigms with highly confined movements. Since everyday movements are conducted in three-dimensional space, it is important to further our understanding about the effect that gravitational forces or perceptual anisotropy might or might not have on motor learning along all different dimensions relative to the body. Here we test how well existing concepts of motor learning generalize to movements in 3D. We ask how a subject's variability in movement planning and sensory perception influences motor adaptation along three different body axes. To extract variability and relate it to adaptation rate, we employed a novel hierarchical two-state space model using Bayesian modeling via Hamiltonian Monte Carlo procedures. Our results show that differences in adaptation rate occur between the coronal, sagittal and horizontal planes and can be explained by the Kalman gain, i.e., a statistically optimal solution integrating planning and sensory information weighted by the inverse of their variability. This indicates that optimal integration theory for error correction holds for 3D movements and explains adaptation rate variation between movements in different planes.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Human behaviour,Motor control,Neuroscience},
  file = {/Users/matt/Zotero/storage/VA8WUTIR/Ferrea et al. - 2022 - Statistical determinants of visuomotor adaptation .pdf}
}

@misc{Fort2021CLIPadversarialstickers,
  title = {Pixels Still Beat Text: {{Attacking}} the {{OpenAI CLIP}} Model with Text Patches and Adversarial Pixel Perturbations},
  author = {Fort, Stanislav},
  year = {2021},
  month = mar
}

@misc{fuDreamSimLearningNew2023,
  title = {{{DreamSim}}: {{Learning New Dimensions}} of {{Human Visual Similarity}} Using {{Synthetic Data}}},
  shorttitle = {{{DreamSim}}},
  author = {Fu, Stephanie and Tamir, Netanel and Sundaram, Shobhita and Chai, Lucy and Zhang, Richard and Dekel, Tali and Isola, Phillip},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2306.09344},
  urldate = {2025-01-26},
  abstract = {Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic content while also being sensitive to color and layout. Notably, despite being trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval and reconstruction tasks. Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks.},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@article{fujiwaraModularEncodingDecoding2013,
  title = {Modular {{Encoding}} and {{Decoding Models Derived}} from {{Bayesian Canonical Correlation Analysis}}},
  author = {Fujiwara, Yusuke and Miyawaki, Yoichi and Kamitani, Yukiyasu},
  year = {2013},
  month = apr,
  journal = {Neural Computation},
  volume = {25},
  number = {4},
  pages = {979--1005},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00423},
  urldate = {2025-03-12},
  abstract = {Neural encoding and decoding provide perspectives for understanding neural representations of sensory inputs. Recent functional magnetic resonance imaging (fMRI) studies have succeeded in building prediction models for encoding and decoding numerous stimuli by representing a complex stimulus as a combination of simple elements. While arbitrary visual images were reconstructed using a modular model that combined the outputs of decoder modules for multiscale local image bases (elements), the shapes of the image bases were heuristically determined. In this work, we propose a method to establish mappings between the stimulus and the brain by automatically extracting modules from measured data. We develop a model based on Bayesian canonical correlation analysis, in which each module is modeled by a latent variable that relates a set of pixels in a visual image to a set of voxels in an fMRI activity pattern. The estimated mapping from a latent variable to pixels can be regarded as an image basis. We show that the model estimates a modular representation with spatially localized multiscale image bases. Further, using the estimated mappings, we derive encoding and decoding models that produce accurate predictions for brain activity and stimulus images. Our approach thus provides a novel means of revealing neural representations of stimuli by automatically extracting modules, which can be used to generate effective prediction models for encoding and decoding.},
  langid = {english}
}

@article{giljaHighperformanceNeuralProsthesis2012,
  title = {A High-Performance Neural Prosthesis Enabled by Control Algorithm Design},
  author = {Gilja, Vikash and Nuyujukian, Paul and Chestek, Cindy A and Cunningham, John P and Yu, Byron M and Fan, Joline M and Churchland, Mark M and Kaufman, Matthew T and Kao, Jonathan C and Ryu, Stephen I and Shenoy, Krishna V},
  year = {2012},
  month = dec,
  journal = {Nature Neuroscience},
  volume = {15},
  number = {12},
  pages = {1752--1757},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3265},
  urldate = {2024-02-02},
  langid = {english},
  file = {/Users/matt/Zotero/storage/BAJXVDRK/Gilja et al. - 2012 - A high-performance neural prosthesis enabled by co.pdf}
}

@misc{goodfellowExplainingHarnessingAdversarial2014,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  year = {2014},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1412.6572},
  urldate = {2025-02-21},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@article{grill-spectorHUMANVISUALCORTEX2004,
  title = {{{THE HUMAN VISUAL CORTEX}}},
  author = {{Grill-Spector}, Kalanit and Malach, Rafael},
  year = {2004},
  month = jul,
  journal = {Annual Review of Neuroscience},
  volume = {27},
  number = {1},
  pages = {649--677},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev.neuro.27.070203.144220},
  urldate = {2025-03-12},
  abstract = {▪ Abstract{\enspace} The discovery and analysis of cortical visual areas is a major accomplishment of visual neuroscience. In the past decade the use of noninvasive functional imaging, particularly functional magnetic resonance imaging (fMRI), has dramatically increased our detailed knowledge of the functional organization of the human visual cortex and its relation to visual perception. The fMRI method offers a major advantage over other techniques applied in neuroscience by providing a large-scale neuroanatomical perspective that stems from its ability to image the entire brain essentially at once. This bird's eye view has the potential to reveal large-scale principles within the very complex plethora of visual areas. Thus, it could arrange the entire constellation of human visual areas in a unified functional organizational framework. Here we review recent findings and methods employed to uncover the functional properties of the human visual cortex focusing on two themes: functional specialization and hierarchical processing.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/CXHMKBTP/Grill-Spector and Malach - 2004 - THE HUMAN VISUAL CORTEX.pdf}
}

@incollection{guoDeepCoreComprehensiveLibrary2022,
  title = {{{DeepCore}}: {{A Comprehensive Library}} for {{Coreset Selection}} in {{Deep Learning}}},
  shorttitle = {{{DeepCore}}},
  booktitle = {Database and {{Expert Systems Applications}}},
  author = {Guo, Chengcheng and Zhao, Bo and Bai, Yanbing},
  editor = {Strauss, Christine and Cuzzocrea, Alfredo and Kotsis, Gabriele and Tjoa, A Min and Khalil, Ismail},
  year = {2022},
  volume = {13426},
  pages = {181--195},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-12423-5_14},
  urldate = {2024-11-18},
  isbn = {978-3-031-12422-8 978-3-031-12423-5},
  langid = {english},
  file = {/Users/matt/Zotero/storage/FJ9GGYZZ/Guo et al. - 2022 - DeepCore A Comprehensive Library for Coreset Sele.pdf}
}

@article{harrisAccuracyTetrodeSpike2000,
  title = {Accuracy of {{Tetrode Spike Separation}} as {{Determined}} by {{Simultaneous Intracellular}} and {{Extracellular Measurements}}},
  author = {Harris, Kenneth D. and Henze, Darrell A. and Csicsvari, Jozsef and Hirase, Hajime and Buzs{\'a}ki, Gy{\"o}rgy},
  year = {2000},
  month = jul,
  journal = {Journal of Neurophysiology},
  volume = {84},
  number = {1},
  pages = {401--414},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.2000.84.1.401},
  urldate = {2024-02-22},
  langid = {english},
  file = {/Users/matt/Zotero/storage/W2V7WSJM/Harris et al. - 2000 - Accuracy of Tetrode Spike Separation as Determined.pdf}
}

@article{hebartTHINGSdataMultimodalCollection2023,
  title = {{{THINGS-data}}, a Multimodal Collection of Large-Scale Datasets for Investigating Object Representations in Human Brain and Behavior},
  author = {Hebart, Martin N and Contier, Oliver and Teichmann, Lina and Rockter, Adam H and Zheng, Charles Y and Kidder, Alexis and Corriveau, Anna and {Vaziri-Pashkam}, Maryam and Baker, Chris I},
  year = {2023},
  month = feb,
  journal = {eLife},
  volume = {12},
  pages = {e82580},
  issn = {2050-084X},
  doi = {10.7554/eLife.82580},
  urldate = {2024-10-17},
  abstract = {Understanding object representations requires a broad, comprehensive sampling of the objects in our visual world with dense measurements of brain activity and behavior. Here, we present THINGS-\-data, a multimodal collection of large-\-scale neuroimaging and behavioral datasets in humans, comprising densely sampled functional MRI and magnetoencephalographic recordings, as well as 4.70 million similarity judgments in response to thousands of photographic images for up to 1,854 object concepts. THINGS-d\- ata is unique in its breadth of richly annotated objects, allowing for testing countless hypotheses at scale while assessing the reproducibility of previous findings. Beyond the unique insights promised by each individual dataset, the multimodality of THINGS-\-data allows combining datasets for a much broader view into object processing than previously possible. Our analyses demonstrate the high quality of the datasets and provide five examples of hypothesis-\- driven and data-\-driven applications. THINGS-d\- ata constitutes the core public release of the THINGS initiative (https://things-initiative.org) for bridging the gap between disciplines and the advancement of cognitive neuroscience.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/4GDH4Y6B/Hebart et al. - 2023 - THINGS-data, a multimodal collection of large-scal.pdf}
}

@article{hernandez-rojasBrainComputerInterfaceControlled2022,
  title = {Brain-{{Computer Interface Controlled Functional Electrical Stimulation}}: {{Evaluation With Healthy Subjects}} and {{Spinal Cord Injury Patients}}},
  shorttitle = {Brain-{{Computer Interface Controlled Functional Electrical Stimulation}}},
  author = {{Hernandez-Rojas}, Luis G. and {Cantillo-Negrete}, Jessica and {Mendoza-Montoya}, Omar and {Carino-Escobar}, Ruben I. and {Leyva-Martinez}, Ismael and {Aguirre-Guemez}, Ana V. and {Barrera-Ortiz}, Aida and {Carrillo-Mora}, Paul and Antelis, Javier M.},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {46834--46852},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3170906},
  urldate = {2024-02-19},
  abstract = {This work presents the design, implementation, and feasibility evaluation of a Motor Imagery (MI) based Brain-Computer Interface (BCI) developed to control a Functional Electrical Stimulation (FES) device. The aim of this system is to assist the upper limb motor recovery of patients with spinal cord injury (SCI). With this BCI-controlled FES system, the user performs open and close MI with either the left or right hand, which if detected is used to provide visual feedback and electroestimulation to muscles in the forearm to perform the corresponding grasping movement. The system was evaluated with seven healthy subjects (HS group) and two SCI patients (SC group) in several experimental sessions across different days. Each experimental session consisted of a training routine devoted to collect calibration EEG data to train the BCI machine learning model, and of a validation routine devoted to validate system in online operation. The online system validation showed an accuracy of the recognition of the MI task that ranged between 78\% and 81\% for HS participants and between 63\% and 93\% for SCI participants. Additionally, the time taken by the BCI system to trigger the FES device ranged between 7.05 and 7.29 s for HS participants and between 8.43 s and 13.91 s for SCI participants. Finally, significant negative correlations were observed ( r=-0.418 , p=0.024 and r=-0.437 , p=0.018 for left and right hand MI conditions, respectively) between the online BCI performance with a quantitative EEG parameter based on event-related desynchronization/synchronization analysis. The results of this work indicate the feasibility of the proposed BCI coupled to a FES device to be used for SCI patients with a moderate level of disability and provides evidence of the functionality of the proposed BCI system in a motor rehabilitation context.},
  keywords = {Brain-computer interface,Brain-computer interfaces,electroencephalography,Electroencephalography,functional electrical stimulation,Grasping,Iron,motor imagery,motor rehabilitation,Software,spinal cord injury,Task analysis,Visualization},
  file = {/Users/matt/Zotero/storage/XTRYYT47/Hernandez-Rojas et al. - 2022 - Brain-Computer Interface Controlled Functional Ele.pdf;/Users/matt/Zotero/storage/GYKWP73S/9764729.html}
}

@article{hoerlRidgeRegressionBiased1970,
  title = {Ridge {{Regression}}: {{Biased Estimation}} for {{Nonorthogonal Problems}}},
  shorttitle = {Ridge {{Regression}}},
  author = {Hoerl, Arthur E. and Kennard, Robert W.},
  year = {1970},
  month = feb,
  journal = {Technometrics},
  volume = {12},
  number = {1},
  pages = {55--67},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.1970.10488634},
  urldate = {2025-01-22},
  langid = {english}
}

@article{horikawaAttentionModulatesNeural2022,
  title = {Attention Modulates Neural Representation to Render Reconstructions According to Subjective Appearance},
  author = {Horikawa, Tomoyasu and Kamitani, Yukiyasu},
  year = {2022},
  month = jan,
  journal = {Communications Biology},
  volume = {5},
  number = {1},
  pages = {34},
  issn = {2399-3642},
  doi = {10.1038/s42003-021-02975-5},
  urldate = {2025-03-12},
  abstract = {Abstract             Stimulus images can be reconstructed from visual cortical activity. However, our perception of stimuli is shaped by both stimulus-induced and top-down processes, and it is unclear whether and how reconstructions reflect top-down aspects of perception. Here, we investigate the effect of attention on reconstructions using fMRI activity measured while subjects attend to one of two superimposed images. A state-of-the-art method is used for image reconstruction, in which brain activity is translated (decoded) to deep neural network (DNN) features of hierarchical layers then to an image. Reconstructions resemble the attended rather than unattended images. They can be modeled by superimposed images with biased contrasts, comparable to the appearance during attention. Attentional modulations are found in a broad range of hierarchical visual representations and mirror the brain--DNN correspondence. Our results demonstrate that top-down attention counters stimulus-induced responses, modulating neural representations to render reconstructions in accordance with subjective appearance.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/NBW3P7LC/Horikawa and Kamitani - 2022 - Attention modulates neural representation to rende.pdf}
}

@article{horikawaGenericDecodingSeen2017,
  title = {Generic Decoding of Seen and Imagined Objects Using Hierarchical Visual Features},
  author = {Horikawa, Tomoyasu and Kamitani, Yukiyasu},
  year = {2017},
  month = may,
  journal = {Nature Communications},
  volume = {8},
  number = {1},
  pages = {15037},
  issn = {2041-1723},
  doi = {10.1038/ncomms15037},
  urldate = {2024-05-15},
  abstract = {Abstract             Object recognition is a key function in both human and machine vision. While brain decoding of seen and imagined objects has been achieved, the prediction is limited to training examples. We present a decoding approach for arbitrary objects using the machine vision principle that an object category is represented by a set of features rendered invariant through hierarchical processing. We show that visual features, including those derived from a deep convolutional neural network, can be predicted from fMRI patterns, and that greater accuracy is achieved for low-/high-level features with lower-/higher-level visual areas, respectively. Predicted features are used to identify seen/imagined object categories (extending beyond decoder training) from a set of computed features for numerous object images. Furthermore, decoding of imagined objects reveals progressive recruitment of higher-to-lower visual representations. Our results demonstrate a homology between human and machine vision and its utility for brain-based information retrieval.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/GJ6CW97F/Horikawa and Kamitani - 2017 - Generic decoding of seen and imagined objects usin.pdf}
}

@article{horikawaNeuralDecodingVisual2013,
  title = {Neural {{Decoding}} of {{Visual Imagery During Sleep}}},
  author = {Horikawa, T. and Tamaki, M. and Miyawaki, Y. and Kamitani, Y.},
  year = {2013},
  month = may,
  journal = {Science},
  volume = {340},
  number = {6132},
  pages = {639--642},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1234330},
  urldate = {2025-03-12},
  abstract = {Reading Dreams                            How specific visual dream contents are represented by brain activity is unclear. Machine-learning--based analyses can decode the stimulus- and task-induced brain activity patterns that represent specific visual contents.                                Horikawa                 et al.                              (p.               639               , published online 4 April) examined patterns of brain activity during dreaming and compared these to waking responses to visual stimuli. The findings suggest that the visual content of dreams is represented by the same neural substrate as observed during awake perception.                        ,              Machine-learning models can predict specific visual dream contents from brain activity measurement alone.           ,              Visual imagery during sleep has long been a topic of persistent speculation, but its private nature has hampered objective analysis. Here we present a neural decoding approach in which machine-learning models predict the contents of visual imagery during the sleep-onset period, given measured brain activity, by discovering links between human functional magnetic resonance imaging patterns and verbal reports with the assistance of lexical and image databases. Decoding models trained on stimulus-induced brain activity in visual cortical areas showed accurate classification, detection, and identification of contents. Our findings demonstrate that specific visual experience during sleep is represented by brain activity patterns shared by stimulus perception, providing a means to uncover subjective contents of dreaming using objective neural measurement.},
  langid = {english}
}

@inproceedings{Hu_2023_ICCV,
  title = {{{PromptCap}}: {{Prompt-guided}} Image Captioning for {{VQA}} with {{GPT-3}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author = {Hu, Yushi and Hua, Hang and Yang, Zhengyuan and Shi, Weijia and Smith, Noah A. and Luo, Jiebo},
  year = {2023},
  month = oct,
  pages = {2963--2975},
  file = {/Users/matt/Zotero/storage/GUDTHULM/Hu et al. - 2023 - PromptCap Prompt-guided image captioning for VQA .pdf}
}

@misc{junRealtimeSpikeSorting2017,
  title = {Real-Time Spike Sorting Platform for High-Density Extracellular Probes with Ground-Truth Validation and Drift Correction},
  author = {Jun, James J. and Mitelut, Catalin and Lai, Chongxi and Gratiy, Sergey L. and Anastassiou, Costas A. and Harris, Timothy D.},
  year = {2017},
  month = jan,
  primaryclass = {New Results},
  pages = {101030},
  publisher = {bioRxiv},
  doi = {10.1101/101030},
  urldate = {2024-02-27},
  abstract = {Electrical recordings from a large array of electrodes give us access to neural population activity with single-cell, single-spike resolution. These recordings contain extracellular spikes which must be correctly detected and assigned to individual neurons. Despite numerous spike-sorting techniques developed in the past, a lack of high-quality ground-truth datasets hinders the validation of spike-sorting approaches. Furthermore, existing approaches requiring manual corrections are not scalable for hours of recordings exceeding 100 channels. To address these issues, we built a comprehensive spike-sorting pipeline that performs reliably under noise and probe drift by incorporating covariance-based features and unsupervised clustering based on fast density-peak finding. We validated performance of our workflow using multiple ground-truth datasets that recently became available. Our software scales linearly and processes up to 1000-channel recording in real-time using a single workstation. Accurate, real-time spike sorting from large recording arrays will enable more precise control of closed-loop feedback experiments and brain-computer interfaces.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2017, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/6IHYKNI7/Jun et al. - 2017 - Real-time spike sorting platform for high-density .pdf}
}

@article{kamitaniDecodingVisualSubjective2005,
  title = {Decoding the Visual and Subjective Contents of the Human Brain},
  author = {Kamitani, Yukiyasu and Tong, Frank},
  year = {2005},
  month = may,
  journal = {Nature Neuroscience},
  volume = {8},
  number = {5},
  pages = {679--685},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1444},
  urldate = {2024-05-16},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/matt/Zotero/storage/LDCSPWGF/Kamitani and Tong - 2005 - Decoding the visual and subjective contents of the.pdf}
}

@misc{KamitaniLabBdpy2024,
  title = {{{KamitaniLab}}/Bdpy},
  year = {2024},
  month = dec,
  urldate = {2025-01-23},
  abstract = {Python package for brain decoding analysis (BrainDecoderToolbox2 data format, machine learning analysis, functional MRI)},
  copyright = {MIT},
  howpublished = {Kamitani Lab}
}

@article{kamitaniSpatialSmoothingHurts2010,
  title = {Spatial Smoothing Hurts Localization but Not Information: {{Pitfalls}} for Brain Mappers},
  shorttitle = {Spatial Smoothing Hurts Localization but Not Information},
  author = {Kamitani, Yukiyasu and Sawahata, Yasuhito},
  year = {2010},
  month = feb,
  journal = {NeuroImage},
  volume = {49},
  number = {3},
  pages = {1949--1952},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2009.06.040},
  urldate = {2025-03-12},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/matt/Zotero/storage/VXKF9HN8/Kamitani and Sawahata - 2010 - Spatial smoothing hurts localization but not infor.pdf}
}

@article{kanwisherFusiformFaceArea1997,
  title = {The {{Fusiform Face Area}}: {{A Module}} in {{Human Extrastriate Cortex Specialized}} for {{Face Perception}}},
  shorttitle = {The {{Fusiform Face Area}}},
  author = {Kanwisher, Nancy and McDermott, Josh and Chun, Marvin M.},
  year = {1997},
  month = jun,
  journal = {The Journal of Neuroscience},
  volume = {17},
  number = {11},
  pages = {4302--4311},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.17-11-04302.1997},
  urldate = {2025-01-25},
  abstract = {Using functional magnetic resonance imaging (fMRI), we found an area in the fusiform gyrus in 12 of the 15 subjects tested that was significantly more active when the subjects viewed faces than when they viewed assorted common objects. This face activation was used to define a specific region of interest individually for each subject, within which several new tests of face specificity were run. In each of five subjects tested, the predefined candidate ``face area'' also responded significantly more strongly to passive viewing of (1) intact than scrambled two-tone faces, (2) full front-view face photos than front-view photos of houses, and (in a different set of five subjects) (3) three-quarter-view face photos (with hair concealed) than photos of human hands; it also responded more strongly during (4) a consecutive matching task performed on three-quarter-view faces versus hands. Our technique of running multiple tests applied to the same region defined functionally within individual subjects provides a solution to two common problems in functional imaging: (1) the requirement to correct for multiple statistical comparisons and (2) the inevitable ambiguity in the interpretation of any study in which only two or three conditions are compared. Our data allow us to reject alternative accounts of the function of the fusiform face area (area ``FF'') that appeal to visual attention, subordinate-level classification, or general processing of any animate or human forms, demonstrating that this region is               selectively               involved in the perception of faces.},
  copyright = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
  langid = {english},
  file = {/Users/matt/Zotero/storage/BSFA7PSC/Kanwisher et al. - 1997 - The Fusiform Face Area A Module in Human Extrastr.pdf}
}

@article{kapoorREFORMSConsensusbasedRecommendations2024,
  title = {{{REFORMS}}: {{Consensus-based Recommendations}} for {{Machine-learning-based Science}}},
  shorttitle = {{{REFORMS}}},
  author = {Kapoor, Sayash and Cantrell, Emily M. and Peng, Kenny and Pham, Thanh Hien and Bail, Christopher A. and Gundersen, Odd Erik and Hofman, Jake M. and Hullman, Jessica and Lones, Michael A. and Malik, Momin M. and Nanayakkara, Priyanka and Poldrack, Russell A. and Raji, Inioluwa Deborah and Roberts, Michael and Salganik, Matthew J. and {Serra-Garcia}, Marta and Stewart, Brandon M. and Vandewiele, Gilles and Narayanan, Arvind},
  year = {2024},
  month = may,
  journal = {Science Advances},
  volume = {10},
  number = {18},
  pages = {eadk3452},
  issn = {2375-2548},
  doi = {10.1126/sciadv.adk3452},
  urldate = {2024-06-07},
  abstract = {Machine learning (ML) methods are proliferating in scientific research. However, the adoption of these methods has been accompanied by failures of validity, reproducibility, and generalizability. These failures can hinder scientific progress, lead to false consensus around invalid claims, and undermine the credibility of ML-based science. ML methods are often applied and fail in similar ways across disciplines. Motivated by this observation, our goal is to provide clear recommendations for conducting and reporting ML-based science. Drawing from an extensive review of past literature, we present the REFORMS checklist (recommendations for machine-learning-based science). It consists of 32 questions and a paired set of guidelines. REFORMS was developed on the basis of a consensus of 19 researchers across computer science, data science, mathematics, social sciences, and biomedical sciences. REFORMS can serve as a resource for researchers when designing and implementing a study, for referees when reviewing papers, and for journals when enforcing standards for transparency and reproducibility.           ,              We provide a checklist to improve reporting practices in ML-based science based on a review of best practices and common errors.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/LDE43MAR/Kapoor et al. - 2024 - REFORMS Consensus-based Recommendations for Machi.pdf}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.6980},
  urldate = {2025-03-03},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/HNZPD73H/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/Users/matt/Zotero/storage/LIZCHNG3/1412.html}
}

@article{kolluruLearningFewerImages2021,
  title = {Learning {{With Fewer Images}} via {{Image Clustering}}: {{Application}} to {{Intravascular OCT Image Segmentation}}},
  shorttitle = {Learning {{With Fewer Images}} via {{Image Clustering}}},
  author = {Kolluru, Chaitanya and Lee, Juhwan and Gharaibeh, Yazan and Bezerra, Hiram G. and Wilson, David L.},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {37273--37280},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3058890},
  urldate = {2024-11-15},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  file = {/Users/matt/Zotero/storage/S4JZIZHQ/Kolluru et al. - 2021 - Learning With Fewer Images via Image Clustering A.pdf}
}

@inproceedings{koppen2000curse,
  title = {The Curse of Dimensionality},
  booktitle = {5th Online World Conference on Soft Computing in Industrial Applications ({{WSC5}})},
  author = {K{\"o}ppen, Mario},
  year = {2000},
  volume = {1},
  pages = {4--8}
}

@article{kourtziCorticalRegionsInvolved2000,
  title = {Cortical {{Regions Involved}} in {{Perceiving Object Shape}}},
  author = {Kourtzi, Zoe and Kanwisher, Nancy},
  year = {2000},
  month = may,
  journal = {The Journal of Neuroscience},
  volume = {20},
  number = {9},
  pages = {3310--3318},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.20-09-03310.2000},
  urldate = {2025-01-25},
  abstract = {The studies described here use functional magnetic resonance imaging to test whether common or distinct cognitive and/or neural mechanisms are involved in extracting object structure from the different image cues defining an object's shape, such as contours, shading, and monocular depth cues. We found overlapping activations in the lateral and ventral occipital cortex [known as the lateral occipital complex (LOC)] for objects defined by different visual cues (e.g., grayscale photographs and line drawings) when each was compared with its own scrambled-object control. In a second experiment we found a reduced response when objects were repeated, independent of whether they appeared in the same or a different format (i.e., grayscale images vs line drawings). A third experiment showed that activation in the LOC was no stronger for three-dimensional shapes defined by contours or monocular depth cues, such as occlusion, than for two-dimensional shapes, suggesting that these regions are not selectively involved in processing three-dimensional shape information. These results suggest that common regions in the LOC are involved in extracting and/or representing information about object structure from different image cues.},
  copyright = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
  langid = {english},
  file = {/Users/matt/Zotero/storage/FA4TIHVA/Kourtzi and Kanwisher - 2000 - Cortical Regions Involved in Perceiving Object Sha.pdf}
}

@misc{kurakinAdversarialMachineLearning2017,
  title = {Adversarial {{Machine Learning}} at {{Scale}}},
  author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  year = {2017},
  month = feb,
  number = {arXiv:1611.01236},
  eprint = {1611.01236},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1611.01236},
  urldate = {2025-02-28},
  abstract = {Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a "label leaking" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/matt/Zotero/storage/RXH968FT/Kurakin et al. - 2017 - Adversarial Machine Learning at Scale.pdf;/Users/matt/Zotero/storage/MYQUU5C4/1611.html}
}

@article{lewickiReviewMethodsSpike1998,
  title = {A Review of Methods for Spike Sorting: The Detection and Classification of Neural Action Potentials},
  shorttitle = {A Review of Methods for Spike Sorting},
  author = {Lewicki, Michael S.},
  year = {1998},
  month = nov,
  journal = {Network: Computation in Neural Systems},
  volume = {9},
  number = {4},
  pages = {R53},
  issn = {0954-898X},
  doi = {10.1088/0954-898X/9/4/001},
  urldate = {2024-03-05},
  abstract = {The detection of neural spike activity is a technical challenge that is a prerequisite for studying many types of brain function. Measuring the activity of individual neurons accurately can be difficult due to large amounts of background noise and the difficulty in distinguishing the action potentials of one neuron from those of others in the local area. This article reviews algorithms and methods for detecting and classifying action potentials, a problem commonly referred to as spike sorting. The article first discusses the challenges of measuring neural activity and the basic issues of signal detection and classification. It reviews and illustrates algorithms and techniques that have been applied to many of the problems in spike sorting and discusses the advantages and limitations of each and the applicability of these methods for different types of experimental demands. The article is written both for the physiologist wanting to use simple methods that will improve experimental yield and minimize the selection biases of traditional techniques and for those who want to apply or extend more sophisticated algorithms to meet new experimental challenges.},
  langid = {english}
}

@article{liDivergenceAgnosticUnsupervisedDomain2022,
  title = {Divergence-{{Agnostic Unsupervised Domain Adaptation}} by {{Adversarial Attacks}}},
  author = {Li, Jingjing and Du, Zhekai and Zhu, Lei and Ding, Zhengming and Lu, Ke and Shen, Heng Tao},
  year = {2022},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {11},
  pages = {8196--8211},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3109287},
  urldate = {2025-02-28},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@article{lindenLFPyToolBiophysical2014,
  title = {{{LFPy}}: A Tool for Biophysical Simulation of Extracellular Potentials Generated by Detailed Model Neurons},
  shorttitle = {{{LFPy}}},
  author = {Lind{\'e}n, Henrik and Hagen, Espen and Leski, Szymon and Norheim, Eivind and Pettersen, Klas and Einevoll, Gaute},
  year = {2014},
  journal = {Frontiers in Neuroinformatics},
  volume = {7},
  issn = {1662-5196},
  urldate = {2024-03-05},
  abstract = {Electrical extracellular recordings, i.e., recordings of the electrical potentials in the extracellular medium between cells, have been a main work-horse in electrophysiology for almost a century. The high-frequency part of the signal ({$\greaterequivlnt$}500 Hz), i.e., the multi-unit activity (MUA), contains information about the firing of action potentials in surrounding neurons, while the low-frequency part, the local field potential (LFP), contains information about how these neurons integrate synaptic inputs. As the recorded extracellular signals arise from multiple neural processes, their interpretation is typically ambiguous and difficult. Fortunately, a precise biophysical modeling scheme linking activity at the cellular level and the recorded signal has been established: the extracellular potential can be calculated as a weighted sum of all transmembrane currents in all cells located in the vicinity of the electrode. This computational scheme can considerably aid the modeling and analysis of MUA and LFP signals. Here, we describe LFPy, an open source Python package for numerical simulations of extracellular potentials. LFPy consists of a set of easy-to-use classes for defining cells, synapses and recording electrodes as Python objects, implementing this biophysical modeling scheme. It runs on top of the widely used NEURON simulation environment, which allows for flexible usage of both new and existing cell models. Further, calculation of extracellular potentials using the line-source-method is efficiently implemented. We describe the theoretical framework underlying the extracellular potential calculations and illustrate by examples how LFPy can be used both for simulating LFPs, i.e., synaptic contributions from single cells as well a populations of cells, and MUAs, i.e., extracellular signatures of action potentials.},
  file = {/Users/matt/Zotero/storage/9FRXBRDS/Lindén et al. - 2014 - LFPy a tool for biophysical simulation of extrace.pdf}
}

@misc{lipplWhenDoesCompositional2024,
  title = {When Does Compositional Structure Yield Compositional Generalization? {{A}} Kernel Theory},
  shorttitle = {When Does Compositional Structure Yield Compositional Generalization?},
  author = {Lippl, Samuel and Stachenfeld, Kim},
  year = {2024},
  month = may,
  number = {arXiv:2405.16391},
  eprint = {2405.16391},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-06-12},
  abstract = {Compositional generalization (the ability to respond correctly to novel combinations of familiar components) is thought to be a cornerstone of intelligent behavior. Compositionally structured (e.g. disentangled) representations are essential for this; however, the conditions under which they yield compositional generalization remain unclear. To address this gap, we present a general theory of compositional generalization in kernel models with fixed, potentially nonlinear representations (which also applies to neural networks in the ``lazy regime''). We prove that these models are functionally limited to adding up values assigned to conjunctions/combinations of components that have been seen during training (``conjunction-wise additivity''), and identify novel compositionality failure modes that arise from the data and model structure, even for disentangled inputs. For models in the representation learning (or ``rich'') regime, we show that networks can generalize on an important non-additive task (associative inference), and give a mechanistic explanation for why. Finally, we validate our theory empirically, showing that it captures the behavior of deep neural networks trained on a set of compositional tasks. In sum, our theory characterizes the principles giving rise to compositional generalization in kernel models and shows how representation learning can overcome their limitations. We further provide a formally grounded, novel generalization class for compositional tasks that highlights fundamental differences in the required learning mechanisms (conjunction-wise additivity).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/Users/matt/Zotero/storage/WBLG3HP7/Lippl and Stachenfeld - 2024 - When does compositional structure yield compositio.pdf}
}

@article{liuUnsupervisedBlindImage2020,
  title = {Unsupervised {{Blind Image Quality Evaluation}} via {{Statistical Measurements}} of {{Structure}}, {{Naturalness}}, and {{Perception}}},
  author = {Liu, Yutao and Gu, Ke and Zhang, Yongbing and Li, Xiu and Zhai, Guangtao and Zhao, Debin and Gao, Wen},
  year = {2020},
  month = apr,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {30},
  number = {4},
  pages = {929--943},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2019.2900472},
  urldate = {2025-03-05},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {/Users/matt/Zotero/storage/XDEIJG8E/Liu et al. - 2020 - Unsupervised Blind Image Quality Evaluation via St.pdf}
}

@misc{madryDeepLearningModels2019,
  title = {Towards {{Deep Learning Models Resistant}} to {{Adversarial Attacks}}},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  year = {2019},
  month = sep,
  number = {arXiv:1706.06083},
  eprint = {1706.06083},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.06083},
  urldate = {2025-02-28},
  abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/matt/Zotero/storage/R3MUW6DL/Madry et al. - 2019 - Towards Deep Learning Models Resistant to Adversar.pdf;/Users/matt/Zotero/storage/G4B8FYFH/1706.html}
}

@misc{maglandUnimodalClusteringUsing2016,
  title = {Unimodal Clustering Using Isotonic Regression: {{ISO-SPLIT}}},
  shorttitle = {Unimodal Clustering Using Isotonic Regression},
  author = {Magland, Jeremy F. and Barnett, Alex H.},
  year = {2016},
  month = may,
  number = {arXiv:1508.04841},
  eprint = {1508.04841},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1508.04841},
  urldate = {2024-03-04},
  abstract = {A limitation of many clustering algorithms is the requirement to tune adjustable parameters for each application or even for each dataset. Some techniques require an {\textbackslash}emph\{a priori\} estimate of the number of clusters while density-based techniques usually require a scale parameter. Other parametric methods, such as mixture modeling, make assumptions about the underlying cluster distributions. Here we introduce a non-parametric clustering method that does not involve tunable parameters and only assumes that clusters are unimodal, in the sense that they have a single point of maximal density when projected onto any line, and that clusters are separated from one another by a separating hyperplane of relatively lower density. The technique uses a non-parametric variant of Hartigan's dip statistic using isotonic regression as the kernel operation repeated at every iteration. We compare the method against k-means++, DBSCAN, and Gaussian mixture methods and show in simulations that it performs better than these standard methods in many situations. The algorithm is suited for low-dimensional datasets with a large number of observations, and was motivated by the problem of "spike sorting" in neural electrical recordings. Source code is freely available.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/matt/Zotero/storage/TG7TZVTJ/Magland and Barnett - 2016 - Unimodal clustering using isotonic regression ISO.pdf;/Users/matt/Zotero/storage/3RQJWD39/1508.html}
}

@inproceedings{maheshwariImageClusteringUsing2009,
  title = {Image {{Clustering Using Color}} and {{Texture}}},
  booktitle = {2009 {{First International Conference}} on {{Computational Intelligence}}, {{Communication Systems}} and {{Networks}}},
  author = {Maheshwari, Manish and Silakari, Sanjay and Motwani, Mahesh},
  year = {2009},
  month = jul,
  pages = {403--408},
  publisher = {IEEE},
  address = {Indore, India},
  doi = {10.1109/CICSYN.2009.69},
  urldate = {2024-11-26},
  isbn = {978-0-7695-3743-6},
  file = {/Users/matt/Zotero/storage/VRNIN92V/Maheshwari et al. - 2009 - Image Clustering Using Color and Texture.pdf}
}

@misc{maiUniBrainUnifyImage2023,
  title = {{{UniBrain}}: {{Unify Image Reconstruction}} and {{Captioning All}} in {{One Diffusion Model}} from {{Human Brain Activity}}},
  shorttitle = {{{UniBrain}}},
  author = {Mai, Weijian and Zhang, Zhijun},
  year = {2023},
  month = aug,
  number = {arXiv:2308.07428},
  eprint = {2308.07428},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.07428},
  urldate = {2025-03-04},
  abstract = {Image reconstruction and captioning from brain activity evoked by visual stimuli allow researchers to further understand the connection between the human brain and the visual perception system. While deep generative models have recently been employed in this field, reconstructing realistic captions and images with both low-level details and high semantic fidelity is still a challenging problem. In this work, we propose UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity. For the first time, we unify image reconstruction and captioning from visual-evoked functional magnetic resonance imaging (fMRI) through a latent diffusion model termed Versatile Diffusion. Specifically, we transform fMRI voxels into text and image latent for low-level information and guide the backward diffusion process through fMRI-based image and text conditions derived from CLIP to generate realistic captions and images. UniBrain outperforms current methods both qualitatively and quantitatively in terms of image reconstruction and reports image captioning results for the first time on the Natural Scenes Dataset (NSD) dataset. Moreover, the ablation experiments and functional region-of-interest (ROI) analysis further exhibit the superiority of UniBrain and provide comprehensive insight for visual-evoked brain decoding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/RZQUITSD/Mai and Zhang - 2023 - UniBrain Unify Image Reconstruction and Captionin.pdf;/Users/matt/Zotero/storage/2XXM844L/2308.html}
}

@misc{mcinnesUMAPUniformManifold2018,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1802.03426},
  urldate = {2025-02-10},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computational Geometry (cs.CG),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@article{meyerDualSortOnlineSpike2023,
  title = {{{DualSort}}: Online Spike Sorting with a Running Neural Network},
  shorttitle = {{{DualSort}}},
  author = {Meyer, L. M. and Samann, F. and Schanze, T.},
  year = {2023},
  month = oct,
  journal = {Journal of Neural Engineering},
  volume = {20},
  number = {5},
  pages = {056031},
  publisher = {IOP Publishing},
  issn = {1741-2552},
  doi = {10.1088/1741-2552/acfb3a},
  urldate = {2024-03-04},
  abstract = {Objective. Spike sorting, i.e. the detection and separation of measured action potentials from different extracellularly recorded neurons, remains one of the bottlenecks in deciphering the brain. In recent years, the application of neural networks (NNs) for spike sorting has garnered significant attention. Most methods focus on specific sub-problems within the conventional spike sorting pipeline, such as spike detection or feature extraction, and attempt to solve them with complex network architectures. This paper presents DualSort, a simple NN that gets combined with downstream post-processing for real-time spike sorting. It shows high efficiency, low complexity, and requires a comparatively small amount of human interaction. Approach. Synthetic and experimentally obtained extracellular single-channel recordings were utilized to train and evaluate the proposed NN. For training, spike waveforms were labeled with respect to their associated neuron and position in the signal, allowing the detection and categorization of spikes in unison. DualSort classifies a single spike multiple times in succession, as it runs over the signal in a step-by-step manner and uses a post-processing algorithm that transmits the network output into spike trains. Main results. With the used datasets, DualSort was able to detect and distinguish different spike waveforms and separate them from background activity. The post-processing algorithm significantly strengthened the overall performance of the model, making the system more robust as a whole. Although DualSort is an end-to-end solution that efficiently transforms filtered signals into spike trains, it competes with contemporary state-of-the-art technologies that exclusively target single sub-problems in the conventional spike sorting pipeline. Significance. This work demonstrates that even under high noise levels, complex NNs are not necessary by any means to achieve high performance in spike detection and sorting. The utilization of data augmentation on a limited quantity of spikes could substantially decrease hand-labeling compared to other studies. Furthermore, the proposed framework can be utilized without human interaction when combined with an unsupervised technique that provides pseudo labels for DualSort. Due to the low complexity of our network, it works efficiently and enables real-time processing on basic hardware. The proposed approach is not limited to spike sorting, as it may also be used to process different signals, such as electroencephalogram (EEG), which needs to be investigated in future research.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/2JA6EKQ7/Meyer et al. - 2023 - DualSort online spike sorting with a running neur.pdf}
}

@misc{mildenbergerKamitaniLabBrain_diffuser,
  title = {{{KamitaniLab}}/Brain\_diffuser},
  author = {Mildenberger, Matthias},
  journal = {GitHub},
  urldate = {2025-01-22},
  howpublished = {https://github.com/KamitaniLab/brain\_diffuser},
  langid = {english},
  file = {/Users/matt/Zotero/storage/Y4NG7IGB/brain_diffuser.html}
}

@article{miyawakiVisualImageReconstruction2008,
  title = {Visual {{Image Reconstruction}} from {{Human Brain Activity}} Using a {{Combination}} of {{Multiscale Local Image Decoders}}},
  author = {Miyawaki, Yoichi and Uchida, Hajime and Yamashita, Okito and Sato, Masa-aki and Morito, Yusuke and Tanabe, Hiroki C. and Sadato, Norihiro and Kamitani, Yukiyasu},
  year = {2008},
  month = dec,
  journal = {Neuron},
  volume = {60},
  number = {5},
  pages = {915--929},
  issn = {08966273},
  doi = {10.1016/j.neuron.2008.11.004},
  urldate = {2024-05-16},
  abstract = {Perceptual experience consists of an enormous number of possible states. Previous fMRI studies have predicted a perceptual state by classifying brain activity into prespecified categories. Constraint-free visual image reconstruction is more challenging, as it is impractical to specify brain activity for all possible images. In this study, we reconstructed visual images by combining local image bases of multiple scales, whose contrasts were independently decoded from fMRI activity by automatically selecting relevant voxels and exploiting their correlated patterns. Binarycontrast, 10 3 10-patch images (2100 possible states) were accurately reconstructed without any image prior on a single trial or volume basis by measuring brain activity only for several hundred random images. Reconstruction was also used to identify the presented image among millions of candidates. The results suggest that our approach provides an effective means to read out complex perceptual states from brain activity while discovering information representation in multivoxel patterns.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/G28LT8YB/Miyawaki et al. - 2008 - Visual Image Reconstruction from Human Brain Activ.pdf}
}

@misc{ModelZoo,
  title = {Model {{Zoo}}},
  journal = {GitHub},
  urldate = {2025-01-23},
  abstract = {Caffe: a fast open framework for deep learning. Contribute to BVLC/caffe development by creating an account on GitHub.},
  howpublished = {https://github.com/BVLC/caffe/wiki/Model-Zoo},
  langid = {english},
  file = {/Users/matt/Zotero/storage/CC3AYAS5/Model-Zoo.html}
}

@article{moghaddasiUnsupervisedAutomaticOnline2020,
  title = {Unsupervised Automatic Online Spike Sorting Using Reward-Based Online Clustering},
  author = {Moghaddasi, Masoud and Aliyari Shoorehdeli, Mahdi and Fatahi, Zahra and Haghparast, Abbas},
  year = {2020},
  month = feb,
  journal = {Biomedical Signal Processing and Control},
  volume = {56},
  pages = {101701},
  issn = {1746-8094},
  doi = {10.1016/j.bspc.2019.101701},
  urldate = {2024-03-04},
  abstract = {Brain-machine interfaces (BMIs) can enable paralyzed people to regain mobility. In these interfaces, some different type of signals can be obtained from the brain, one of which is the action potential waveform (spike). In the case of using spikes, sorting the recorded signals and isolating the effects of the individual neurons can lead to a greater efficiency. Also, because of the nature of BMIs, real-time spike sorting is necessary. In many spike sorting approaches, the main outline consists of the following steps: spike detection, feature extraction, and clustering. In this study, a novel method for clustering is presented. This method is referred to as Reward-Based Online Clustering (RBOC) which is formed based on the reinforcement learning algorithm. The significant property of this proposed technique is its capability for real-time implementation that is required by BMIs. This method can automatically detect the clusters while there is no knowledge about the number of clusters. The performance of the proposed method is demonstrated through both simulation and experimental study. Evaluation with artificially simulated (ground truth) data shows that, on average, the accuracy of categorizing the spikes from the same origins is above 94 percent. Moreover, implementation of the method on the experimental data obtained from the rat brain represents convincing sorting results. It is noteworthy to say that, in most cases, this new method outperforms the results of similar previous works.},
  keywords = {Brain-machine interfaces,Extracellular recording,Q-Learning,Reward-based online clustering,Spike sorting},
  file = {/Users/matt/Zotero/storage/3XKSKEDX/S1746809419302824.html}
}

@inproceedings{moosavi-dezfooliUniversalAdversarialPerturbations2017,
  title = {Universal {{Adversarial Perturbations}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {{Moosavi-Dezfooli}, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
  year = {2017},
  month = jul,
  pages = {86--94},
  publisher = {IEEE},
  address = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.17},
  urldate = {2024-10-21},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/Users/matt/Zotero/storage/KGNP6RRK/Moosavi-Dezfooli et al. - 2017 - Universal Adversarial Perturbations.pdf}
}

@article{musialSignaltonoiseRatioImprovement2002,
  title = {Signal-to-Noise Ratio Improvement in Multiple Electrode Recording},
  author = {Musial, P.G and Baker, S.N and Gerstein, G.L and King, E.A and Keating, J.G},
  year = {2002},
  month = mar,
  journal = {Journal of Neuroscience Methods},
  volume = {115},
  number = {1},
  pages = {29--43},
  issn = {01650270},
  doi = {10.1016/S0165-0270(01)00516-7},
  urldate = {2024-02-20},
  langid = {english},
  file = {/Users/matt/Zotero/storage/FLN2USLV/Musial et al. - 2002 - Signal-to-noise ratio improvement in multiple elec.pdf}
}

@misc{naseerIntriguingPropertiesVision2021,
  title = {Intriguing {{Properties}} of {{Vision Transformers}}},
  author = {Naseer, Muzammal and Ranasinghe, Kanchana and Khan, Salman and Hayat, Munawar and Khan, Fahad Shahbaz and Yang, Ming-Hsuan},
  year = {2021},
  month = nov,
  number = {arXiv:2105.10497},
  eprint = {2105.10497},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.10497},
  urldate = {2025-02-28},
  abstract = {Vision transformers (ViT) have demonstrated impressive performance across various machine vision problems. These models are based on multi-head self-attention mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility in attending image-wide context conditioned on a given patch can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a) Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60\% top-1 accuracy on ImageNet even after randomly occluding 80\% of the image content. (b) The robust performance to occlusions is not due to a bias towards local textures, and ViTs are significantly less biased towards textures compared to CNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c) Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d) Off-the-shelf features from a single ViT model can be combined to create a feature ensemble, leading to high accuracy rates across a range of classification datasets in both traditional and few-shot learning paradigms. We show effective features of ViTs are due to flexible and dynamic receptive fields possible via the self-attention mechanism.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/M3H45YP7/Naseer et al. - 2021 - Intriguing Properties of Vision Transformers.pdf;/Users/matt/Zotero/storage/QXUFB87S/2105.html}
}

@article{naselarisEncodingDecodingFMRI2011,
  title = {Encoding and Decoding in {{fMRI}}},
  author = {Naselaris, Thomas and Kay, Kendrick N. and Nishimoto, Shinji and Gallant, Jack L.},
  year = {2011},
  month = may,
  journal = {NeuroImage},
  volume = {56},
  number = {2},
  pages = {400--410},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.07.073},
  urldate = {2025-03-12},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/matt/Zotero/storage/6PWULKC7/Naselaris et al. - 2011 - Encoding and decoding in fMRI.pdf;/Users/matt/Zotero/storage/J7RS4G9J/Naselaris et al. - 2011 - Encoding and decoding in fMRI.pdf}
}

@misc{OpenAI_ChatGPT_2024,
  title = {{{ChatGPT}} ({{GPT-4o}})},
  author = {{OpenAI}},
  year = {2024}
}

@misc{ozcelikNaturalSceneReconstruction2023,
  title = {Natural Scene Reconstruction from {{fMRI}} Signals Using Generative Latent Diffusion},
  author = {Ozcelik, Furkan and VanRullen, Rufin},
  year = {2023},
  month = jun,
  number = {arXiv:2303.05334},
  eprint = {2303.05334},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-06-12},
  abstract = {In neural decoding research, one of the most intriguing topics is the reconstruction of perceived natural images based on fMRI signals. Previous studies have succeeded in re-creating different aspects of the visuals, such as low-level properties (shape, texture, layout) or high-level features (category of objects, descriptive semantics of scenes) but have typically failed to reconstruct these properties together for complex scene images. Generative AI has recently made a leap forward with latent diffusion models capable of generating high-complexity images. Here, we investigate how to take advantage of this innovative technology for brain decoding. We present a two-stage scene reconstruction framework called ``Brain-Diffuser''. In the first stage, starting from fMRI signals, we reconstruct images that capture low-level properties and overall layout using a VDVAE (Very Deep Variational Autoencoder) model. In the second stage, we use the image-to-image framework of a latent diffusion model (Versatile Diffusion) conditioned on predicted multimodal (text and visual) features, to generate final reconstructed images. On the publicly available Natural Scenes Dataset benchmark, our method outperforms previous models both qualitatively and quantitatively. When applied to synthetic fMRI patterns generated from individual ROI (region-of-interest) masks, our trained model creates compelling ``ROI-optimal'' scenes consistent with neuroscientific knowledge. Thus, the proposed methodology can have an impact on both applied (e.g. brain-computer interface) and fundamental neuroscience.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Neurons and Cognition},
  file = {/Users/matt/Zotero/storage/CA7ZXH5T/Ozcelik and VanRullen - 2023 - Natural scene reconstruction from fMRI signals usi.pdf}
}

@misc{ozcelikOzcelikfuBraindiffuser2025,
  title = {Ozcelikfu/Brain-Diffuser},
  author = {Ozcelik, Furkan},
  year = {2025},
  month = jan,
  urldate = {2025-01-22},
  abstract = {Official repository for the paper "Brain-Diffuser: Natural scene reconstruction from fMRI signals using generative latent diffusion" by Furkan Ozcelik and Rufin VanRullen.},
  copyright = {MIT}
}

@inproceedings{pachitariuFastAccurateSpike2016,
  title = {Fast and Accurate Spike Sorting of High-Channel Count Probes with {{KiloSort}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pachitariu, Marius and Steinmetz, Nicholas A and Kadir, Shabnam N and Carandini, Matteo and Harris, Kenneth D},
  year = {2016},
  volume = {29},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-02-22},
  abstract = {New silicon technology is enabling large-scale electrophysiological recordings in vivo from hundreds to thousands of channels. Interpreting these recordings requires scalable and accurate automated methods for spike sorting, which should minimize the time required for manual curation of the results. Here we introduce KiloSort, a new integrated spike sorting framework that uses template matching both during spike detection and during spike clustering. KiloSort models the electrical voltage as a sum of template waveforms triggered on the spike times, which allows overlapping spikes to be identified and resolved. Unlike previous algorithms that compress the data with PCA, KiloSort operates on the raw data which allows it to construct a more accurate model of the waveforms. Processing times are faster than in previous algorithms thanks to batch-based optimization on GPUs. We compare KiloSort to an established algorithm and show favorable performance, at much reduced processing times. A novel post-clustering merging step based on the continuity of the templates further reduced substantially the number of manual operations required on this data, for the neurons with near-zero error rates, paving the way for fully automated spike sorting of multichannel electrode recordings.},
  file = {/Users/matt/Zotero/storage/FUQY9GB8/Pachitariu et al. - 2016 - Fast and accurate spike sorting of high-channel co.pdf}
}

@misc{pachitariuSolvingSpikeSorting2023,
  title = {Solving the Spike Sorting Problem with {{Kilosort}}},
  author = {Pachitariu, Marius and Sridhar, Shashwat and Stringer, Carsen},
  year = {2023},
  month = jan,
  primaryclass = {New Results},
  pages = {2023.01.07.523036},
  publisher = {bioRxiv},
  doi = {10.1101/2023.01.07.523036},
  urldate = {2024-02-26},
  abstract = {Spike sorting is the computational process of extracting the firing times of single neurons from recordings of local electrical fields. This is an important but hard problem in neuroscience, complicated by the non-stationarity of the recordings and the dense overlap in electrical fields between nearby neurons. To solve the spike sorting problem, we have continuously developed over the past eight years a framework known as Kilosort. This paper describes the various algorithmic steps introduced in different versions of Kilosort. We also report the development of Kilosort4, a new version with substantially improved performance due to new clustering algorithms inspired by graph-based approaches. To test the performance of Kilosort, we developed a realistic simulation framework which uses densely sampled electrical fields from real experiments to generate non-stationary spike waveforms and realistic noise. We find that nearly all versions of Kilosort outperform other algorithms on a variety of simulated conditions, and Kilosort4 performs best in all cases, correctly identifying even neurons with low amplitudes and small spatial extents in high drift conditions.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  file = {/Users/matt/Zotero/storage/SAFAQINM/Pachitariu et al. - 2023 - Solving the spike sorting problem with Kilosort.pdf}
}

@misc{papernotPracticalBlackBoxAttacks2017,
  title = {Practical {{Black-Box Attacks}} against {{Machine Learning}}},
  author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
  year = {2017},
  month = mar,
  number = {arXiv:1602.02697},
  eprint = {1602.02697},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1602.02697},
  urldate = {2025-02-28},
  abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24\% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19\% and 88.94\%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/KXDQLMS8/Papernot et al. - 2017 - Practical Black-Box Attacks against Machine Learni.pdf;/Users/matt/Zotero/storage/QBV547UX/1602.html}
}

@inproceedings{pavlichenkoBestPromptsTexttoImage2023,
  title = {Best {{Prompts}} for {{Text-to-Image Models}} and {{How}} to {{Find Them}}},
  booktitle = {Proceedings of the 46th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Pavlichenko, Nikita and Ustalov, Dmitry},
  year = {2023},
  month = jul,
  pages = {2067--2071},
  publisher = {ACM},
  address = {Taipei Taiwan},
  doi = {10.1145/3539618.3592000},
  urldate = {2025-03-04},
  isbn = {978-1-4503-9408-6},
  langid = {english},
  file = {/Users/matt/Zotero/storage/R5LHHPT8/Pavlichenko and Ustalov - 2023 - Best Prompts for Text-to-Image Models and How to F.pdf}
}

@article{pedreiraHowManyNeurons2012,
  title = {How Many Neurons Can We See with Current Spike Sorting Algorithms?},
  author = {Pedreira, Carlos and Martinez, Juan and Ison, Matias J. and Quian Quiroga, Rodrigo},
  year = {2012},
  month = oct,
  journal = {Journal of Neuroscience Methods},
  volume = {211},
  number = {1},
  pages = {58--65},
  issn = {01650270},
  doi = {10.1016/j.jneumeth.2012.07.010},
  urldate = {2024-02-22},
  langid = {english},
  file = {/Users/matt/Zotero/storage/5765ESFU/Pedreira et al. - 2012 - How many neurons can we see with current spike sor.pdf}
}

@inproceedings{poursaeedGenerativeAdversarialPerturbations2018,
  title = {Generative {{Adversarial Perturbations}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Poursaeed, Omid and Katsman, Isay and Gao, Bicheng and Belongie, Serge},
  year = {2018},
  month = jun,
  pages = {4422--4431},
  publisher = {IEEE},
  address = {Salt Lake City, UT},
  doi = {10.1109/CVPR.2018.00465},
  urldate = {2024-10-21},
  abstract = {In this paper, we propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models. We present trainable deep neural networks for transforming images to adversarial perturbations. Our proposed models can produce image-agnostic and image-dependent perturbations for targeted and nontargeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling both classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. Using extensive experiments on challenging high-resolution datasets such as ImageNet and Cityscapes, we show that our perturbations achieve high fooling rates with small perturbation norms. Moreover, our attacks are considerably faster than current iterative methods at inference time.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {/Users/matt/Zotero/storage/W8EV6KUJ/Poursaeed et al. - 2018 - Generative Adversarial Perturbations.pdf}
}

@article{radfordLearningTransferableVisual,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  abstract = {SOTA computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study performance on over 30 different computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers nontrivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/BTPJMQXI/Radford et al. - Learning Transferable Visual Models From Natural L.pdf}
}

@misc{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.00020},
  urldate = {2025-01-22},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/UL9GKCHF/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf;/Users/matt/Zotero/storage/AFDS68XK/2103.html}
}

@article{rensinkVisualSensingSeeing2004,
  title = {Visual {{Sensing Without Seeing}}},
  author = {Rensink, Ronald A.},
  year = {2004},
  month = jan,
  journal = {Psychological Science},
  volume = {15},
  number = {1},
  pages = {27--32},
  issn = {0956-7976, 1467-9280},
  doi = {10.1111/j.0963-7214.2004.01501005.x},
  urldate = {2025-03-03},
  abstract = {It has often been assumed that when we use vision to become aware of an object or event in our surroundings, this must be accompanied by a corresponding visual experience (i.e., seeing). The studies reported here show that this assumption is incorrect. When observers view a sequence of displays alternating between an image of a scene and the same image changed in some way, they often feel (or sense) the change even though they have no visual experience of it. The subjective difference between sensing and seeing is mirrored in several behavioral differences, suggesting that these are two distinct modes of conscious visual perception.},
  copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
  langid = {english},
  file = {/Users/matt/Zotero/storage/MDZIFSSP/Rensink - 2004 - Visual Sensing Without Seeing.pdf}
}

@article{reyPresentFutureSpike2015,
  title = {Past, Present and Future of Spike Sorting Techniques},
  author = {Rey, Hernan Gonzalo and Pedreira, Carlos and Quian Quiroga, Rodrigo},
  year = {2015},
  month = oct,
  journal = {Brain Research Bulletin},
  volume = {119},
  pages = {106--117},
  issn = {03619230},
  doi = {10.1016/j.brainresbull.2015.04.007},
  urldate = {2024-02-22},
  langid = {english},
  file = {/Users/matt/Zotero/storage/SUCIXWJK/Rey et al. - 2015 - Past, present and future of spike sorting techniqu.pdf}
}

@inproceedings{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bjorn},
  year = {2022},
  month = jun,
  pages = {10674--10685},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.01042},
  urldate = {2025-03-04},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-66546-946-3},
  file = {/Users/matt/Zotero/storage/IFS4GS5T/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf}
}

@inproceedings{sahariaPhotorealisticTexttoimageDiffusion2022,
  title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and Ho, Jonathan and Fleet, David J and Norouzi, Mohammad},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {36479--36494},
  publisher = {Curran Associates, Inc.},
  file = {/Users/matt/Zotero/storage/8G3ZNZQK/Saharia et al. - 2022 - Photorealistic text-to-image diffusion models with.pdf}
}

@article{saif-ur-rehmanSpikeDeepclassifierDeeplearningBased2021,
  title = {{{SpikeDeep-classifier}}: A Deep-Learning Based Fully Automatic Offline Spike Sorting Algorithm},
  shorttitle = {{{SpikeDeep-classifier}}},
  author = {{Saif-ur-Rehman}, Muhammad and Ali, Omair and Dyck, Susanne and Lienk{\"a}mper, Robin and Metzler, Marita and Parpaley, Yaroslav and Wellmer, J{\"o}rg and Liu, Charles and Lee, Brian and Kellis, Spencer and Andersen, Richard and Iossifidis, Ioannis and Glasmachers, Tobias and Klaes, Christian},
  year = {2021},
  month = feb,
  journal = {Journal of Neural Engineering},
  volume = {18},
  number = {1},
  pages = {016009},
  publisher = {IOP Publishing},
  issn = {1741-2552},
  doi = {10.1088/1741-2552/abc8d4},
  urldate = {2024-02-22},
  abstract = {Objective. Advancements in electrode design have resulted in micro-electrode arrays with hundreds of channels for single cell recordings. In the resulting electrophysiological recordings, each implanted electrode can record spike activity (SA) of one or more neurons along with background activity (BA). The aim of this study is to isolate SA of each neural source. This process is called spike sorting or spike classification. Advanced spike sorting algorithms are time consuming because of the human intervention at various stages of the pipeline. Current approaches lack generalization because the values of hyperparameters are not fixed, even for multiple recording sessions of the same subject. In this study, a fully automatic spike sorting algorithm called `SpikeDeep-Classifier' is proposed. The values of hyperparameters remain fixed for all the evaluation data. Approach. The proposed approach is based on our previous study (SpikeDeeptector) and a novel background activity rejector (BAR), which are both supervised learning algorithms and an unsupervised learning algorithm (K-means). SpikeDeeptector and BAR are used to extract meaningful channels and remove BA from the extracted meaningful channels, respectively. The process of clustering becomes straight-forward once the BA is completely removed from the data. Then, K-means with a predefined maximum number of clusters is applied on the remaining data originating from neural sources only. Lastly, a similarity-based criterion and a threshold are used to keep distinct clusters and merge similar looking clusters. The proposed approach is called cluster accept or merge (CAOM) and it has only two hyperparameters (maximum number of clusters and similarity threshold) which are kept fixed for all the evaluation data after tuning. Main results. We compared the results of our algorithm with ground-truth labels. The algorithm is evaluated on data of human patients and publicly available labeled non-human primates (NHPs) datasets. The average accuracy of BAR on datasets of human patients is 92.3\% which is further reduced to 88.03\% after (K-means + CAOM). In addition, the average accuracy of BAR on a publicly available labeled dataset of NHPs is 95.40\% which reduces to 86.95\% after (K-mean + CAOM). Lastly, we compared the performance of the SpikeDeep-Classifier with two human experts, where SpikeDeep-Classifier has produced comparable results. Significance. The SpikeDeep-Classifier is evaluated on the datasets of multiple recording sessions of different species, different brain areas and different electrode types without further retraining. The results demonstrate that `SpikeDeep-Classifier' possesses the ability to generalize well on a versatile dataset and henceforth provides a generalized and fully automated solution to offline spike sorting. Clinical trial registration number The clinical trial registration number for patients implanted with the Utah array is NCT 01849822. For the epilepsy patients, approval from the local ethics committee at the Ruhr-University Bochum, Germany, was obtained prior to implantation. The Clinical trial registration number for the epilepsy patients implanted with microwires is 16--5670.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/4H9G2N42/Saif-ur-Rehman et al. - 2021 - SpikeDeep-classifier a deep-learning based fully .pdf}
}

@article{samuelGeneratingImagesRare2024,
  title = {Generating {{Images}} of {{Rare Concepts Using Pre-trained Diffusion Models}}},
  author = {Samuel, Dvir and {Ben-Ari}, Rami and Raviv, Simon and Darshan, Nir and Chechik, Gal},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {5},
  pages = {4695--4703},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v38i5.28270},
  urldate = {2025-03-04},
  abstract = {Text-to-image diffusion models can synthesize high quality images, but they have various limitations. Here we highlight a common failure mode of these models, namely, generating uncommon concepts and structured concepts like hand palms. We show that their limitation is partly due to the long-tail nature of their training data: web-crawled data sets are strongly unbalanced, causing models to under-represent concepts from the tail of the distribution. We characterize the effect of unbalanced training data on text-to-image models and offer a remedy. We show that rare concepts can be correctly generated by carefully selecting suitable generation seeds in the noise space, using a small reference set of images, a technique that we call SeedSelect. SeedSelect does not require retraining or finetuning the diffusion model. We assess the faithfulness, quality and diversity of SeedSelect in creating rare objects and generating complex formations like hand images, and find it consistently achieves superior performance. We further show the advantage of SeedSelect in semantic data augmentation. Generating semantically appropriate images can successfully improve performance in few-shot recognition benchmarks, for classes from the head and from the tail of the training data of diffusion models.},
  file = {/Users/matt/Zotero/storage/VKAMPBPP/Samuel et al. - 2024 - Generating Images of Rare Concepts Using Pre-train.pdf}
}

@article{saraImageQualityAssessment2019,
  title = {Image {{Quality Assessment}} through {{FSIM}}, {{SSIM}}, {{MSE}} and {{PSNR}}---{{A Comparative Study}}},
  author = {Sara, Umme and Akter, Morium and Uddin, Mohammad Shorif},
  year = {2019},
  journal = {Journal of Computer and Communications},
  volume = {07},
  number = {03},
  pages = {8--18},
  issn = {2327-5219, 2327-5227},
  doi = {10.4236/jcc.2019.73002},
  urldate = {2024-10-25},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  file = {/Users/matt/Zotero/storage/INN6JNJ8/Sara et al. - 2019 - Image Quality Assessment through FSIM, SSIM, MSE a.pdf}
}

@misc{schuhmannLAION400MOpenDataset2021,
  title = {{{LAION-400M}}: {{Open Dataset}} of {{CLIP-Filtered}} 400 {{Million Image-Text Pairs}}},
  shorttitle = {{{LAION-400M}}},
  author = {Schuhmann, Christoph and Vencu, Richard and Beaumont, Romain and Kaczmarczyk, Robert and Mullis, Clayton and Katta, Aarush and Coombes, Theo and Jitsev, Jenia and Komatsuzaki, Aran},
  year = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2111.02114},
  urldate = {2025-01-22},
  abstract = {Multi-modal language-vision models trained on hundreds of millions of image-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable capability to perform zero- or few-shot learning and transfer even in absence of per-sample labels on target image data. Despite this trend, to date there has been no publicly available datasets of sufficient scale for training such models from scratch. To address this issue, in a community effort we build and release for public LAION-400M, a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices that allow efficient similarity search.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computation and Language (cs.CL),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@inproceedings{schuhmannLAION5BOpenLargescale2022,
  title = {{{LAION-5B}}: {{An}} Open Large-Scale Dataset for Training next Generation Image-Text Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {25278--25294},
  publisher = {Curran Associates, Inc.}
}

@misc{scottiMindEye2SharedSubjectModels2024,
  title = {{{MindEye2}}: {{Shared-Subject Models Enable fMRI-To-Image With}} 1 {{Hour}} of {{Data}}},
  shorttitle = {{{MindEye2}}},
  author = {Scotti, Paul S. and Tripathy, Mihir and Villanueva, Cesar Kadir Torrico and Kneeland, Reese and Chen, Tong and Narang, Ashutosh and Santhirasegaran, Charan and Xu, Jonathan and Naselaris, Thomas and Norman, Kenneth A. and Abraham, Tanishq Mathew},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2403.11207},
  urldate = {2025-03-15},
  abstract = {Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited. This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results. The present work showcases high-quality reconstructions using only 1 hour of fMRI training data. We pretrain our model across 7 subjects and then fine-tune on minimal data from a new subject. Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. We then map from CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP latents as inputs instead of text. This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruction metrics compared to single-subject approaches. MindEye2 demonstrates how accurate reconstructions of perception are possible from a single visit to the MRI facility. All code is available on GitHub.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Biological sciences,FOS: Computer and information sciences,Neurons and Cognition (q-bio.NC)},
  file = {/Users/matt/Zotero/storage/NA48UMFS/Scotti et al. - 2024 - MindEye2 Shared-Subject Models Enable fMRI-To-Ima.pdf}
}

@misc{scottiReconstructingMindsEye2023,
  title = {Reconstructing the {{Mind}}'s {{Eye}}: {{fMRI-to-Image}} with {{Contrastive Learning}} and {{Diffusion Priors}}},
  shorttitle = {Reconstructing the {{Mind}}'s {{Eye}}},
  author = {Scotti, Paul S. and Banerjee, Atmadeep and Goode, Jimmie and Shabalin, Stepan and Nguyen, Alex and Cohen, Ethan and Dempster, Aidan J. and Verlinde, Nathalie and Yundler, Elad and Weisberg, David and Norman, Kenneth A. and Abraham, Tanishq Mathew},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2305.18274},
  urldate = {2025-03-15},
  abstract = {We present MindEye, a novel fMRI-to-image approach to retrieve and reconstruct viewed images from brain activity. Our model comprises two parallel submodules that are specialized for retrieval (using contrastive learning) and reconstruction (using a diffusion prior). MindEye can map fMRI brain activity to any high dimensional multimodal latent space, like CLIP image space, enabling image reconstruction using generative models that accept embeddings from this latent space. We comprehensively compare our approach with other existing methods, using both qualitative side-by-side comparisons and quantitative evaluations, and show that MindEye achieves state-of-the-art performance in both reconstruction and retrieval tasks. In particular, MindEye can retrieve the exact original image even among highly similar candidates indicating that its brain embeddings retain fine-grained image-specific information. This allows us to accurately retrieve images even from large-scale databases like LAION-5B. We demonstrate through ablations that MindEye's performance improvements over previous methods result from specialized submodules for retrieval and reconstruction, improved training techniques, and training models with orders of magnitude more parameters. Furthermore, we show that MindEye can better preserve low-level image features in the reconstructions by using img2img, with outputs from a separate autoencoder. All code is available on GitHub.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Biological sciences,FOS: Computer and information sciences,Neurons and Cognition (q-bio.NC)}
}

@misc{senerActiveLearningConvolutional2018,
  title = {Active {{Learning}} for {{Convolutional Neural Networks}}: {{A Core-Set Approach}}},
  shorttitle = {Active {{Learning}} for {{Convolutional Neural Networks}}},
  author = {Sener, Ozan and Savarese, Silvio},
  year = {2018},
  month = jun,
  number = {arXiv:1708.00489},
  eprint = {1708.00489},
  primaryclass = {stat},
  publisher = {arXiv},
  urldate = {2024-11-15},
  abstract = {Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (ie. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, ie. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/matt/Zotero/storage/W6WLE8HI/Sener and Savarese - 2018 - Active Learning for Convolutional Neural Networks.pdf;/Users/matt/Zotero/storage/Q5LRKUGC/1708.html}
}

@article{serenoBordersMultipleVisual1995,
  title = {Borders of {{Multiple Visual Areas}} in {{Humans Revealed}} by {{Functional Magnetic Resonance Imaging}}},
  author = {Sereno, M. I. and Dale, A. M. and Reppas, J. B. and Kwong, K. K. and Belliveau, J. W. and Brady, T. J. and Rosen, B. R. and Tootell, R. B. H.},
  year = {1995},
  month = may,
  journal = {Science},
  volume = {268},
  number = {5212},
  pages = {889--893},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.7754376},
  urldate = {2025-01-25},
  abstract = {The borders of human visual areas V1, V2, VP, V3, and V4 were precisely and noninvasively determined. Functional magnetic resonance images were recorded during phase-encoded retinal stimulation. This volume data set was then sampled with a cortical surface reconstruction, making it possible to calculate the local visual field sign (mirror image versus non-mirror image representation). This method automatically and objectively outlines area borders because adjacent areas often have the opposite field sign. Cortical magnification factor curves for striate and extrastriate cortical areas were determined, which showed that human visual areas have a greater emphasis on the center-of-gaze than their counterparts in monkeys. Retinotopically organized visual areas in humans extend anteriorly to overlap several areas previously shown to be activated by written words.},
  langid = {english}
}

@article{shenDeepImageReconstruction2019,
  title = {Deep Image Reconstruction from Human Brain Activity},
  author = {Shen, Guohua and Horikawa, Tomoyasu and Majima, Kei and Kamitani, Yukiyasu},
  editor = {O'Reilly, Jill},
  year = {2019},
  month = jan,
  journal = {PLOS Computational Biology},
  volume = {15},
  number = {1},
  pages = {e1006633},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006633},
  urldate = {2024-05-15},
  abstract = {The mental contents of perception and imagery are thought to be encoded in hierarchical representations in the brain, but previous attempts to visualize perceptual contents have failed to capitalize on multiple levels of the hierarchy, leaving it challenging to reconstruct internal imagery. Recent work showed that visual cortical activity measured by functional magnetic resonance imaging (fMRI) can be decoded (translated) into the hierarchical features of a pre-trained deep neural network (DNN) for the same input image, providing a way to make use of the information from hierarchical visual features. Here, we present a novel image reconstruction method, in which the pixel values of an image are optimized to make its DNN features similar to those decoded from human brain activity at multiple layers. We found that our method was able to reliably produce reconstructions that resembled the viewed natural images. A natural image prior introduced by a deep generator neural network effectively rendered semantically meaningful details to the reconstructions. Human judgment of the reconstructions supported the effectiveness of combining multiple DNN layers to enhance the visual quality of generated images. While our model was solely trained with natural images, it successfully generalized to artificial shapes, indicating that our model was not simply matching to exemplars. The same analysis applied to mental imagery demonstrated rudimentary reconstructions of the subjective content. Our results suggest that our method can effectively combine hierarchical neural representations to reconstruct perceptual and subjective images, providing a new window into the internal contents of the brain.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/GLQ76TPU/Shen et al. - 2019 - Deep image reconstruction from human brain activit.pdf}
}

@article{shirakawaCriticalAssessmentGenerative2023,
  title = {Critical Assessment of Generative {{AI}} Methods and Natural Image Datasets for Visual Image Reconstruction from Brain Activity},
  author = {Shirakawa, Ken and Majima, Kei and Kamitani, Yukiyasu and Tanaka, Misato and Aoki, Shuntaro},
  year = {2023},
  publisher = {OSF},
  doi = {10.17605/OSF.IO/NMFC5},
  urldate = {2024-06-12},
  abstract = {Ongoing progress reports detailing the evaluation and characterization of text-to-image diffusion methods and natural image datasets used in recent image reconstruction studies},
  collaborator = {{Center For Open Science}},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {brain decoding,Deep neural network,generative AI,neuroscience,reconstruction}
}

@misc{shirakawaSpuriousReconstructionBrain2024,
  title = {Spurious Reconstruction from Brain Activity},
  author = {Shirakawa, Ken and Nagano, Yoshihiro and Tanaka, Misato and Aoki, Shuntaro C. and Majima, Kei and Muraki, Yusuke and Kamitani, Yukiyasu},
  year = {2024},
  month = may,
  number = {arXiv:2405.10078},
  eprint = {2405.10078},
  primaryclass = {q-bio},
  publisher = {arXiv},
  urldate = {2024-06-05},
  abstract = {The rapid advances in brain decoding, particularly visual image reconstruction, have sparked discussions about the potential societal implications and ethical considerations surrounding neurotechnology. As these methods aim to recover perceived images from brain activity and achieve prediction over diverse images beyond training samples (zero-shot prediction), it is crucial to critically assess their capabilities and limitations to prevent misguided public expectations and inform future regulations. Our case study of recent text-guided reconstruction methods, which leverage a large-scale dataset (the Natural Scene Dataset, NSD) and text-to-image diffusion models, reveals significant limitations in their generalizability. We found a notable decrease in performance when applying these methods to a different dataset, which was designed to prevent category overlaps between training and test sets. UMAP visualization of the text features with NSD images showed a limited diversity of distinct semantic and visual clusters, with substantial overlap between training and test sets. Formal analysis and simulations demonstrated that clustered training samples can lead to ``output dimension collapse,'' restricting the output feature dimensions predictable from brain activity. Diversifying the training set to ensure a broader feature distribution improved generalizability beyond the trained clusters. However, text features alone are insufficient for a complete mapping to the visual space, even if perfectly predicted from brain activity. We argue that recent photo-like reconstructions may primarily be a blend of classification into trained categories and the generation of convincing yet inauthentic images through text-to-image diffusion (hallucination). To achieve genuine zero-shot prediction, diverse datasets and compositional representations spanning the image space are essential. As neurotechnology advances, engaging in interdisciplinary discussions involving neuroscientists, ethicists, policymakers, and the public is crucial to ensure responsible development and application of these techniques. These discussions should be grounded in a clear understanding of the current capabilities and limitations of the technology, as well as a careful consideration of the potential ethical and societal impacts.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {/Users/matt/Zotero/storage/BP33X6SV/Shirakawa et al. - 2024 - Spurious reconstruction from brain activity.10078}
}

@misc{simonyanVeryDeepConvolutional2014,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2014},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1409.1556},
  urldate = {2025-01-23},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{smithSpatialTemporalScales2008,
  title = {Spatial and {{Temporal Scales}} of {{Neuronal Correlation}} in {{Primary Visual Cortex}}},
  author = {Smith, Matthew A. and Kohn, Adam},
  year = {2008},
  month = nov,
  journal = {The Journal of Neuroscience},
  volume = {28},
  number = {48},
  pages = {12591--12603},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2929-08.2008},
  urldate = {2025-03-11},
  abstract = {The spiking activity of cortical neurons is correlated. For instance, trial-to-trial fluctuations in response strength are shared between neurons, and spikes often occur synchronously. Understanding the properties and mechanisms that generate these forms of correlation is critical for determining their role in cortical processing. We therefore investigated the spatial extent and functional specificity of correlated spontaneous and evoked activity. Because feedforward, recurrent, and feedback pathways have distinct extents and specificity, we reasoned that these measurements could elucidate the contribution of each type of input. We recorded single unit activity with microelectrode arrays which allowed us to measure correlation in many hundreds of pairings, across a large range of spatial scales. Our data show that correlated evoked activity is generated by two mechanisms that link neurons with similar orientation preferences on different spatial scales: one with high temporal precision and a limited spatial extent ({$\sim$}3 mm), and a second that gives rise to correlation on a slow time scale and extends as far as we were able to measure (10 mm). The former is consistent with common input provided by horizontal connections; the latter likely involves feedback from extrastriate cortex. Spontaneous activity was correlated over a similar spatial extent, but approximately twice as strongly as evoked activity. Visual stimuli thus caused a substantial decrease in correlation, particularly at response onset. These properties and the circuit mechanism they imply provide new constraints on the functional role that correlation may play in visual processing.},
  copyright = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
  langid = {english},
  file = {/Users/matt/Zotero/storage/LBARKMKF/Smith and Kohn - 2008 - Spatial and Temporal Scales of Neuronal Correlatio.pdf}
}

@article{staviskyBrainmachineInterfaceCursor2018,
  title = {Brain-Machine Interface Cursor Position Only Weakly Affects Monkey and Human Motor Cortical Activity in the Absence of Arm Movements},
  author = {Stavisky, Sergey D. and Kao, Jonathan C. and Nuyujukian, Paul and Pandarinath, Chethan and Blabe, Christine and Ryu, Stephen I. and Hochberg, Leigh R. and Henderson, Jaimie M. and Shenoy, Krishna V.},
  year = {2018},
  month = nov,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {16357},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-34711-1},
  urldate = {2024-02-02},
  abstract = {Abstract             Brain-machine interfaces (BMIs) that decode movement intentions should ignore neural modulation sources distinct from the intended command. However, neurophysiology and control theory suggest that motor cortex reflects the motor effector's position, which could be a nuisance variable. We investigated motor cortical correlates of BMI cursor position with or without concurrent arm movement. We show in two monkeys that subtracting away estimated neural correlates of position improves online BMI performance only if the animals were allowed to move their arm. To understand why, we compared the neural variance attributable to cursor position when the same task was performed using arm reaching, versus arms-restrained BMI use. Firing rates correlated with both BMI cursor and hand positions, but hand positional effects were greater. To examine whether BMI position influences decoding in people with paralysis, we analyzed data from two intracortical BMI clinical trial participants and performed an online decoder comparison in one participant. We found only small motor cortical correlates, which did not affect performance. These results suggest that arm movement and proprioception are the major contributors to position-related motor cortical correlates. Cursor position visual feedback is therefore unlikely to affect the performance of BMI-driven prosthetic systems being developed for people with paralysis.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/SN22F6ND/Stavisky et al. - 2018 - Brain-machine interface cursor position only weakl.pdf}
}

@article{steinmetzNeuropixelsMiniaturizedHighdensity2021,
  title = {Neuropixels 2.0: {{A}} Miniaturized High-Density Probe for Stable, Long-Term Brain Recordings},
  shorttitle = {Neuropixels 2.0},
  author = {Steinmetz, Nicholas A. and Aydin, Cagatay and Lebedeva, Anna and Okun, Michael and Pachitariu, Marius and Bauza, Marius and Beau, Maxime and Bhagat, Jai and B{\"o}hm, Claudia and Broux, Martijn and Chen, Susu and Colonell, Jennifer and Gardner, Richard J. and Karsh, Bill and Kloosterman, Fabian and Kostadinov, Dimitar and {Mora-Lopez}, Carolina and O'Callaghan, John and Park, Junchol and Putzeys, Jan and Sauerbrei, Britton and {van Daal}, Rik J. J. and Vollan, Abraham Z. and Wang, Shiwei and Welkenhuysen, Marleen and Ye, Zhiwen and Dudman, Joshua T. and Dutta, Barundeb and Hantman, Adam W. and Harris, Kenneth D. and Lee, Albert K. and Moser, Edvard I. and O'Keefe, John and Renart, Alfonso and Svoboda, Karel and H{\"a}usser, Michael and Haesler, Sebastian and Carandini, Matteo and Harris, Timothy D.},
  year = {2021},
  month = apr,
  journal = {Science},
  volume = {372},
  number = {6539},
  pages = {eabf4588},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.abf4588},
  urldate = {2024-02-27},
  abstract = {Measuring the dynamics of neural processing across time scales requires following the spiking of thousands of individual neurons over milliseconds and months. To address this need, we introduce the Neuropixels 2.0 probe together with newly designed analysis algorithms. The probe has more than 5000 sites and is miniaturized to facilitate chronic implants in small mammals and recording during unrestrained behavior. High-quality recordings over long time scales were reliably obtained in mice and rats in six laboratories. Improved site density and arrangement combined with newly created data processing methods enable automatic post hoc correction for brain movements, allowing recording from the same neurons for more than 2 months. These probes and algorithms enable stable recordings from thousands of sites during free behavior, even in small animals such as mice.},
  file = {/Users/matt/Zotero/storage/N2M9DEMN/Steinmetz et al. - 2021 - Neuropixels 2.0 A miniaturized high-density probe.pdf}
}

@misc{sundaramWhenDoesPerceptual2024,
  title = {When {{Does Perceptual Alignment Benefit Vision Representations}}?},
  author = {Sundaram, Shobhita and Fu, Stephanie and Muttenthaler, Lukas and Tamir, Netanel Y. and Chai, Lucy and Kornblith, Simon and Darrell, Trevor and Isola, Phillip},
  year = {2024},
  month = oct,
  number = {arXiv:2410.10817},
  eprint = {2410.10817},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.10817},
  urldate = {2025-01-26},
  abstract = {Humans judge perceptual similarity according to diverse visual attributes, including scene layout, subject location, and camera pose. Existing vision models understand a wide range of semantic abstractions but improperly weigh these attributes and thus make inferences misaligned with human perception. While vision representations have previously benefited from alignment in contexts like image generation, the utility of perceptually aligned representations in more general-purpose settings remains unclear. Here, we investigate how aligning vision model representations to human perceptual judgments impacts their usability across diverse computer vision tasks. We finetune state-of-the-art models on human similarity judgments for image triplets and evaluate them across standard vision benchmarks. We find that aligning models to perceptual judgments yields representations that improve upon the original backbones across many downstream tasks, including counting, segmentation, depth estimation, instance retrieval, and retrieval-augmented generation. In addition, we find that performance is widely preserved on other tasks, including specialized out-of-distribution domains such as in medical imaging and 3D environment frames. Our results suggest that injecting an inductive bias about human perceptual knowledge into vision models can contribute to better representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/MDRY67XY/Sundaram et al. - 2024 - When Does Perceptual Alignment Benefit Vision Repr.pdf;/Users/matt/Zotero/storage/MGJST7G6/2410.html}
}

@article{takagiHighResolutionImageReconstruction,
  title = {High-{{Resolution Image Reconstruction With Latent Diffusion Models From Human Brain Activity}}},
  author = {Takagi, Yu and Nishimoto, Shinji},
  langid = {english},
  file = {/Users/matt/Zotero/storage/GQQKPUKB/Takagi and Nishimoto - High-Resolution Image Reconstruction With Latent D.pdf}
}

@inproceedings{tamPyNeurodeRealtimeNeural2022,
  title = {{{pyNeurode}}: A Real-Time Neural Signal Processing Framework},
  shorttitle = {{{pyNeurode}}},
  booktitle = {2022 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Tam, Wing-Kin and Nolan, Matthew F.},
  year = {2022},
  month = may,
  pages = {1943--1947},
  issn = {2158-1525},
  doi = {10.1109/ISCAS48785.2022.9937512},
  urldate = {2024-03-04},
  abstract = {Accurate decoding of neural signals often requires assigning extracellular waveforms acquired on the same electrode to their originating neurons, a process known as spike sorting. While many offline sorters are available, accurate online sorting of spikes with many channels is still a challenging problem. Existing online sorters either use simple algorithms with low accuracy, can only process a handful of channels, or depend on a complex runtime environment that is difficult to set up. We have developed a state-of-the-art online spike sorting platform in Python that enables large-scale, fully automatic real-time spike sorting and decoding on hundreds of channels. Our system is cross-platform and works seamlessly with the Open Ephys suite of open-source hardware and software widely used in many neuroscience laboratories worldwide. It also comes with a user-friendly graphical user interface to monitor the cluster quality, spike waveforms and neuronal firing rate. Our platform has comparable accuracy to offline sorters and can achieve an end-to-end sorting latency of around 160 ms for 128-channel signals. It will be useful for research in fundamental neuroscience, closed-loop feedback neuromodulation and brain-computer interfaces.},
  keywords = {brain-computer interface,Decoding,neural signal processing,Neuroscience,Real-time systems,Runtime environment,Signal processing,Signal processing algorithms,Software,spike sorting},
  file = {/Users/matt/Zotero/storage/MJX4926D/Tam and Nolan - 2022 - pyNeurode a real-time neural signal processing fr.pdf;/Users/matt/Zotero/storage/IEWS9NRI/9937512.html}
}

@inproceedings{tamPyNeurodeRealtimeNeural2022a,
  title = {{{pyNeurode}}: A Real-Time Neural Signal Processing Framework},
  shorttitle = {{{pyNeurode}}},
  booktitle = {2022 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Tam, Wing-Kin and Nolan, Matthew F.},
  year = {2022},
  month = may,
  pages = {1943--1947},
  issn = {2158-1525},
  doi = {10.1109/ISCAS48785.2022.9937512},
  urldate = {2024-03-19},
  abstract = {Accurate decoding of neural signals often requires assigning extracellular waveforms acquired on the same electrode to their originating neurons, a process known as spike sorting. While many offline sorters are available, accurate online sorting of spikes with many channels is still a challenging problem. Existing online sorters either use simple algorithms with low accuracy, can only process a handful of channels, or depend on a complex runtime environment that is difficult to set up. We have developed a state-of-the-art online spike sorting platform in Python that enables large-scale, fully automatic real-time spike sorting and decoding on hundreds of channels. Our system is cross-platform and works seamlessly with the Open Ephys suite of open-source hardware and software widely used in many neuroscience laboratories worldwide. It also comes with a user-friendly graphical user interface to monitor the cluster quality, spike waveforms and neuronal firing rate. Our platform has comparable accuracy to offline sorters and can achieve an end-to-end sorting latency of around 160 ms for 128-channel signals. It will be useful for research in fundamental neuroscience, closed-loop feedback neuromodulation and brain-computer interfaces.},
  keywords = {brain-computer interface,Decoding,neural signal processing,Neuroscience,Real-time systems,Runtime environment,Signal processing,Signal processing algorithms,Software,spike sorting},
  file = {/Users/matt/Zotero/storage/YMEWBG3S/Tam and Nolan - 2022 - pyNeurode a real-time neural signal processing fr.pdf;/Users/matt/Zotero/storage/ZRFV8H44/9937512.html}
}

@misc{volpiAdversarialFeatureAugmentation2018,
  title = {Adversarial {{Feature Augmentation}} for {{Unsupervised Domain Adaptation}}},
  author = {Volpi, Riccardo and Morerio, Pietro and Savarese, Silvio and Murino, Vittorio},
  year = {2018},
  month = may,
  number = {arXiv:1711.08561},
  eprint = {1711.08561},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.08561},
  urldate = {2025-02-28},
  abstract = {Recent works showed that Generative Adversarial Networks (GANs) can be successfully applied in unsupervised domain adaptation, where, given a labeled source dataset and an unlabeled target dataset, the goal is to train powerful classifiers for the target samples. In particular, it was shown that a GAN objective function can be used to learn target features indistinguishable from the source ones. In this work, we extend this framework by (i) forcing the learned feature extractor to be domain-invariant, and (ii) training it through data augmentation in the feature space, namely performing feature augmentation. While data augmentation in the image space is a well established technique in deep learning, feature augmentation has not yet received the same level of attention. We accomplish it by means of a feature generator trained by playing the GAN minimax game against source features. Results show that both enforcing domain-invariance and performing feature augmentation lead to superior or comparable performance to state-of-the-art results in several unsupervised domain adaptation benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/ZCU3CEGN/Volpi et al. - 2018 - Adversarial Feature Augmentation for Unsupervised .pdf;/Users/matt/Zotero/storage/8YPPE5S9/1711.html}
}

@inproceedings{wangDataDropoutOptimizing2018,
  title = {Data {{Dropout}}: {{Optimizing Training Data}} for {{Convolutional Neural Networks}}},
  shorttitle = {Data {{Dropout}}},
  booktitle = {2018 {{IEEE}} 30th {{International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}})},
  author = {Wang, Tianyang and Huan, Jun and Li, Bo},
  year = {2018},
  month = nov,
  pages = {39--46},
  publisher = {IEEE},
  address = {Volos},
  doi = {10.1109/ICTAI.2018.00017},
  urldate = {2024-11-15},
  isbn = {978-1-5386-7449-9},
  file = {/Users/matt/Zotero/storage/P4FI5G93/Wang et al. - 2018 - Data Dropout Optimizing Training Data for Convolu.pdf}
}

@misc{wangDatasetDistillation2018,
  title = {Dataset {{Distillation}}},
  author = {Wang, Tongzhou and Zhu, Jun-Yan and Torralba, Antonio and Efros, Alexei A.},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1811.10959},
  urldate = {2025-03-05},
  abstract = {Model distillation aims to distill the knowledge of a complex model into a simpler one. In this paper, we consider an alternative formulation called dataset distillation: we keep the model fixed and instead attempt to distill the knowledge from a large training dataset into a small one. The idea is to synthesize a small number of data points that do not need to come from the correct data distribution, but will, when given to the learning algorithm as training data, approximate the model trained on the original data. For example, we show that it is possible to compress 60,000 MNIST training images into just 10 synthetic distilled images (one per class) and achieve close to original performance with only a few gradient descent steps, given a fixed network initialization. We evaluate our method in various initialization settings and with different learning objectives. Experiments on multiple datasets show the advantage of our approach compared to alternative methods.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/Users/matt/Zotero/storage/8Q7VV242/Wang et al. - 2018 - Dataset Distillation.pdf}
}

@article{wangLessBetterUnweighted2020,
  title = {Less {{Is Better}}: {{Unweighted Data Subsampling}} via {{Influence Function}}},
  shorttitle = {Less {{Is Better}}},
  author = {Wang, Zifeng and Zhu, Hong and Dong, Zhenhua and He, Xiuqiang and Huang, Shao-Lun},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {6340--6347},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i04.6103},
  urldate = {2024-11-15},
  abstract = {In the time of Big Data, training complex models on large-scale data sets is challenging, making it appealing to reduce data volume for saving computation resources by subsampling. Most previous works in subsampling are weighted methods designed to help the performance of subset-model approach the full-set-model, hence the weighted methods have no chance to acquire a subset-model that is better than the full-set-model. However, we question that how can we achieve better model with less data? In this work, we propose a novel Unweighted Influence Data Subsampling (UIDS) method, and prove that the subset-model acquired through our method can outperform the full-set-model. Besides, we show that overly confident on a given test set for sampling is common in Influence-based subsampling methods, which can eventually cause our subset-model's failure in out-of-sample test. To mitigate it, we develop a probabilistic sampling scheme to control the worst-case risk over all distributions close to the empirical distribution. The experiment results demonstrate our methods superiority over existed subsampling methods in diverse tasks, such as text classification, image classification, click-through prediction, etc.},
  copyright = {https://www.aaai.org},
  file = {/Users/matt/Zotero/storage/CVDA772J/Wang et al. - 2020 - Less Is Better Unweighted Data Subsampling via In.pdf}
}

@misc{wenHardPromptsMade2023,
  title = {Hard {{Prompts Made Easy}}: {{Gradient-Based Discrete Optimization}} for {{Prompt Tuning}} and {{Discovery}}},
  shorttitle = {Hard {{Prompts Made Easy}}},
  author = {Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2302.03668},
  urldate = {2025-03-04},
  abstract = {The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical "hard" prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also "soft" prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface. We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effective in tuning LMs for classification.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/Users/matt/Zotero/storage/VFPQSI5X/Wen et al. - 2023 - Hard Prompts Made Easy Gradient-Based Discrete Op.pdf}
}

@misc{witteveenInvestigatingPromptEngineering2022,
  title = {Investigating {{Prompt Engineering}} in {{Diffusion Models}}},
  author = {Witteveen, Sam and Andrews, Martin},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2211.15462},
  urldate = {2025-03-04},
  abstract = {With the spread of the use of Text2Img diffusion models such as DALL-E 2, Imagen, Mid Journey and Stable Diffusion, one challenge that artists face is selecting the right prompts to achieve the desired artistic output. We present techniques for measuring the effect that specific words and phrases in prompts have, and (in the Appendix) present guidance on the selection of prompts to produce desired effects.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{wolfeFiveFactorsThat2017,
  title = {Five Factors That Guide Attention in Visual Search},
  author = {Wolfe, Jeremy M. and Horowitz, Todd S.},
  year = {2017},
  month = mar,
  journal = {Nature Human Behaviour},
  volume = {1},
  number = {3},
  pages = {0058},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0058},
  urldate = {2025-03-03},
  langid = {english},
  file = {/Users/matt/Zotero/storage/KILYZH33/Wolfe and Horowitz - 2017 - Five factors that guide attention in visual search.pdf}
}

@incollection{wolfeVisualAttention2000,
  title = {Visual {{Attention}}},
  booktitle = {Seeing},
  author = {Wolfe, Jeremy M.},
  year = {2000},
  pages = {335--386},
  publisher = {Elsevier},
  doi = {10.1016/B978-012443760-9/50010-6},
  urldate = {2025-03-30},
  isbn = {978-0-12-443760-9},
  langid = {english},
  file = {/Users/matt/Zotero/storage/D4I8KNSN/Wolfe - 2000 - Visual Attention.pdf}
}

@article{woodVariabilityManualSpike2004,
  title = {On the Variability of Manual Spike Sorting},
  author = {Wood, F. and Black, M.J. and {Vargas-Irwin}, C. and Fellows, M. and Donoghue, J.P.},
  year = {2004},
  month = jun,
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {51},
  number = {6},
  pages = {912--918},
  issn = {1558-2531},
  doi = {10.1109/TBME.2004.826677},
  urldate = {2024-02-22},
  abstract = {The analysis of action potentials, or "spikes," is central to systems neuroscience research. Spikes are typically identified from raw waveforms manually for off-line analysis or automatically by human-configured algorithms for on-line applications. The variability of manual spike "sorting" is studied and its implications for neural prostheses discussed. Waveforms were recorded using a micro-electrode array and were used to construct a statistically similar synthetic dataset. Results showed wide variability in the number of neurons and spikes detected in real data. Additionally, average error rates of 23\% false positive and 30\% false negative were found for synthetic data.},
  keywords = {Computer science,Decoding,Electrodes,Electrophysiology,Neural prosthesis,Neurons,Neuroscience,Prosthetics,Signal to noise ratio,Sorting},
  file = {/Users/matt/Zotero/storage/DTQAX4DA/Wood et al. - 2004 - On the variability of manual spike sorting.pdf;/Users/matt/Zotero/storage/8RATT57J/1300782.html}
}

@misc{xieAdversarialExamplesImprove2020,
  title = {Adversarial {{Examples Improve Image Recognition}}},
  author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan and Le, Quoc V.},
  year = {2020},
  month = apr,
  number = {arXiv:1911.09665},
  eprint = {1911.09665},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.09665},
  urldate = {2025-02-28},
  abstract = {Adversarial examples are commonly viewed as a threat to ConvNets. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose AdvProp, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. We show that AdvProp improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying AdvProp to the latest EfficientNet-B7 [28] on ImageNet, we achieve significant improvements on ImageNet (+0.7\%), ImageNet-C (+6.5\%), ImageNet-A (+7.0\%), Stylized-ImageNet (+4.8\%). With an enhanced EfficientNet-B8, our method achieves the state-of-the-art 85.5\% ImageNet top-1 accuracy without extra data. This result even surpasses the best model in [20] which is trained with 3.5B Instagram images ({\textasciitilde}3000X more than ImageNet) and {\textasciitilde}9.4X more parameters. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/F5UPTHUH/Xie et al. - 2020 - Adversarial Examples Improve Image Recognition.pdf;/Users/matt/Zotero/storage/NISUKJY3/1911.html}
}

@inproceedings{xuPromptFreeDiffusionTaking2024,
  title = {Prompt-{{Free Diffusion}}: {{Taking}} ``{{Text}}'' {{Out}} of {{Text-to-Image Diffusion Models}}},
  shorttitle = {Prompt-{{Free Diffusion}}},
  booktitle = {2024 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xu, Xingqian and Guo, Jiayi and Wang, Zhangyang and Huang, Gao and Essa, Irfan and Shi, Humphrey},
  year = {2024},
  month = jun,
  pages = {8682--8692},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR52733.2024.00829},
  urldate = {2025-03-04},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350353006},
  file = {/Users/matt/Zotero/storage/T53P7NZW/Xu et al. - 2024 - Prompt-Free Diffusion Taking “Text” Out of Text-t.pdf}
}

@inproceedings{xuPromptFreeDiffusionTaking2024a,
  title = {Prompt-{{Free Diffusion}}: {{Taking}} ``{{Text}}'' {{Out}} of {{Text-to-Image Diffusion Models}}},
  shorttitle = {Prompt-{{Free Diffusion}}},
  booktitle = {2024 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xu, Xingqian and Guo, Jiayi and Wang, Zhangyang and Huang, Gao and Essa, Irfan and Shi, Humphrey},
  year = {2024},
  month = jun,
  pages = {8682--8692},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR52733.2024.00829},
  urldate = {2025-03-04},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350353006},
  file = {/Users/matt/Zotero/storage/K9JH5ELL/Xu et al. - 2024 - Prompt-Free Diffusion Taking “Text” Out of Text-t.pdf}
}

@misc{xuVersatileDiffusionText2024,
  title = {Versatile {{Diffusion}}: {{Text}}, {{Images}} and {{Variations All}} in {{One Diffusion Model}}},
  shorttitle = {Versatile {{Diffusion}}},
  author = {Xu, Xingqian and Wang, Zhangyang and Zhang, Eric and Wang, Kai and Shi, Humphrey},
  year = {2024},
  month = jan,
  number = {arXiv:2211.08332},
  eprint = {2211.08332},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.08332},
  urldate = {2025-03-03},
  abstract = {Recent advances in diffusion models have set an impressive milestone in many generation tasks, and trending works such as DALL-E2, Imagen, and Stable Diffusion have attracted great interest. Despite the rapid landscape changes, recent new approaches focus on extensions and performance rather than capacity, thus requiring separate models for separate tasks. In this work, we expand the existing single-flow diffusion pipeline into a multi-task multimodal network, dubbed Versatile Diffusion (VD), that handles multiple flows of text-to-image, image-to-text, and variations in one unified model. The pipeline design of VD instantiates a unified multi-flow diffusion framework, consisting of sharable and swappable layer modules that enable the crossmodal generality beyond images and text. Through extensive experiments, we demonstrate that VD successfully achieves the following: a) VD outperforms the baseline approaches and handles all its base tasks with competitive quality; b) VD enables novel extensions such as disentanglement of style and semantics, dual- and multi-context blending, etc.; c) The success of our multi-flow multimodal framework over images and text may inspire further diffusion-based universal AI research. Our code and models are open-sourced at https://github.com/SHI-Labs/Versatile-Diffusion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/8QHEYZ8P/Xu et al. - 2024 - Versatile Diffusion Text, Images and Variations A.pdf;/Users/matt/Zotero/storage/8A57VHKY/2211.html}
}

@article{yamashitaSparseEstimationAutomatically2008,
  title = {Sparse Estimation Automatically Selects Voxels Relevant for the Decoding of {{fMRI}} Activity Patterns},
  author = {Yamashita, Okito and Sato, Masa-aki and Yoshioka, Taku and Tong, Frank and Kamitani, Yukiyasu},
  year = {2008},
  month = oct,
  journal = {NeuroImage},
  volume = {42},
  number = {4},
  pages = {1414--1429},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2008.05.050},
  urldate = {2025-03-12},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/matt/Zotero/storage/TT5XIQNM/Yamashita et al. - 2008 - Sparse estimation automatically selects voxels rel.pdf;/Users/matt/Zotero/storage/UKR6I6R4/Yamashita et al. - 2008 - Sparse estimation automatically selects voxels rel.pdf}
}

@inproceedings{yanEnhancingClassificationPerformance2023,
  title = {Enhancing {{Classification Performance}} in {{Knee Magnetic Resonance Imaging Using Adversarial Data Augmentation}}},
  booktitle = {2023 {{IEEE}} 14th {{International Conference}} on {{Software Engineering}} and {{Service Science}} ({{ICSESS}})},
  author = {Yan, Zhongbo and Yang, Xu and Chong, Chak Fong and Wang, Yapeng},
  year = {2023},
  month = oct,
  pages = {19--24},
  publisher = {IEEE},
  address = {Beijing, China},
  doi = {10.1109/ICSESS58500.2023.10293076},
  urldate = {2025-02-28},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350336269 9798350336276},
  file = {/Users/matt/Zotero/storage/PF4R9HK2/Yan et al. - 2023 - Enhancing Classification Performance in Knee Magne.pdf}
}

@article{yuDatasetDistillationComprehensive2024,
  title = {Dataset {{Distillation}}: {{A Comprehensive Review}}},
  shorttitle = {Dataset {{Distillation}}},
  author = {Yu, Ruonan and Liu, Songhua and Wang, Xinchao},
  year = {2024},
  month = jan,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {46},
  number = {1},
  pages = {150--170},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2023.3323376},
  urldate = {2024-11-18},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {/Users/matt/Zotero/storage/JJ9K4VGG/Yu et al. - 2024 - Dataset Distillation A Comprehensive Review.pdf}
}

@misc{zhangLongCLIPUnlockingLongText2024,
  title = {Long-{{CLIP}}: {{Unlocking}} the {{Long-Text Capability}} of {{CLIP}}},
  shorttitle = {Long-{{CLIP}}},
  author = {Zhang, Beichen and Zhang, Pan and Dong, Xiaoyi and Zang, Yuhang and Wang, Jiaqi},
  year = {2024},
  month = jul,
  number = {arXiv:2403.15378},
  eprint = {2403.15378},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.15378},
  urldate = {2024-12-18},
  abstract = {Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for zero-shot classification, text-image retrieval, and text-image generation by aligning image and text modalities. Despite its widespread adoption, a significant limitation of CLIP lies in the inadequate length of text input. The length of the text token is restricted to 77, and an empirical study shows the actual effective length is even less than 20. This prevents CLIP from handling detailed descriptions, limiting its applications for image retrieval and text-to-image generation with extensive prerequisites. To this end, we propose Long-CLIP as a plug-and-play alternative to CLIP that supports long-text input, retains or even surpasses its zero-shot generalizability, and aligns the CLIP latent space, making it readily replace CLIP without any further adaptation in downstream frameworks. Nevertheless, achieving this goal is far from straightforward, as simplistic fine-tuning can result in a significant degradation of CLIP's performance. Moreover, substituting the text encoder with a language model supporting longer contexts necessitates pretraining with vast amounts of data, incurring significant expenses. Accordingly, Long-CLIP introduces an efficient fine-tuning solution on CLIP with two novel strategies designed to maintain the original capabilities, including (1) a knowledge-preserved stretching of positional embedding and (2) a primary component matching of CLIP features. With leveraging just one million extra long text-image pairs, Long-CLIP has shown the superiority to CLIP for about 20\% in long caption text-image retrieval and 6\% in traditional text-image retrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers enhanced capabilities for generating images from detailed text descriptions by replacing CLIP in a plug-and-play manner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/YQRP6DLR/Zhang et al. - 2024 - Long-CLIP Unlocking the Long-Text Capability of C.pdf;/Users/matt/Zotero/storage/WGF9YILB/2403.html}
}

@misc{zhaoNovelCrossPerturbationSingle2024,
  title = {A {{Novel Cross-Perturbation}} for {{Single Domain Generalization}}},
  author = {Zhao, Dongjia and Qi, Lei and Shi, Xiao and Shi, Yinghuan and Geng, Xin},
  year = {2024},
  month = jun,
  number = {arXiv:2308.00918},
  eprint = {2308.00918},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.00918},
  urldate = {2025-02-28},
  abstract = {Single domain generalization aims to enhance the ability of the model to generalize to unknown domains when trained on a single source domain. However, the limited diversity in the training data hampers the learning of domain-invariant features, resulting in compromised generalization performance. To address this, data perturbation (augmentation) has emerged as a crucial method to increase data diversity. Nevertheless, existing perturbation methods often focus on either image-level or feature-level perturbations independently, neglecting their synergistic effects. To overcome these limitations, we propose CPerb, a simple yet effective cross-perturbation method. Specifically, CPerb utilizes both horizontal and vertical operations. Horizontally, it applies image-level and feature-level perturbations to enhance the diversity of the training data, mitigating the issue of limited diversity in single-source domains. Vertically, it introduces multi-route perturbation to learn domain-invariant features from different perspectives of samples with the same semantic category, thereby enhancing the generalization capability of the model. Additionally, we propose MixPatch, a novel feature-level perturbation method that exploits local image style information to further diversify the training data. Extensive experiments on various benchmark datasets validate the effectiveness of our method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/624Y9U9M/Zhao et al. - 2024 - A Novel Cross-Perturbation for Single Domain Gener.pdf;/Users/matt/Zotero/storage/W82V2UBQ/2308.html}
}
