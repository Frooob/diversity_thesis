@article{ahmedRecentReviewImage2015,
  title = {Recent Review on Image Clustering},
  author = {Ahmed, Nasir},
  year = {2015},
  month = nov,
  journal = {IET Image Processing},
  volume = {9},
  number = {11},
  pages = {1020--1032},
  issn = {1751-9667, 1751-9667},
  doi = {10.1049/iet-ipr.2014.0885},
  urldate = {2024-11-26},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {/Users/matt/Zotero/storage/UHHBU5PA/Ahmed - 2015 - Recent review on image clustering.pdf}
}

@article{allenMassive7TFMRI2022,
  title = {A Massive {{7T fMRI}} Dataset to Bridge Cognitive Neuroscience and Artificial Intelligence},
  author = {Allen, Emily J. and {St-Yves}, Ghislain and Wu, Yihan and Breedlove, Jesse L. and Prince, Jacob S. and Dowdle, Logan T. and Nau, Matthias and Caron, Brad and Pestilli, Franco and Charest, Ian and Hutchinson, J. Benjamin and Naselaris, Thomas and Kay, Kendrick},
  year = {2022},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {25},
  number = {1},
  pages = {116--126},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-021-00962-x},
  urldate = {2025-01-21},
  langid = {english},
  file = {/Users/matt/Zotero/storage/VRE8CBJ4/Allen et al. - 2022 - A massive 7T fMRI dataset to bridge cognitive neur.pdf}
}

@article{baiSurveyAutomaticImage2018,
  title = {A Survey on Automatic Image Caption Generation},
  author = {Bai, Shuang and An, Shan},
  year = {2018},
  month = oct,
  journal = {Neurocomputing},
  volume = {311},
  pages = {291--304},
  issn = {09252312},
  doi = {10.1016/j.neucom.2018.05.080},
  urldate = {2025-03-04},
  langid = {english},
  file = {/Users/matt/Zotero/storage/ABQMC8E5/Bai and An - 2018 - A survey on automatic image caption generation.pdf}
}

@article{barTopdownFacilitationVisual2006,
  title = {Top-down Facilitation of Visual Recognition},
  author = {Bar, M. and Kassam, K. S. and Ghuman, A. S. and Boshyan, J. and Schmid, A. M. and Dale, A. M. and H{\"a}m{\"a}l{\"a}inen, M. S. and Marinkovic, K. and Schacter, D. L. and Rosen, B. R. and Halgren, E.},
  year = {2006},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {103},
  number = {2},
  pages = {449--454},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0507062103},
  urldate = {2025-04-02},
  abstract = {Cortical analysis related to visual object recognition is traditionally thought to propagate serially along a bottom-up hierarchy of ventral areas. Recent proposals gradually promote the role of top-down processing in recognition, but how such facilitation is triggered remains a puzzle. We tested a specific model, proposing that low spatial frequencies facilitate visual object recognition by initiating top-down processes projected from orbitofrontal to visual cortex. The present study combined magnetoencephalography, which has superior temporal resolution, functional magnetic resonance imaging, and a behavioral task that yields successful recognition with stimulus repetitions. Object recognition elicited differential activity that developed in the left orbitofrontal cortex 50 ms earlier than it did in recognition-related areas in the temporal cortex. This early orbitofrontal activity was directly modulated by the presence of low spatial frequencies in the image. Taken together, the dynamics we revealed provide strong support for the proposal of how top-down facilitation of object recognition is initiated, and our observations are used to derive predictions for future research.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/IKGAWG64/Bar et al. - 2006 - Top-down facilitation of visual recognition.pdf}
}

@article{bennettIntrinsicDimensionalitySignal1969,
  title = {The Intrinsic Dimensionality of Signal Collections},
  author = {Bennett, R.},
  year = {1969},
  month = sep,
  journal = {IEEE Transactions on Information Theory},
  volume = {15},
  number = {5},
  pages = {517--525},
  issn = {0018-9448},
  doi = {10.1109/TIT.1969.1054365},
  urldate = {2025-04-03},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english}
}

@article{bianWhenDoesDiversity2022,
  title = {When {{Does Diversity Help Generalization}} in {{Classification Ensembles}}?},
  author = {Bian, Yijun and Chen, Huanhuan},
  year = {2022},
  month = sep,
  journal = {IEEE Transactions on Cybernetics},
  volume = {52},
  number = {9},
  pages = {9059--9075},
  issn = {2168-2267, 2168-2275},
  doi = {10.1109/TCYB.2021.3053165},
  urldate = {2025-04-01},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {/Users/matt/Zotero/storage/JJ3CJZDJ/Bian and Chen - 2022 - When Does Diversity Help Generalization in Classif.pdf}
}

@misc{birodkarSemanticRedundanciesImageClassification2019,
  title = {Semantic {{Redundancies}} in {{Image-Classification Datasets}}: {{The}} 10\% {{You Don}}'t {{Need}}},
  shorttitle = {Semantic {{Redundancies}} in {{Image-Classification Datasets}}},
  author = {Birodkar, Vighnesh and Mobahi, Hossein and Bengio, Samy},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1901.11409},
  urldate = {2024-11-18},
  abstract = {Large datasets have been crucial to the success of deep learning models in the recent years, which keep performing better as they are trained with more labelled data. While there have been sustained efforts to make these models more data-efficient, the potential benefit of understanding the data itself, is largely untapped. Specifically, focusing on object recognition tasks, we wonder if for common benchmark datasets we can do better than random subsets of the data and find a subset that can generalize on par with the full dataset when trained on. To our knowledge, this is the first result that can find notable redundancies in CIFAR-10 and ImageNet datasets (at least 10\%). Interestingly, we observe semantic correlations between required and redundant images. We hope that our findings can motivate further research into identifying additional redundancies and exploiting them for more efficient training or data-collection.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/Users/matt/Zotero/storage/77RLGCST/Birodkar et al. - 2019 - Semantic Redundancies in Image-Classification Data.pdf}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-31073-2},
  langid = {english},
  lccn = {Q327 .B52 2006},
  keywords = {Machine learning,Pattern perception},
  file = {/Users/matt/Zotero/storage/V6QBMMNE/Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@article{canatarOutofDistributionGeneralizationKernel,
  title = {Out-of-{{Distribution Generalization}} in {{Kernel Regression}}},
  author = {Canatar, Abdulkadir and Bordelon, Blake and Pehlevan, Cengiz},
  abstract = {In real word applications, the data generating process for training a machine learning model often differs from what the model encounters in the test stage. Understanding how and whether machine learning models generalize under such distributional shifts remains a theoretical challenge. Here, we study generalization in kernel regression when the training and test distributions are different using the replica method from statistical physics. We derive an analytical formula for the out-of-distribution generalization error applicable to any kernel and real datasets. We identify an overlap matrix that quantifies the mismatch between distributions for a given kernel as a key determinant of generalization performance under distribution shift. Using our analytical expressions we elucidate various generalization phenomena including possible improvement in generalization when there is a mismatch. We develop procedures for optimizing training and test distributions for a given data budget to find best and worst case generalizations under the shift. We present applications of our theory to real and synthetic datasets and for many kernels. We compare results of our theory applied to Neural Tangent Kernel with simulations of wide networks and show agreement. We analyze linear regression in further depth.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/QM83882D/Canatar et al. - Out-of-Distribution Generalization in Kernel Regre.pdf}
}

@article{canatarSpectralBiasTaskmodel2021,
  title = {Spectral Bias and Task-Model Alignment Explain Generalization in Kernel Regression and Infinitely Wide Neural Networks},
  author = {Canatar, Abdulkadir and Bordelon, Blake and Pehlevan, Cengiz},
  year = {2021},
  month = may,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {2914},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-23103-1},
  urldate = {2024-06-07},
  abstract = {Abstract             A theoretical understanding of generalization remains an open problem for many machine learning models, including deep networks where overparameterization leads to better performance, contradicting the conventional wisdom from classical statistics. Here, we investigate generalization error for kernel regression, which, besides being a popular machine learning method, also describes certain infinitely overparameterized neural networks. We use techniques from statistical mechanics to derive an analytical expression for generalization error applicable to any kernel and data distribution. We present applications of our theory to real and synthetic datasets, and for many kernels including those that arise from training deep networks in the infinite-width limit. We elucidate an inductive bias of kernel regression to explain data with simple functions, characterize whether a kernel is compatible with a learning task, and show that more data may impair generalization when noisy or not expressible by the kernel, leading to non-monotonic learning curves with possibly many peaks.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/3XF74247/Canatar et al. - 2021 - Spectral bias and task-model alignment explain gen.pdf}
}

@inproceedings{chang2017deep,
  title = {Deep Adaptive Image Clustering},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Chang, Jianlong and Wang, Lingfeng and Meng, Gaofeng and Xiang, Shiming and Pan, Chunhong},
  year = {2017},
  pages = {5879--5887}
}

@misc{chenExploringNaturalnessAIGenerated2023,
  title = {Exploring the {{Naturalness}} of {{AI-Generated Images}}},
  author = {Chen, Zijian and Sun, Wei and Wu, Haoning and Zhang, Zicheng and Jia, Jun and Ji, Zhongpeng and Sun, Fengyu and Jui, Shangling and Min, Xiongkuo and Zhai, Guangtao and Zhang, Wenjun},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2312.05476},
  urldate = {2025-03-04},
  abstract = {The proliferation of Artificial Intelligence-Generated Images (AGIs) has greatly expanded the Image Naturalness Assessment (INA) problem. Different from early definitions that mainly focus on tone-mapped images with limited distortions (e.g., exposure, contrast, and color reproduction), INA on AI-generated images is especially challenging as it has more diverse contents and could be affected by factors from multiple perspectives, including low-level technical distortions and high-level rationality distortions. In this paper, we take the first step to benchmark and assess the visual naturalness of AI-generated images. First, we construct the AI-Generated Image Naturalness (AGIN) database by conducting a large-scale subjective study to collect human opinions on the overall naturalness as well as perceptions from technical and rationality perspectives. AGIN verifies that naturalness is universally and disparately affected by technical and rationality distortions. Second, we propose the Joint Objective Image Naturalness evaluaTor (JOINT), to automatically predict the naturalness of AGIs that aligns human ratings. Specifically, JOINT imitates human reasoning in naturalness evaluation by jointly learning both technical and rationality features. We demonstrate that JOINT significantly outperforms baselines for providing more subjectively consistent results on naturalness assessment.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/matt/Zotero/storage/4778JGY8/Chen et al. - 2023 - Exploring the Naturalness of AI-Generated Images.pdf}
}

@misc{chenExploringNaturalnessAIGenerated2023a,
  title = {Exploring the {{Naturalness}} of {{AI-Generated Images}}},
  author = {Chen, Zijian and Sun, Wei and Wu, Haoning and Zhang, Zicheng and Jia, Jun and Ji, Zhongpeng and Sun, Fengyu and Jui, Shangling and Min, Xiongkuo and Zhai, Guangtao and Zhang, Wenjun},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2312.05476},
  urldate = {2025-03-05},
  abstract = {The proliferation of Artificial Intelligence-Generated Images (AGIs) has greatly expanded the Image Naturalness Assessment (INA) problem. Different from early definitions that mainly focus on tone-mapped images with limited distortions (e.g., exposure, contrast, and color reproduction), INA on AI-generated images is especially challenging as it has more diverse contents and could be affected by factors from multiple perspectives, including low-level technical distortions and high-level rationality distortions. In this paper, we take the first step to benchmark and assess the visual naturalness of AI-generated images. First, we construct the AI-Generated Image Naturalness (AGIN) database by conducting a large-scale subjective study to collect human opinions on the overall naturalness as well as perceptions from technical and rationality perspectives. AGIN verifies that naturalness is universally and disparately affected by technical and rationality distortions. Second, we propose the Joint Objective Image Naturalness evaluaTor (JOINT), to automatically predict the naturalness of AGIs that aligns human ratings. Specifically, JOINT imitates human reasoning in naturalness evaluation by jointly learning both technical and rationality features. We demonstrate that JOINT significantly outperforms baselines for providing more subjectively consistent results on naturalness assessment.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/matt/Zotero/storage/997IBX96/Chen et al. - 2023 - Exploring the Naturalness of AI-Generated Images.pdf}
}

@article{chengReconstructingVisualIllusory2023,
  title = {Reconstructing Visual Illusory Experiences from Human Brain Activity},
  author = {Cheng, Fan L. and Horikawa, Tomoyasu and Majima, Kei and Tanaka, Misato and Abdelhack, Mohamed and Aoki, Shuntaro C. and Hirano, Jin and Kamitani, Yukiyasu},
  year = {2023},
  month = nov,
  journal = {Science Advances},
  volume = {9},
  number = {46},
  pages = {eadj3906},
  issn = {2375-2548},
  doi = {10.1126/sciadv.adj3906},
  urldate = {2025-03-12},
  abstract = {Visual illusions provide valuable insights into the brain's interpretation of the world given sensory inputs. However, the precise manner in which brain activity translates into illusory experiences remains largely unknown. Here, we leverage a brain decoding technique combined with deep neural network (DNN) representations to reconstruct illusory percepts as images from brain activity. The reconstruction model was trained on natural images to establish a link between brain activity and perceptual features and then tested on two types of illusions: illusory lines and neon color spreading. Reconstructions revealed lines and colors consistent with illusory experiences, which varied across the source visual cortical areas. This framework offers a way to materialize subjective experiences, shedding light on the brain's internal representations of the world.           ,              Images generated from brain activity mirror illusory experiences, revealing neural representations behind subjective perception.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/4MF37QDE/Cheng et al. - 2023 - Reconstructing visual illusory experiences from hu.pdf}
}

@misc{childVeryDeepVAEs2020,
  title = {Very {{Deep VAEs Generalize Autoregressive Models}} and {{Can Outperform Them}} on {{Images}}},
  author = {Child, Rewon},
  year = {2020},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2011.10650},
  urldate = {2025-01-22},
  abstract = {We present a hierarchical VAE that, for the first time, generates samples quickly while outperforming the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@misc{chittaTrainingDataSubset2020,
  title = {Training {{Data Subset Search}} with {{Ensemble Active Learning}}},
  author = {Chitta, Kashyap and Alvarez, Jose M. and Haussmann, Elmar and Farabet, Clement},
  year = {2020},
  month = nov,
  number = {arXiv:1905.12737},
  eprint = {1905.12737},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-15},
  abstract = {Deep Neural Networks (DNNs) often rely on very large datasets for training. Given the large size of such datasets, it is conceivable that they contain certain samples that either do not contribute or negatively impact the DNN's performance. If there is a large number of such samples, subsampling the training dataset in a way that removes them could provide an effective solution to both improve performance and reduce training time. In this paper, we propose an approach called Active Dataset Subsampling (ADS), to identify favorable subsets within a dataset for training using ensemble based uncertainty estimation. When applied to three image classification benchmarks (CIFAR-10, CIFAR-100 and ImageNet) we find that there are low uncertainty subsets, which can be as large as 50\% of the full dataset, that negatively impact performance. These subsets are identified and removed with ADS. We demonstrate that datasets obtained using ADS with a lightweight ResNet18 ensemble remain effective when used to train deeper models like ResNet-101. Our results provide strong empirical evidence that using all the available data for training can hurt performance on large scale vision tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/matt/Zotero/storage/3U7UW5SJ/Chitta et al. - 2020 - Training Data Subset Search with Ensemble Active L.pdf}
}

@article{couchSizeClassBalance2024,
  title = {Beyond Size and Class Balance: {{Alpha}} as a New Dataset Quality Metric for Deep Learning},
  author = {Couch, Josiah D. and Arnaout, Ramy A. and Arnaout, Rima},
  year = {2024},
  journal = {ArXiv},
  file = {/Users/matt/Zotero/storage/X9QTGEGR/Couch et al. - 2024 - Beyond size and class balance Alpha as a new data.pdf}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
  year = {2009},
  month = jun,
  pages = {248--255},
  publisher = {IEEE},
  address = {Miami, FL},
  doi = {10.1109/CVPR.2009.5206848},
  urldate = {2025-01-21},
  isbn = {978-1-4244-3992-8}
}

@article{dicarloHowDoesBrain2012,
  title = {How {{Does}} the {{Brain Solve Visual Object Recognition}}?},
  author = {DiCarlo, James~J. and Zoccolan, Davide and Rust, Nicole~C.},
  year = {2012},
  month = feb,
  journal = {Neuron},
  volume = {73},
  number = {3},
  pages = {415--434},
  issn = {08966273},
  doi = {10.1016/j.neuron.2012.01.010},
  urldate = {2025-04-02},
  langid = {english},
  file = {/Users/matt/Zotero/storage/CHQ7JRKI/DiCarlo et al. - 2012 - How Does the Brain Solve Visual Object Recognition.pdf}
}

@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.11929},
  urldate = {2025-02-28},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/5ZL2MQHJ/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf;/Users/matt/Zotero/storage/EU4B83VK/2010.html}
}

@misc{ds001506:1.3.1,
  title = {"{{Deep}} Image Reconstruction"},
  author = {Shen, Guohua and Horikawa, Tomoyasu and Majima, Kei and Kamitani, Yukiyasu},
  year = {2020},
  publisher = {OpenNeuro},
  doi = {10.18112/openneuro.ds001506.v1.3.1}
}

@inproceedings{durdovTrainingDatasetPruning2024,
  title = {Training {{Dataset Pruning Algorithm}} with {{Evaluation}} on {{Medical Datasets}}},
  booktitle = {2024 {{International Conference}} on {{Software}}, {{Telecommunications}} and {{Computer Networks}} ({{SoftCOM}})},
  author = {Durdov, Bo{\v z}o and Prvan, Marina and {\v C}oko, Duje and Musi{\'c}, Josip},
  year = {2024},
  month = sep,
  pages = {1--8},
  publisher = {IEEE},
  address = {Split, Croatia},
  doi = {10.23919/SoftCOM62040.2024.10721725},
  urldate = {2024-11-15},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-953-290-138-2},
  file = {/Users/matt/Zotero/storage/7PS9SDJZ/Durdov et al. - 2024 - Training Dataset Pruning Algorithm with Evaluation.pdf}
}

@article{engelFMRIHumanVisual1994,
  title = {{{fMRI}} of Human Visual Cortex},
  author = {Engel, Stephen A. and Rumelhart, David E. and Wandell, Brian A. and Lee, Adrian T. and Glover, Gary H. and Chichilnisky, Eduardo-Jose and Shadlen, Michael N.},
  year = {1994},
  month = jun,
  journal = {Nature},
  volume = {369},
  number = {6481},
  pages = {525--525},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/369525a0},
  urldate = {2025-01-25},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/matt/Zotero/storage/IW3EQ74J/Engel et al. - 1994 - fMRI of human visual cortex.pdf}
}

@article{epsteinCorticalRepresentationLocal1998,
  title = {A Cortical Representation of the Local Visual Environment},
  author = {Epstein, Russell and Kanwisher, Nancy},
  year = {1998},
  month = apr,
  journal = {Nature},
  volume = {392},
  number = {6676},
  pages = {598--601},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/33402},
  urldate = {2025-01-25},
  copyright = {http://www.springer.com/tdm},
  langid = {english}
}

@article{fellemanDistributedHierarchicalProcessing1991a,
  title = {Distributed Hierarchical Processing in the Primate Cerebral Cortex},
  author = {Felleman, Daniel J. and Van Essen, David C.},
  year = {1991},
  month = jan,
  journal = {Cerebral Cortex},
  volume = {1},
  number = {1},
  pages = {1--47},
  issn = {1047-3211},
  doi = {10.1093/cercor/1.1.1-a},
  abstract = {In recent years, many new cortical areas have been identified in the macaque monkey. The number of iden tified connections hetween areas has increased even more dramatically. We report here on (1) a summary of the layout of cortical areas associated with vision and with other modalities, (2) a computerized database for storing and representing large amounts of information on connectivity patterns, and (3) the application of these data to the analysis of hierarchical organization of the cerebral cortex. Our analysis concentrates on the visual system, which includes 25 neocortical areas that are predominantly or exclusively visual in function, plus an additional 7 areas that we regard as visual-association areas on the basis of their extensive visual inputs. A total of 305 connections among these 32 visual and visual-association areas have been reported. This represents 31\% of the possible number of pathways it each area were connected with all others. The actual degree of connectivity is likely to he closer to 40\%. The great majority of pathways involve reciprocal connections be tween areas. There are also extensive connections with cortical areas outside the visual system proper, including the somatosensory cortex, as well as neocortical, transitional, and archicortical regions in the temporal and frontal lobes. In the somatosensory/motor system, there are 62 identified pathways linking 13 cortical areas, suggesting an overall connectivity of about 40\%. Based on the laminar patterns of connections between areas, we propose a hierarchy of visual areas and of somato sensory/motor areas that is more comprehensive thao those suggested in other recent studies. The current version of the visual hierarchy includes 10 levels of cortical processing. Altogether, it contains 14 levels if one includes the retina and lateral geniculate nucleus at the bottom as well as the entorhinal cortex and hippocampus at the top. Within this hierarchy, there are multiple, intertwined processing streams, which, at a low level, are related to the compartmental organization of areas V1 and V2 and, at a high level, are related to the distinction between processing centers in the temporal and parietal lobes. However, there are some pathways and relationships (about 10\% of the total) whose descriptions do not fit cleanly into this hierarchical scheme for one reason or another. In most instances, though, it is unclear whether these represent genuine exceptions to a strict hierarchy rather than inaccuracies or uncertainties in the reported assignment.}
}

@misc{Fort2021CLIPadversarialstickers,
  title = {Pixels Still Beat Text: {{Attacking}} the {{OpenAI CLIP}} Model with Text Patches and Adversarial Pixel Perturbations},
  author = {Fort, Stanislav},
  year = {2021},
  month = mar
}

@misc{fuDreamSimLearningNew2023,
  title = {{{DreamSim}}: {{Learning New Dimensions}} of {{Human Visual Similarity}} Using {{Synthetic Data}}},
  shorttitle = {{{DreamSim}}},
  author = {Fu, Stephanie and Tamir, Netanel and Sundaram, Shobhita and Chai, Lucy and Zhang, Richard and Dekel, Tali and Isola, Phillip},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2306.09344},
  urldate = {2025-01-26},
  abstract = {Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic content while also being sensitive to color and layout. Notably, despite being trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval and reconstruction tasks. Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks.},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@article{fujiwaraModularEncodingDecoding2013,
  title = {Modular {{Encoding}} and {{Decoding Models Derived}} from {{Bayesian Canonical Correlation Analysis}}},
  author = {Fujiwara, Yusuke and Miyawaki, Yoichi and Kamitani, Yukiyasu},
  year = {2013},
  month = apr,
  journal = {Neural Computation},
  volume = {25},
  number = {4},
  pages = {979--1005},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00423},
  urldate = {2025-03-12},
  abstract = {Neural encoding and decoding provide perspectives for understanding neural representations of sensory inputs. Recent functional magnetic resonance imaging (fMRI) studies have succeeded in building prediction models for encoding and decoding numerous stimuli by representing a complex stimulus as a combination of simple elements. While arbitrary visual images were reconstructed using a modular model that combined the outputs of decoder modules for multiscale local image bases (elements), the shapes of the image bases were heuristically determined. In this work, we propose a method to establish mappings between the stimulus and the brain by automatically extracting modules from measured data. We develop a model based on Bayesian canonical correlation analysis, in which each module is modeled by a latent variable that relates a set of pixels in a visual image to a set of voxels in an fMRI activity pattern. The estimated mapping from a latent variable to pixels can be regarded as an image basis. We show that the model estimates a modular representation with spatially localized multiscale image bases. Further, using the estimated mappings, we derive encoding and decoding models that produce accurate predictions for brain activity and stimulus images. Our approach thus provides a novel means of revealing neural representations of stimuli by automatically extracting modules, which can be used to generate effective prediction models for encoding and decoding.},
  langid = {english}
}

@misc{gifford7TFMRIDataset2025,
  title = {A {{7T fMRI}} Dataset of Synthetic Images for Out-of-Distribution Modeling of Vision},
  author = {Gifford, Alessandro T. and Cichy, Radoslaw M. and Naselaris, Thomas and Kay, Kendrick},
  year = {2025},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2503.06286},
  urldate = {2025-04-03},
  abstract = {Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD) are boosting NeuroAI research by enabling computational models of the brain with performances beyond what was possible just a decade ago. However, these datasets lack out-of-distribution (OOD) components, which are crucial for the development of more robust models. Here, we address this limitation by releasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the eight NSD subjects for 284 carefully controlled synthetic images. We show that NSD-synthetic's fMRI responses reliably encode stimulus-related information and are OOD with respect to NSD. Furthermore, OOD generalization tests on NSD-synthetic reveal differences between models of the brain that are not detected with NSD - specifically, self-supervised deep neural networks better explain neural responses than their task-supervised counterparts. These results showcase how NSD-synthetic enables OOD generalization tests that facilitate the development of more robust models of visual processing, and the formulation of more accurate theories of human vision.},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  keywords = {FOS: Biological sciences,Neurons and Cognition (q-bio.NC)}
}

@article{goldman-rakicPrefaceCerebralCortex1991,
  title = {Preface: {{Cerebral Cortex Has Come}} of {{Age}}},
  shorttitle = {Preface},
  author = {{Goldman-Rakic}, P. S. and Rakic, P.},
  year = {1991},
  month = jan,
  journal = {Cerebral Cortex},
  volume = {1},
  number = {1},
  pages = {1--1},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/1.1.1-a},
  urldate = {2025-04-02},
  langid = {english}
}

@article{goldman-rakicPrefaceCerebralCortex1991a,
  title = {Preface: {{Cerebral Cortex Has Come}} of {{Age}}},
  shorttitle = {Preface},
  author = {{Goldman-Rakic}, P. S. and Rakic, P.},
  year = {1991},
  month = jan,
  journal = {Cerebral Cortex},
  volume = {1},
  number = {1},
  pages = {1--1},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/1.1.1-a},
  urldate = {2025-04-02},
  langid = {english}
}

@article{gongDiversityMachineLearning2019,
  title = {Diversity in {{Machine Learning}}},
  author = {Gong, Zhiqiang and Zhong, Ping and Hu, Weidong},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {64323--64350},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2917620},
  urldate = {2025-04-01},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/OAPA.html},
  file = {/Users/matt/Zotero/storage/I6NGTLVT/Gong et al. - 2019 - Diversity in Machine Learning.pdf}
}

@misc{goodfellowExplainingHarnessingAdversarial2014,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  year = {2014},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1412.6572},
  urldate = {2025-02-21},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@article{grill-spectorHUMANVISUALCORTEX2004,
  title = {{{THE HUMAN VISUAL CORTEX}}},
  author = {{Grill-Spector}, Kalanit and Malach, Rafael},
  year = {2004},
  month = jul,
  journal = {Annual Review of Neuroscience},
  volume = {27},
  number = {1},
  pages = {649--677},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev.neuro.27.070203.144220},
  urldate = {2025-03-12},
  abstract = {▪ Abstract{\enspace} The discovery and analysis of cortical visual areas is a major accomplishment of visual neuroscience. In the past decade the use of noninvasive functional imaging, particularly functional magnetic resonance imaging (fMRI), has dramatically increased our detailed knowledge of the functional organization of the human visual cortex and its relation to visual perception. The fMRI method offers a major advantage over other techniques applied in neuroscience by providing a large-scale neuroanatomical perspective that stems from its ability to image the entire brain essentially at once. This bird's eye view has the potential to reveal large-scale principles within the very complex plethora of visual areas. Thus, it could arrange the entire constellation of human visual areas in a unified functional organizational framework. Here we review recent findings and methods employed to uncover the functional properties of the human visual cortex focusing on two themes: functional specialization and hierarchical processing.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/CXHMKBTP/Grill-Spector and Malach - 2004 - THE HUMAN VISUAL CORTEX.pdf}
}

@incollection{guoDeepCoreComprehensiveLibrary2022,
  title = {{{DeepCore}}: {{A Comprehensive Library}} for {{Coreset Selection}} in {{Deep Learning}}},
  shorttitle = {{{DeepCore}}},
  booktitle = {Database and {{Expert Systems Applications}}},
  author = {Guo, Chengcheng and Zhao, Bo and Bai, Yanbing},
  editor = {Strauss, Christine and Cuzzocrea, Alfredo and Kotsis, Gabriele and Tjoa, A Min and Khalil, Ismail},
  year = {2022},
  volume = {13426},
  pages = {181--195},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-12423-5_14},
  urldate = {2024-11-18},
  isbn = {978-3-031-12422-8 978-3-031-12423-5},
  langid = {english},
  file = {/Users/matt/Zotero/storage/FJ9GGYZZ/Guo et al. - 2022 - DeepCore A Comprehensive Library for Coreset Sele.pdf}
}

@article{hebartTHINGSdataMultimodalCollection2023,
  title = {{{THINGS-data}}, a Multimodal Collection of Large-Scale Datasets for Investigating Object Representations in Human Brain and Behavior},
  author = {Hebart, Martin N and Contier, Oliver and Teichmann, Lina and Rockter, Adam H and Zheng, Charles Y and Kidder, Alexis and Corriveau, Anna and {Vaziri-Pashkam}, Maryam and Baker, Chris I},
  year = {2023},
  month = feb,
  journal = {eLife},
  volume = {12},
  pages = {e82580},
  issn = {2050-084X},
  doi = {10.7554/eLife.82580},
  urldate = {2024-10-17},
  abstract = {Understanding object representations requires a broad, comprehensive sampling of the objects in our visual world with dense measurements of brain activity and behavior. Here, we present THINGS-\-data, a multimodal collection of large-\-scale neuroimaging and behavioral datasets in humans, comprising densely sampled functional MRI and magnetoencephalographic recordings, as well as 4.70 million similarity judgments in response to thousands of photographic images for up to 1,854 object concepts. THINGS-d\- ata is unique in its breadth of richly annotated objects, allowing for testing countless hypotheses at scale while assessing the reproducibility of previous findings. Beyond the unique insights promised by each individual dataset, the multimodality of THINGS-\-data allows combining datasets for a much broader view into object processing than previously possible. Our analyses demonstrate the high quality of the datasets and provide five examples of hypothesis-\- driven and data-\-driven applications. THINGS-d\- ata constitutes the core public release of the THINGS initiative (https://things-initiative.org) for bridging the gap between disciplines and the advancement of cognitive neuroscience.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/4GDH4Y6B/Hebart et al. - 2023 - THINGS-data, a multimodal collection of large-scal.pdf}
}

@article{hoerlRidgeRegressionBiased1970,
  title = {Ridge {{Regression}}: {{Biased Estimation}} for {{Nonorthogonal Problems}}},
  shorttitle = {Ridge {{Regression}}},
  author = {Hoerl, Arthur E. and Kennard, Robert W.},
  year = {1970},
  month = feb,
  journal = {Technometrics},
  volume = {12},
  number = {1},
  pages = {55--67},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.1970.10488634},
  urldate = {2025-01-22},
  langid = {english}
}

@article{horikawaAttentionModulatesNeural2022,
  title = {Attention Modulates Neural Representation to Render Reconstructions According to Subjective Appearance},
  author = {Horikawa, Tomoyasu and Kamitani, Yukiyasu},
  year = {2022},
  month = jan,
  journal = {Communications Biology},
  volume = {5},
  number = {1},
  pages = {34},
  issn = {2399-3642},
  doi = {10.1038/s42003-021-02975-5},
  urldate = {2025-03-12},
  abstract = {Abstract             Stimulus images can be reconstructed from visual cortical activity. However, our perception of stimuli is shaped by both stimulus-induced and top-down processes, and it is unclear whether and how reconstructions reflect top-down aspects of perception. Here, we investigate the effect of attention on reconstructions using fMRI activity measured while subjects attend to one of two superimposed images. A state-of-the-art method is used for image reconstruction, in which brain activity is translated (decoded) to deep neural network (DNN) features of hierarchical layers then to an image. Reconstructions resemble the attended rather than unattended images. They can be modeled by superimposed images with biased contrasts, comparable to the appearance during attention. Attentional modulations are found in a broad range of hierarchical visual representations and mirror the brain--DNN correspondence. Our results demonstrate that top-down attention counters stimulus-induced responses, modulating neural representations to render reconstructions in accordance with subjective appearance.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/NBW3P7LC/Horikawa and Kamitani - 2022 - Attention modulates neural representation to rende.pdf}
}

@article{horikawaGenericDecodingSeen2017,
  title = {Generic Decoding of Seen and Imagined Objects Using Hierarchical Visual Features},
  author = {Horikawa, Tomoyasu and Kamitani, Yukiyasu},
  year = {2017},
  month = may,
  journal = {Nature Communications},
  volume = {8},
  number = {1},
  pages = {15037},
  issn = {2041-1723},
  doi = {10.1038/ncomms15037},
  urldate = {2024-05-15},
  abstract = {Abstract             Object recognition is a key function in both human and machine vision. While brain decoding of seen and imagined objects has been achieved, the prediction is limited to training examples. We present a decoding approach for arbitrary objects using the machine vision principle that an object category is represented by a set of features rendered invariant through hierarchical processing. We show that visual features, including those derived from a deep convolutional neural network, can be predicted from fMRI patterns, and that greater accuracy is achieved for low-/high-level features with lower-/higher-level visual areas, respectively. Predicted features are used to identify seen/imagined object categories (extending beyond decoder training) from a set of computed features for numerous object images. Furthermore, decoding of imagined objects reveals progressive recruitment of higher-to-lower visual representations. Our results demonstrate a homology between human and machine vision and its utility for brain-based information retrieval.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/GJ6CW97F/Horikawa and Kamitani - 2017 - Generic decoding of seen and imagined objects usin.pdf}
}

@article{horikawaNeuralDecodingVisual2013,
  title = {Neural {{Decoding}} of {{Visual Imagery During Sleep}}},
  author = {Horikawa, T. and Tamaki, M. and Miyawaki, Y. and Kamitani, Y.},
  year = {2013},
  month = may,
  journal = {Science},
  volume = {340},
  number = {6132},
  pages = {639--642},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1234330},
  urldate = {2025-03-12},
  abstract = {Reading Dreams                            How specific visual dream contents are represented by brain activity is unclear. Machine-learning--based analyses can decode the stimulus- and task-induced brain activity patterns that represent specific visual contents.                                Horikawa                 et al.                              (p.               639               , published online 4 April) examined patterns of brain activity during dreaming and compared these to waking responses to visual stimuli. The findings suggest that the visual content of dreams is represented by the same neural substrate as observed during awake perception.                        ,              Machine-learning models can predict specific visual dream contents from brain activity measurement alone.           ,              Visual imagery during sleep has long been a topic of persistent speculation, but its private nature has hampered objective analysis. Here we present a neural decoding approach in which machine-learning models predict the contents of visual imagery during the sleep-onset period, given measured brain activity, by discovering links between human functional magnetic resonance imaging patterns and verbal reports with the assistance of lexical and image databases. Decoding models trained on stimulus-induced brain activity in visual cortical areas showed accurate classification, detection, and identification of contents. Our findings demonstrate that specific visual experience during sleep is represented by brain activity patterns shared by stimulus perception, providing a means to uncover subjective contents of dreaming using objective neural measurement.},
  langid = {english}
}

@inproceedings{Hu_2023_ICCV,
  title = {{{PromptCap}}: {{Prompt-guided}} Image Captioning for {{VQA}} with {{GPT-3}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author = {Hu, Yushi and Hua, Hang and Yang, Zhengyuan and Shi, Weijia and Smith, Noah A. and Luo, Jiebo},
  year = {2023},
  month = oct,
  pages = {2963--2975},
  file = {/Users/matt/Zotero/storage/GUDTHULM/Hu et al. - 2023 - PromptCap Prompt-guided image captioning for VQA .pdf}
}

@article{JMLR:v6:brown05a,
  title = {Managing Diversity in Regression Ensembles},
  author = {Brown, Gavin and Wyatt, Jeremy L. and Ti\&\#328;o, Peter},
  year = {2005},
  journal = {Journal of Machine Learning Research},
  volume = {6},
  number = {55},
  pages = {1621--1650},
  file = {/Users/matt/Zotero/storage/9VML9TF6/Brown et al. - 2005 - Managing diversity in regression ensembles.pdf}
}

@article{kamitaniDecodingVisualSubjective2005,
  title = {Decoding the Visual and Subjective Contents of the Human Brain},
  author = {Kamitani, Yukiyasu and Tong, Frank},
  year = {2005},
  month = may,
  journal = {Nature Neuroscience},
  volume = {8},
  number = {5},
  pages = {679--685},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1444},
  urldate = {2024-05-16},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/matt/Zotero/storage/LDCSPWGF/Kamitani and Tong - 2005 - Decoding the visual and subjective contents of the.pdf}
}

@misc{KamitaniLabBdpy2024,
  title = {{{KamitaniLab}}/Bdpy},
  year = {2024},
  month = dec,
  urldate = {2025-01-23},
  abstract = {Python package for brain decoding analysis (BrainDecoderToolbox2 data format, machine learning analysis, functional MRI)},
  copyright = {MIT},
  howpublished = {Kamitani Lab}
}

@article{kamitaniSpatialSmoothingHurts2010,
  title = {Spatial Smoothing Hurts Localization but Not Information: {{Pitfalls}} for Brain Mappers},
  shorttitle = {Spatial Smoothing Hurts Localization but Not Information},
  author = {Kamitani, Yukiyasu and Sawahata, Yasuhito},
  year = {2010},
  month = feb,
  journal = {NeuroImage},
  volume = {49},
  number = {3},
  pages = {1949--1952},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2009.06.040},
  urldate = {2025-03-12},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/matt/Zotero/storage/VXKF9HN8/Kamitani and Sawahata - 2010 - Spatial smoothing hurts localization but not infor.pdf}
}

@article{kanwisherFusiformFaceArea1997,
  title = {The {{Fusiform Face Area}}: {{A Module}} in {{Human Extrastriate Cortex Specialized}} for {{Face Perception}}},
  shorttitle = {The {{Fusiform Face Area}}},
  author = {Kanwisher, Nancy and McDermott, Josh and Chun, Marvin M.},
  year = {1997},
  month = jun,
  journal = {The Journal of Neuroscience},
  volume = {17},
  number = {11},
  pages = {4302--4311},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.17-11-04302.1997},
  urldate = {2025-01-25},
  abstract = {Using functional magnetic resonance imaging (fMRI), we found an area in the fusiform gyrus in 12 of the 15 subjects tested that was significantly more active when the subjects viewed faces than when they viewed assorted common objects. This face activation was used to define a specific region of interest individually for each subject, within which several new tests of face specificity were run. In each of five subjects tested, the predefined candidate ``face area'' also responded significantly more strongly to passive viewing of (1) intact than scrambled two-tone faces, (2) full front-view face photos than front-view photos of houses, and (in a different set of five subjects) (3) three-quarter-view face photos (with hair concealed) than photos of human hands; it also responded more strongly during (4) a consecutive matching task performed on three-quarter-view faces versus hands. Our technique of running multiple tests applied to the same region defined functionally within individual subjects provides a solution to two common problems in functional imaging: (1) the requirement to correct for multiple statistical comparisons and (2) the inevitable ambiguity in the interpretation of any study in which only two or three conditions are compared. Our data allow us to reject alternative accounts of the function of the fusiform face area (area ``FF'') that appeal to visual attention, subordinate-level classification, or general processing of any animate or human forms, demonstrating that this region is               selectively               involved in the perception of faces.},
  copyright = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
  langid = {english},
  file = {/Users/matt/Zotero/storage/BSFA7PSC/Kanwisher et al. - 1997 - The Fusiform Face Area A Module in Human Extrastr.pdf}
}

@article{kapoorREFORMSConsensusbasedRecommendations2024,
  title = {{{REFORMS}}: {{Consensus-based Recommendations}} for {{Machine-learning-based Science}}},
  shorttitle = {{{REFORMS}}},
  author = {Kapoor, Sayash and Cantrell, Emily M. and Peng, Kenny and Pham, Thanh Hien and Bail, Christopher A. and Gundersen, Odd Erik and Hofman, Jake M. and Hullman, Jessica and Lones, Michael A. and Malik, Momin M. and Nanayakkara, Priyanka and Poldrack, Russell A. and Raji, Inioluwa Deborah and Roberts, Michael and Salganik, Matthew J. and {Serra-Garcia}, Marta and Stewart, Brandon M. and Vandewiele, Gilles and Narayanan, Arvind},
  year = {2024},
  month = may,
  journal = {Science Advances},
  volume = {10},
  number = {18},
  pages = {eadk3452},
  issn = {2375-2548},
  doi = {10.1126/sciadv.adk3452},
  urldate = {2024-06-07},
  abstract = {Machine learning (ML) methods are proliferating in scientific research. However, the adoption of these methods has been accompanied by failures of validity, reproducibility, and generalizability. These failures can hinder scientific progress, lead to false consensus around invalid claims, and undermine the credibility of ML-based science. ML methods are often applied and fail in similar ways across disciplines. Motivated by this observation, our goal is to provide clear recommendations for conducting and reporting ML-based science. Drawing from an extensive review of past literature, we present the REFORMS checklist (recommendations for machine-learning-based science). It consists of 32 questions and a paired set of guidelines. REFORMS was developed on the basis of a consensus of 19 researchers across computer science, data science, mathematics, social sciences, and biomedical sciences. REFORMS can serve as a resource for researchers when designing and implementing a study, for referees when reviewing papers, and for journals when enforcing standards for transparency and reproducibility.           ,              We provide a checklist to improve reporting practices in ML-based science based on a review of best practices and common errors.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/LDE43MAR/Kapoor et al. - 2024 - REFORMS Consensus-based Recommendations for Machi.pdf}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.6980},
  urldate = {2025-03-03},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/HNZPD73H/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/Users/matt/Zotero/storage/LIZCHNG3/1412.html}
}

@article{kolluruLearningFewerImages2021,
  title = {Learning {{With Fewer Images}} via {{Image Clustering}}: {{Application}} to {{Intravascular OCT Image Segmentation}}},
  shorttitle = {Learning {{With Fewer Images}} via {{Image Clustering}}},
  author = {Kolluru, Chaitanya and Lee, Juhwan and Gharaibeh, Yazan and Bezerra, Hiram G. and Wilson, David L.},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {37273--37280},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3058890},
  urldate = {2024-11-15},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  file = {/Users/matt/Zotero/storage/S4JZIZHQ/Kolluru et al. - 2021 - Learning With Fewer Images via Image Clustering A.pdf}
}

@inproceedings{koppen2000curse,
  title = {The Curse of Dimensionality},
  booktitle = {5th Online World Conference on Soft Computing in Industrial Applications ({{WSC5}})},
  author = {K{\"o}ppen, Mario},
  year = {2000},
  volume = {1},
  pages = {4--8}
}

@article{kourtziCorticalRegionsInvolved2000,
  title = {Cortical {{Regions Involved}} in {{Perceiving Object Shape}}},
  author = {Kourtzi, Zoe and Kanwisher, Nancy},
  year = {2000},
  month = may,
  journal = {The Journal of Neuroscience},
  volume = {20},
  number = {9},
  pages = {3310--3318},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.20-09-03310.2000},
  urldate = {2025-01-25},
  abstract = {The studies described here use functional magnetic resonance imaging to test whether common or distinct cognitive and/or neural mechanisms are involved in extracting object structure from the different image cues defining an object's shape, such as contours, shading, and monocular depth cues. We found overlapping activations in the lateral and ventral occipital cortex [known as the lateral occipital complex (LOC)] for objects defined by different visual cues (e.g., grayscale photographs and line drawings) when each was compared with its own scrambled-object control. In a second experiment we found a reduced response when objects were repeated, independent of whether they appeared in the same or a different format (i.e., grayscale images vs line drawings). A third experiment showed that activation in the LOC was no stronger for three-dimensional shapes defined by contours or monocular depth cues, such as occlusion, than for two-dimensional shapes, suggesting that these regions are not selectively involved in processing three-dimensional shape information. These results suggest that common regions in the LOC are involved in extracting and/or representing information about object structure from different image cues.},
  copyright = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
  langid = {english},
  file = {/Users/matt/Zotero/storage/FA4TIHVA/Kourtzi and Kanwisher - 2000 - Cortical Regions Involved in Perceiving Object Sha.pdf}
}

@article{kravitzNewNeuralFramework2011,
  title = {A New Neural Framework for Visuospatial Processing},
  author = {Kravitz, Dwight J. and Saleem, Kadharbatcha S. and Baker, Chris I. and Mishkin, Mortimer},
  year = {2011},
  month = apr,
  journal = {Nature Reviews Neuroscience},
  volume = {12},
  number = {4},
  pages = {217--230},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3008},
  urldate = {2025-04-02},
  langid = {english},
  file = {/Users/matt/Zotero/storage/73UV8D34/Kravitz et al. - 2011 - A new neural framework for visuospatial processing.pdf}
}

@article{Kuncheva2003Measures,
  title = {Measures of Diversity in Classifier Ensembles and Their Relationship with the Ensemble Accuracy},
  author = {Kuncheva, L. and Whitaker, C. J.},
  year = {2003},
  journal = {Machine Learning},
  volume = {51},
  pages = {181--207},
  doi = {10.1023/A:1022859003006},
  file = {/Users/matt/Zotero/storage/J48LNPR4/Kuncheva and Whitaker - 2003 - Measures of diversity in classifier ensembles and .pdf}
}

@misc{kurakinAdversarialMachineLearning2017,
  title = {Adversarial {{Machine Learning}} at {{Scale}}},
  author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  year = {2017},
  month = feb,
  number = {arXiv:1611.01236},
  eprint = {1611.01236},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1611.01236},
  urldate = {2025-02-28},
  abstract = {Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a "label leaking" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/matt/Zotero/storage/RXH968FT/Kurakin et al. - 2017 - Adversarial Machine Learning at Scale.pdf;/Users/matt/Zotero/storage/MYQUU5C4/1611.html}
}

@article{liDivergenceAgnosticUnsupervisedDomain2022,
  title = {Divergence-{{Agnostic Unsupervised Domain Adaptation}} by {{Adversarial Attacks}}},
  author = {Li, Jingjing and Du, Zhekai and Zhu, Lei and Ding, Zhengming and Lu, Ke and Shen, Heng Tao},
  year = {2022},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {11},
  pages = {8196--8211},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3109287},
  urldate = {2025-02-28},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@misc{lipplWhenDoesCompositional2024,
  title = {When Does Compositional Structure Yield Compositional Generalization? {{A}} Kernel Theory},
  shorttitle = {When Does Compositional Structure Yield Compositional Generalization?},
  author = {Lippl, Samuel and Stachenfeld, Kim},
  year = {2024},
  month = may,
  number = {arXiv:2405.16391},
  eprint = {2405.16391},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-06-12},
  abstract = {Compositional generalization (the ability to respond correctly to novel combinations of familiar components) is thought to be a cornerstone of intelligent behavior. Compositionally structured (e.g. disentangled) representations are essential for this; however, the conditions under which they yield compositional generalization remain unclear. To address this gap, we present a general theory of compositional generalization in kernel models with fixed, potentially nonlinear representations (which also applies to neural networks in the ``lazy regime''). We prove that these models are functionally limited to adding up values assigned to conjunctions/combinations of components that have been seen during training (``conjunction-wise additivity''), and identify novel compositionality failure modes that arise from the data and model structure, even for disentangled inputs. For models in the representation learning (or ``rich'') regime, we show that networks can generalize on an important non-additive task (associative inference), and give a mechanistic explanation for why. Finally, we validate our theory empirically, showing that it captures the behavior of deep neural networks trained on a set of compositional tasks. In sum, our theory characterizes the principles giving rise to compositional generalization in kernel models and shows how representation learning can overcome their limitations. We further provide a formally grounded, novel generalization class for compositional tasks that highlights fundamental differences in the required learning mechanisms (conjunction-wise additivity).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/Users/matt/Zotero/storage/WBLG3HP7/Lippl and Stachenfeld - 2024 - When does compositional structure yield compositio.pdf}
}

@article{liuUnsupervisedBlindImage2020,
  title = {Unsupervised {{Blind Image Quality Evaluation}} via {{Statistical Measurements}} of {{Structure}}, {{Naturalness}}, and {{Perception}}},
  author = {Liu, Yutao and Gu, Ke and Zhang, Yongbing and Li, Xiu and Zhai, Guangtao and Zhao, Debin and Gao, Wen},
  year = {2020},
  month = apr,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {30},
  number = {4},
  pages = {929--943},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2019.2900472},
  urldate = {2025-03-05},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {/Users/matt/Zotero/storage/XDEIJG8E/Liu et al. - 2020 - Unsupervised Blind Image Quality Evaluation via St.pdf}
}

@article{lloydLeastSquaresQuantization1982,
  title = {Least Squares Quantization in {{PCM}}},
  author = {Lloyd, S.},
  year = {1982},
  journal = {IEEE Transactions on Information Theory},
  volume = {28},
  number = {2},
  pages = {129--137},
  doi = {10.1109/TIT.1982.1056489}
}

@misc{madryDeepLearningModels2019,
  title = {Towards {{Deep Learning Models Resistant}} to {{Adversarial Attacks}}},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  year = {2019},
  month = sep,
  number = {arXiv:1706.06083},
  eprint = {1706.06083},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.06083},
  urldate = {2025-02-28},
  abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/matt/Zotero/storage/R3MUW6DL/Madry et al. - 2019 - Towards Deep Learning Models Resistant to Adversar.pdf;/Users/matt/Zotero/storage/G4B8FYFH/1706.html}
}

@inproceedings{maheshwariImageClusteringUsing2009,
  title = {Image {{Clustering Using Color}} and {{Texture}}},
  booktitle = {2009 {{First International Conference}} on {{Computational Intelligence}}, {{Communication Systems}} and {{Networks}}},
  author = {Maheshwari, Manish and Silakari, Sanjay and Motwani, Mahesh},
  year = {2009},
  month = jul,
  pages = {403--408},
  publisher = {IEEE},
  address = {Indore, India},
  doi = {10.1109/CICSYN.2009.69},
  urldate = {2024-11-26},
  isbn = {978-0-7695-3743-6},
  file = {/Users/matt/Zotero/storage/VRNIN92V/Maheshwari et al. - 2009 - Image Clustering Using Color and Texture.pdf}
}

@misc{maiUniBrainUnifyImage2023,
  title = {{{UniBrain}}: {{Unify Image Reconstruction}} and {{Captioning All}} in {{One Diffusion Model}} from {{Human Brain Activity}}},
  shorttitle = {{{UniBrain}}},
  author = {Mai, Weijian and Zhang, Zhijun},
  year = {2023},
  month = aug,
  number = {arXiv:2308.07428},
  eprint = {2308.07428},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.07428},
  urldate = {2025-03-04},
  abstract = {Image reconstruction and captioning from brain activity evoked by visual stimuli allow researchers to further understand the connection between the human brain and the visual perception system. While deep generative models have recently been employed in this field, reconstructing realistic captions and images with both low-level details and high semantic fidelity is still a challenging problem. In this work, we propose UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity. For the first time, we unify image reconstruction and captioning from visual-evoked functional magnetic resonance imaging (fMRI) through a latent diffusion model termed Versatile Diffusion. Specifically, we transform fMRI voxels into text and image latent for low-level information and guide the backward diffusion process through fMRI-based image and text conditions derived from CLIP to generate realistic captions and images. UniBrain outperforms current methods both qualitatively and quantitatively in terms of image reconstruction and reports image captioning results for the first time on the Natural Scenes Dataset (NSD) dataset. Moreover, the ablation experiments and functional region-of-interest (ROI) analysis further exhibit the superiority of UniBrain and provide comprehensive insight for visual-evoked brain decoding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/RZQUITSD/Mai and Zhang - 2023 - UniBrain Unify Image Reconstruction and Captionin.pdf;/Users/matt/Zotero/storage/2XXM844L/2308.html}
}

@misc{mcinnesUMAPUniformManifold2018,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1802.03426},
  urldate = {2025-02-10},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computational Geometry (cs.CG),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@misc{mildenbergerKamitaniLabBrain_diffuser,
  title = {{{KamitaniLab}}/Brain\_diffuser},
  author = {Mildenberger, Matthias},
  journal = {GitHub},
  urldate = {2025-01-22},
  howpublished = {https://github.com/KamitaniLab/brain\_diffuser},
  langid = {english},
  file = {/Users/matt/Zotero/storage/Y4NG7IGB/brain_diffuser.html}
}

@article{miyawakiVisualImageReconstruction2008,
  title = {Visual {{Image Reconstruction}} from {{Human Brain Activity}} Using a {{Combination}} of {{Multiscale Local Image Decoders}}},
  author = {Miyawaki, Yoichi and Uchida, Hajime and Yamashita, Okito and Sato, Masa-aki and Morito, Yusuke and Tanabe, Hiroki C. and Sadato, Norihiro and Kamitani, Yukiyasu},
  year = {2008},
  month = dec,
  journal = {Neuron},
  volume = {60},
  number = {5},
  pages = {915--929},
  issn = {08966273},
  doi = {10.1016/j.neuron.2008.11.004},
  urldate = {2024-05-16},
  abstract = {Perceptual experience consists of an enormous number of possible states. Previous fMRI studies have predicted a perceptual state by classifying brain activity into prespecified categories. Constraint-free visual image reconstruction is more challenging, as it is impractical to specify brain activity for all possible images. In this study, we reconstructed visual images by combining local image bases of multiple scales, whose contrasts were independently decoded from fMRI activity by automatically selecting relevant voxels and exploiting their correlated patterns. Binarycontrast, 10 3 10-patch images (2100 possible states) were accurately reconstructed without any image prior on a single trial or volume basis by measuring brain activity only for several hundred random images. Reconstruction was also used to identify the presented image among millions of candidates. The results suggest that our approach provides an effective means to read out complex perceptual states from brain activity while discovering information representation in multivoxel patterns.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/G28LT8YB/Miyawaki et al. - 2008 - Visual Image Reconstruction from Human Brain Activ.pdf}
}

@misc{ModelZoo,
  title = {Model {{Zoo}}},
  journal = {GitHub},
  urldate = {2025-01-23},
  abstract = {Caffe: a fast open framework for deep learning. Contribute to BVLC/caffe development by creating an account on GitHub.},
  howpublished = {https://github.com/BVLC/caffe/wiki/Model-Zoo},
  langid = {english},
  file = {/Users/matt/Zotero/storage/CC3AYAS5/Model-Zoo.html}
}

@inproceedings{moosavi-dezfooliUniversalAdversarialPerturbations2017,
  title = {Universal {{Adversarial Perturbations}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {{Moosavi-Dezfooli}, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
  year = {2017},
  month = jul,
  pages = {86--94},
  publisher = {IEEE},
  address = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.17},
  urldate = {2024-10-21},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/Users/matt/Zotero/storage/KGNP6RRK/Moosavi-Dezfooli et al. - 2017 - Universal Adversarial Perturbations.pdf}
}

@misc{naseerIntriguingPropertiesVision2021,
  title = {Intriguing {{Properties}} of {{Vision Transformers}}},
  author = {Naseer, Muzammal and Ranasinghe, Kanchana and Khan, Salman and Hayat, Munawar and Khan, Fahad Shahbaz and Yang, Ming-Hsuan},
  year = {2021},
  month = nov,
  number = {arXiv:2105.10497},
  eprint = {2105.10497},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.10497},
  urldate = {2025-02-28},
  abstract = {Vision transformers (ViT) have demonstrated impressive performance across various machine vision problems. These models are based on multi-head self-attention mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility in attending image-wide context conditioned on a given patch can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a) Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60\% top-1 accuracy on ImageNet even after randomly occluding 80\% of the image content. (b) The robust performance to occlusions is not due to a bias towards local textures, and ViTs are significantly less biased towards textures compared to CNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c) Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d) Off-the-shelf features from a single ViT model can be combined to create a feature ensemble, leading to high accuracy rates across a range of classification datasets in both traditional and few-shot learning paradigms. We show effective features of ViTs are due to flexible and dynamic receptive fields possible via the self-attention mechanism.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/M3H45YP7/Naseer et al. - 2021 - Intriguing Properties of Vision Transformers.pdf;/Users/matt/Zotero/storage/QXUFB87S/2105.html}
}

@article{naselarisEncodingDecodingFMRI2011,
  title = {Encoding and Decoding in {{fMRI}}},
  author = {Naselaris, Thomas and Kay, Kendrick N. and Nishimoto, Shinji and Gallant, Jack L.},
  year = {2011},
  month = may,
  journal = {NeuroImage},
  volume = {56},
  number = {2},
  pages = {400--410},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.07.073},
  urldate = {2025-03-12},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/matt/Zotero/storage/6PWULKC7/Naselaris et al. - 2011 - Encoding and decoding in fMRI.pdf;/Users/matt/Zotero/storage/J7RS4G9J/Naselaris et al. - 2011 - Encoding and decoding in fMRI.pdf}
}

@article{nguyenDatasetDistillationInfinitely2021,
  title = {Dataset Distillation with Infinitely Wide Convolutional Networks},
  author = {Nguyen, Timothy and Novak, Roman and Xiao, Lechao and Lee, Jaehoon},
  year = {2021},
  journal = {ArXiv},
  volume = {abs/2107.13034},
  file = {/Users/matt/Zotero/storage/7AVHWFPL/Nguyen et al. - 2021 - Dataset distillation with infinitely wide convolut.pdf}
}

@misc{OpenAI_ChatGPT_2024,
  title = {{{ChatGPT}} ({{GPT-4o}})},
  author = {{OpenAI}},
  year = {2024}
}

@misc{ozcelikNaturalSceneReconstruction2023,
  title = {Natural Scene Reconstruction from {{fMRI}} Signals Using Generative Latent Diffusion},
  author = {Ozcelik, Furkan and VanRullen, Rufin},
  year = {2023},
  month = jun,
  number = {arXiv:2303.05334},
  eprint = {2303.05334},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-06-12},
  abstract = {In neural decoding research, one of the most intriguing topics is the reconstruction of perceived natural images based on fMRI signals. Previous studies have succeeded in re-creating different aspects of the visuals, such as low-level properties (shape, texture, layout) or high-level features (category of objects, descriptive semantics of scenes) but have typically failed to reconstruct these properties together for complex scene images. Generative AI has recently made a leap forward with latent diffusion models capable of generating high-complexity images. Here, we investigate how to take advantage of this innovative technology for brain decoding. We present a two-stage scene reconstruction framework called ``Brain-Diffuser''. In the first stage, starting from fMRI signals, we reconstruct images that capture low-level properties and overall layout using a VDVAE (Very Deep Variational Autoencoder) model. In the second stage, we use the image-to-image framework of a latent diffusion model (Versatile Diffusion) conditioned on predicted multimodal (text and visual) features, to generate final reconstructed images. On the publicly available Natural Scenes Dataset benchmark, our method outperforms previous models both qualitatively and quantitatively. When applied to synthetic fMRI patterns generated from individual ROI (region-of-interest) masks, our trained model creates compelling ``ROI-optimal'' scenes consistent with neuroscientific knowledge. Thus, the proposed methodology can have an impact on both applied (e.g. brain-computer interface) and fundamental neuroscience.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Neurons and Cognition},
  file = {/Users/matt/Zotero/storage/CA7ZXH5T/Ozcelik and VanRullen - 2023 - Natural scene reconstruction from fMRI signals usi.pdf}
}

@misc{ozcelikOzcelikfuBraindiffuser2025,
  title = {Ozcelikfu/Brain-Diffuser},
  author = {Ozcelik, Furkan},
  year = {2025},
  month = jan,
  urldate = {2025-01-22},
  abstract = {Official repository for the paper "Brain-Diffuser: Natural scene reconstruction from fMRI signals using generative latent diffusion" by Furkan Ozcelik and Rufin VanRullen.},
  copyright = {MIT}
}

@misc{papernotPracticalBlackBoxAttacks2017,
  title = {Practical {{Black-Box Attacks}} against {{Machine Learning}}},
  author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
  year = {2017},
  month = mar,
  number = {arXiv:1602.02697},
  eprint = {1602.02697},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1602.02697},
  urldate = {2025-02-28},
  abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24\% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19\% and 88.94\%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/KXDQLMS8/Papernot et al. - 2017 - Practical Black-Box Attacks against Machine Learni.pdf;/Users/matt/Zotero/storage/QBV547UX/1602.html}
}

@inproceedings{pavlichenkoBestPromptsTexttoImage2023,
  title = {Best {{Prompts}} for {{Text-to-Image Models}} and {{How}} to {{Find Them}}},
  booktitle = {Proceedings of the 46th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Pavlichenko, Nikita and Ustalov, Dmitry},
  year = {2023},
  month = jul,
  pages = {2067--2071},
  publisher = {ACM},
  address = {Taipei Taiwan},
  doi = {10.1145/3539618.3592000},
  urldate = {2025-03-04},
  isbn = {978-1-4503-9408-6},
  langid = {english},
  file = {/Users/matt/Zotero/storage/R5LHHPT8/Pavlichenko and Ustalov - 2023 - Best Prompts for Text-to-Image Models and How to F.pdf}
}

@misc{popeIntrinsicDimensionImages2021,
  title = {The {{Intrinsic Dimension}} of {{Images}} and {{Its Impact}} on {{Learning}}},
  author = {Pope, Phillip and Zhu, Chen and Abdelkader, Ahmed and Goldblum, Micah and Goldstein, Tom},
  year = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2104.08894},
  urldate = {2025-04-03},
  abstract = {It is widely believed that natural image data exhibits low-dimensional structure despite the high dimensionality of conventional pixel representations. This idea underlies a common intuition for the remarkable success of deep learning in computer vision. In this work, we apply dimension estimation tools to popular datasets and investigate the role of low-dimensional structure in deep learning. We find that common natural image datasets indeed have very low intrinsic dimension relative to the high number of pixels in the images. Additionally, we find that low dimensional datasets are easier for neural networks to learn, and models solving these tasks generalize better from training to test data. Along the way, we develop a technique for validating our dimension estimation tools on synthetic data generated by GANs allowing us to actively manipulate the intrinsic dimension by controlling the image generation process. Code for our experiments may be found here https://github.com/ppope/dimensions.},
  copyright = {Creative Commons Attribution Share Alike 4.0 International},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,I.2.6; I.5.1,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/Users/matt/Zotero/storage/JBTJJ3P3/Pope et al. - 2021 - The Intrinsic Dimension of Images and Its Impact o.pdf}
}

@inproceedings{poursaeedGenerativeAdversarialPerturbations2018,
  title = {Generative {{Adversarial Perturbations}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Poursaeed, Omid and Katsman, Isay and Gao, Bicheng and Belongie, Serge},
  year = {2018},
  month = jun,
  pages = {4422--4431},
  publisher = {IEEE},
  address = {Salt Lake City, UT},
  doi = {10.1109/CVPR.2018.00465},
  urldate = {2024-10-21},
  abstract = {In this paper, we propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models. We present trainable deep neural networks for transforming images to adversarial perturbations. Our proposed models can produce image-agnostic and image-dependent perturbations for targeted and nontargeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling both classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. Using extensive experiments on challenging high-resolution datasets such as ImageNet and Cityscapes, we show that our perturbations achieve high fooling rates with small perturbation norms. Moreover, our attacks are considerably faster than current iterative methods at inference time.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {/Users/matt/Zotero/storage/W8EV6KUJ/Poursaeed et al. - 2018 - Generative Adversarial Perturbations.pdf}
}

@article{radfordLearningTransferableVisual,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  abstract = {SOTA computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study performance on over 30 different computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers nontrivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/BTPJMQXI/Radford et al. - Learning Transferable Visual Models From Natural L.pdf}
}

@misc{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.00020},
  urldate = {2025-01-22},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/UL9GKCHF/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf;/Users/matt/Zotero/storage/AFDS68XK/2103.html}
}

@article{rensinkVisualSensingSeeing2004,
  title = {Visual {{Sensing Without Seeing}}},
  author = {Rensink, Ronald A.},
  year = {2004},
  month = jan,
  journal = {Psychological Science},
  volume = {15},
  number = {1},
  pages = {27--32},
  issn = {0956-7976, 1467-9280},
  doi = {10.1111/j.0963-7214.2004.01501005.x},
  urldate = {2025-03-03},
  abstract = {It has often been assumed that when we use vision to become aware of an object or event in our surroundings, this must be accompanied by a corresponding visual experience (i.e., seeing). The studies reported here show that this assumption is incorrect. When observers view a sequence of displays alternating between an image of a scene and the same image changed in some way, they often feel (or sense) the change even though they have no visual experience of it. The subjective difference between sensing and seeing is mirrored in several behavioral differences, suggesting that these are two distinct modes of conscious visual perception.},
  copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
  langid = {english},
  file = {/Users/matt/Zotero/storage/MDZIFSSP/Rensink - 2004 - Visual Sensing Without Seeing.pdf}
}

@inproceedings{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bjorn},
  year = {2022},
  month = jun,
  pages = {10674--10685},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.01042},
  urldate = {2025-03-04},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-66546-946-3},
  file = {/Users/matt/Zotero/storage/IFS4GS5T/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf}
}

@inproceedings{sahariaPhotorealisticTexttoimageDiffusion2022,
  title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and Ho, Jonathan and Fleet, David J and Norouzi, Mohammad},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {36479--36494},
  publisher = {Curran Associates, Inc.},
  file = {/Users/matt/Zotero/storage/8G3ZNZQK/Saharia et al. - 2022 - Photorealistic text-to-image diffusion models with.pdf}
}

@article{samuelGeneratingImagesRare2024,
  title = {Generating {{Images}} of {{Rare Concepts Using Pre-trained Diffusion Models}}},
  author = {Samuel, Dvir and {Ben-Ari}, Rami and Raviv, Simon and Darshan, Nir and Chechik, Gal},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {5},
  pages = {4695--4703},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v38i5.28270},
  urldate = {2025-03-04},
  abstract = {Text-to-image diffusion models can synthesize high quality images, but they have various limitations. Here we highlight a common failure mode of these models, namely, generating uncommon concepts and structured concepts like hand palms. We show that their limitation is partly due to the long-tail nature of their training data: web-crawled data sets are strongly unbalanced, causing models to under-represent concepts from the tail of the distribution. We characterize the effect of unbalanced training data on text-to-image models and offer a remedy. We show that rare concepts can be correctly generated by carefully selecting suitable generation seeds in the noise space, using a small reference set of images, a technique that we call SeedSelect. SeedSelect does not require retraining or finetuning the diffusion model. We assess the faithfulness, quality and diversity of SeedSelect in creating rare objects and generating complex formations like hand images, and find it consistently achieves superior performance. We further show the advantage of SeedSelect in semantic data augmentation. Generating semantically appropriate images can successfully improve performance in few-shot recognition benchmarks, for classes from the head and from the tail of the training data of diffusion models.},
  file = {/Users/matt/Zotero/storage/VKAMPBPP/Samuel et al. - 2024 - Generating Images of Rare Concepts Using Pre-train.pdf}
}

@article{saraImageQualityAssessment2019,
  title = {Image {{Quality Assessment}} through {{FSIM}}, {{SSIM}}, {{MSE}} and {{PSNR}}---{{A Comparative Study}}},
  author = {Sara, Umme and Akter, Morium and Uddin, Mohammad Shorif},
  year = {2019},
  journal = {Journal of Computer and Communications},
  volume = {07},
  number = {03},
  pages = {8--18},
  issn = {2327-5219, 2327-5227},
  doi = {10.4236/jcc.2019.73002},
  urldate = {2024-10-25},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  file = {/Users/matt/Zotero/storage/INN6JNJ8/Sara et al. - 2019 - Image Quality Assessment through FSIM, SSIM, MSE a.pdf}
}

@misc{schuhmannLAION400MOpenDataset2021,
  title = {{{LAION-400M}}: {{Open Dataset}} of {{CLIP-Filtered}} 400 {{Million Image-Text Pairs}}},
  shorttitle = {{{LAION-400M}}},
  author = {Schuhmann, Christoph and Vencu, Richard and Beaumont, Romain and Kaczmarczyk, Robert and Mullis, Clayton and Katta, Aarush and Coombes, Theo and Jitsev, Jenia and Komatsuzaki, Aran},
  year = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2111.02114},
  urldate = {2025-01-22},
  abstract = {Multi-modal language-vision models trained on hundreds of millions of image-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable capability to perform zero- or few-shot learning and transfer even in absence of per-sample labels on target image data. Despite this trend, to date there has been no publicly available datasets of sufficient scale for training such models from scratch. To address this issue, in a community effort we build and release for public LAION-400M, a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices that allow efficient similarity search.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computation and Language (cs.CL),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@inproceedings{schuhmannLAION5BOpenLargescale2022,
  title = {{{LAION-5B}}: {{An}} Open Large-Scale Dataset for Training next Generation Image-Text Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {25278--25294},
  publisher = {Curran Associates, Inc.}
}

@misc{scottiMindEye2SharedSubjectModels2024,
  title = {{{MindEye2}}: {{Shared-Subject Models Enable fMRI-To-Image With}} 1 {{Hour}} of {{Data}}},
  shorttitle = {{{MindEye2}}},
  author = {Scotti, Paul S. and Tripathy, Mihir and Villanueva, Cesar Kadir Torrico and Kneeland, Reese and Chen, Tong and Narang, Ashutosh and Santhirasegaran, Charan and Xu, Jonathan and Naselaris, Thomas and Norman, Kenneth A. and Abraham, Tanishq Mathew},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2403.11207},
  urldate = {2025-03-15},
  abstract = {Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited. This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results. The present work showcases high-quality reconstructions using only 1 hour of fMRI training data. We pretrain our model across 7 subjects and then fine-tune on minimal data from a new subject. Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. We then map from CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP latents as inputs instead of text. This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruction metrics compared to single-subject approaches. MindEye2 demonstrates how accurate reconstructions of perception are possible from a single visit to the MRI facility. All code is available on GitHub.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Biological sciences,FOS: Computer and information sciences,Neurons and Cognition (q-bio.NC)},
  file = {/Users/matt/Zotero/storage/NA48UMFS/Scotti et al. - 2024 - MindEye2 Shared-Subject Models Enable fMRI-To-Ima.pdf}
}

@misc{scottiReconstructingMindsEye2023,
  title = {Reconstructing the {{Mind}}'s {{Eye}}: {{fMRI-to-Image}} with {{Contrastive Learning}} and {{Diffusion Priors}}},
  shorttitle = {Reconstructing the {{Mind}}'s {{Eye}}},
  author = {Scotti, Paul S. and Banerjee, Atmadeep and Goode, Jimmie and Shabalin, Stepan and Nguyen, Alex and Cohen, Ethan and Dempster, Aidan J. and Verlinde, Nathalie and Yundler, Elad and Weisberg, David and Norman, Kenneth A. and Abraham, Tanishq Mathew},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2305.18274},
  urldate = {2025-03-15},
  abstract = {We present MindEye, a novel fMRI-to-image approach to retrieve and reconstruct viewed images from brain activity. Our model comprises two parallel submodules that are specialized for retrieval (using contrastive learning) and reconstruction (using a diffusion prior). MindEye can map fMRI brain activity to any high dimensional multimodal latent space, like CLIP image space, enabling image reconstruction using generative models that accept embeddings from this latent space. We comprehensively compare our approach with other existing methods, using both qualitative side-by-side comparisons and quantitative evaluations, and show that MindEye achieves state-of-the-art performance in both reconstruction and retrieval tasks. In particular, MindEye can retrieve the exact original image even among highly similar candidates indicating that its brain embeddings retain fine-grained image-specific information. This allows us to accurately retrieve images even from large-scale databases like LAION-5B. We demonstrate through ablations that MindEye's performance improvements over previous methods result from specialized submodules for retrieval and reconstruction, improved training techniques, and training models with orders of magnitude more parameters. Furthermore, we show that MindEye can better preserve low-level image features in the reconstructions by using img2img, with outputs from a separate autoencoder. All code is available on GitHub.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Biological sciences,FOS: Computer and information sciences,Neurons and Cognition (q-bio.NC)}
}

@misc{senerActiveLearningConvolutional2018,
  title = {Active {{Learning}} for {{Convolutional Neural Networks}}: {{A Core-Set Approach}}},
  shorttitle = {Active {{Learning}} for {{Convolutional Neural Networks}}},
  author = {Sener, Ozan and Savarese, Silvio},
  year = {2018},
  month = jun,
  number = {arXiv:1708.00489},
  eprint = {1708.00489},
  primaryclass = {stat},
  publisher = {arXiv},
  urldate = {2024-11-15},
  abstract = {Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (ie. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, ie. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/matt/Zotero/storage/W6WLE8HI/Sener and Savarese - 2018 - Active Learning for Convolutional Neural Networks.pdf;/Users/matt/Zotero/storage/Q5LRKUGC/1708.html}
}

@article{serenoBordersMultipleVisual1995,
  title = {Borders of {{Multiple Visual Areas}} in {{Humans Revealed}} by {{Functional Magnetic Resonance Imaging}}},
  author = {Sereno, M. I. and Dale, A. M. and Reppas, J. B. and Kwong, K. K. and Belliveau, J. W. and Brady, T. J. and Rosen, B. R. and Tootell, R. B. H.},
  year = {1995},
  month = may,
  journal = {Science},
  volume = {268},
  number = {5212},
  pages = {889--893},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.7754376},
  urldate = {2025-01-25},
  abstract = {The borders of human visual areas V1, V2, VP, V3, and V4 were precisely and noninvasively determined. Functional magnetic resonance images were recorded during phase-encoded retinal stimulation. This volume data set was then sampled with a cortical surface reconstruction, making it possible to calculate the local visual field sign (mirror image versus non-mirror image representation). This method automatically and objectively outlines area borders because adjacent areas often have the opposite field sign. Cortical magnification factor curves for striate and extrastriate cortical areas were determined, which showed that human visual areas have a greater emphasis on the center-of-gaze than their counterparts in monkeys. Retinotopically organized visual areas in humans extend anteriorly to overlap several areas previously shown to be activated by written words.},
  langid = {english}
}

@article{shenDeepImageReconstruction2019,
  title = {Deep Image Reconstruction from Human Brain Activity},
  author = {Shen, Guohua and Horikawa, Tomoyasu and Majima, Kei and Kamitani, Yukiyasu},
  editor = {O'Reilly, Jill},
  year = {2019},
  month = jan,
  journal = {PLOS Computational Biology},
  volume = {15},
  number = {1},
  pages = {e1006633},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006633},
  urldate = {2024-05-15},
  abstract = {The mental contents of perception and imagery are thought to be encoded in hierarchical representations in the brain, but previous attempts to visualize perceptual contents have failed to capitalize on multiple levels of the hierarchy, leaving it challenging to reconstruct internal imagery. Recent work showed that visual cortical activity measured by functional magnetic resonance imaging (fMRI) can be decoded (translated) into the hierarchical features of a pre-trained deep neural network (DNN) for the same input image, providing a way to make use of the information from hierarchical visual features. Here, we present a novel image reconstruction method, in which the pixel values of an image are optimized to make its DNN features similar to those decoded from human brain activity at multiple layers. We found that our method was able to reliably produce reconstructions that resembled the viewed natural images. A natural image prior introduced by a deep generator neural network effectively rendered semantically meaningful details to the reconstructions. Human judgment of the reconstructions supported the effectiveness of combining multiple DNN layers to enhance the visual quality of generated images. While our model was solely trained with natural images, it successfully generalized to artificial shapes, indicating that our model was not simply matching to exemplars. The same analysis applied to mental imagery demonstrated rudimentary reconstructions of the subjective content. Our results suggest that our method can effectively combine hierarchical neural representations to reconstruct perceptual and subjective images, providing a new window into the internal contents of the brain.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/GLQ76TPU/Shen et al. - 2019 - Deep image reconstruction from human brain activit.pdf}
}

@article{shirakawaCriticalAssessmentGenerative2023,
  title = {Critical Assessment of Generative {{AI}} Methods and Natural Image Datasets for Visual Image Reconstruction from Brain Activity},
  author = {Shirakawa, Ken and Majima, Kei and Kamitani, Yukiyasu and Tanaka, Misato and Aoki, Shuntaro},
  year = {2023},
  publisher = {OSF},
  doi = {10.17605/OSF.IO/NMFC5},
  urldate = {2024-06-12},
  abstract = {Ongoing progress reports detailing the evaluation and characterization of text-to-image diffusion methods and natural image datasets used in recent image reconstruction studies},
  collaborator = {{Center For Open Science}},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {brain decoding,Deep neural network,generative AI,neuroscience,reconstruction}
}

@misc{shirakawaSpuriousReconstructionBrain2024,
  title = {Spurious Reconstruction from Brain Activity},
  author = {Shirakawa, Ken and Nagano, Yoshihiro and Tanaka, Misato and Aoki, Shuntaro C. and Majima, Kei and Muraki, Yusuke and Kamitani, Yukiyasu},
  year = {2024},
  month = may,
  number = {arXiv:2405.10078},
  eprint = {2405.10078},
  primaryclass = {q-bio},
  publisher = {arXiv},
  urldate = {2024-06-05},
  abstract = {The rapid advances in brain decoding, particularly visual image reconstruction, have sparked discussions about the potential societal implications and ethical considerations surrounding neurotechnology. As these methods aim to recover perceived images from brain activity and achieve prediction over diverse images beyond training samples (zero-shot prediction), it is crucial to critically assess their capabilities and limitations to prevent misguided public expectations and inform future regulations. Our case study of recent text-guided reconstruction methods, which leverage a large-scale dataset (the Natural Scene Dataset, NSD) and text-to-image diffusion models, reveals significant limitations in their generalizability. We found a notable decrease in performance when applying these methods to a different dataset, which was designed to prevent category overlaps between training and test sets. UMAP visualization of the text features with NSD images showed a limited diversity of distinct semantic and visual clusters, with substantial overlap between training and test sets. Formal analysis and simulations demonstrated that clustered training samples can lead to ``output dimension collapse,'' restricting the output feature dimensions predictable from brain activity. Diversifying the training set to ensure a broader feature distribution improved generalizability beyond the trained clusters. However, text features alone are insufficient for a complete mapping to the visual space, even if perfectly predicted from brain activity. We argue that recent photo-like reconstructions may primarily be a blend of classification into trained categories and the generation of convincing yet inauthentic images through text-to-image diffusion (hallucination). To achieve genuine zero-shot prediction, diverse datasets and compositional representations spanning the image space are essential. As neurotechnology advances, engaging in interdisciplinary discussions involving neuroscientists, ethicists, policymakers, and the public is crucial to ensure responsible development and application of these techniques. These discussions should be grounded in a clear understanding of the current capabilities and limitations of the technology, as well as a careful consideration of the potential ethical and societal impacts.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {/Users/matt/Zotero/storage/8E9KKSBG/Shirakawa et al. - 2024 - Spurious reconstruction from brain activity.pdf}
}

@article{shortenSurveyImageData2019,
  title = {A Survey on {{Image Data Augmentation}} for {{Deep Learning}}},
  author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
  year = {2019},
  month = dec,
  journal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {60},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0197-0},
  urldate = {2025-04-01},
  langid = {english},
  file = {/Users/matt/Zotero/storage/N9XP5ZAA/Shorten and Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf}
}

@misc{simonyanVeryDeepConvolutional2014,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2014},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1409.1556},
  urldate = {2025-01-23},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{smithSpatialTemporalScales2008,
  title = {Spatial and {{Temporal Scales}} of {{Neuronal Correlation}} in {{Primary Visual Cortex}}},
  author = {Smith, Matthew A. and Kohn, Adam},
  year = {2008},
  month = nov,
  journal = {The Journal of Neuroscience},
  volume = {28},
  number = {48},
  pages = {12591--12603},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2929-08.2008},
  urldate = {2025-03-11},
  abstract = {The spiking activity of cortical neurons is correlated. For instance, trial-to-trial fluctuations in response strength are shared between neurons, and spikes often occur synchronously. Understanding the properties and mechanisms that generate these forms of correlation is critical for determining their role in cortical processing. We therefore investigated the spatial extent and functional specificity of correlated spontaneous and evoked activity. Because feedforward, recurrent, and feedback pathways have distinct extents and specificity, we reasoned that these measurements could elucidate the contribution of each type of input. We recorded single unit activity with microelectrode arrays which allowed us to measure correlation in many hundreds of pairings, across a large range of spatial scales. Our data show that correlated evoked activity is generated by two mechanisms that link neurons with similar orientation preferences on different spatial scales: one with high temporal precision and a limited spatial extent ({$\sim$}3 mm), and a second that gives rise to correlation on a slow time scale and extends as far as we were able to measure (10 mm). The former is consistent with common input provided by horizontal connections; the latter likely involves feedback from extrastriate cortex. Spontaneous activity was correlated over a similar spatial extent, but approximately twice as strongly as evoked activity. Visual stimuli thus caused a substantial decrease in correlation, particularly at response onset. These properties and the circuit mechanism they imply provide new constraints on the functional role that correlation may play in visual processing.},
  copyright = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
  langid = {english},
  file = {/Users/matt/Zotero/storage/LBARKMKF/Smith and Kohn - 2008 - Spatial and Temporal Scales of Neuronal Correlatio.pdf}
}

@misc{sundaramWhenDoesPerceptual2024,
  title = {When {{Does Perceptual Alignment Benefit Vision Representations}}?},
  author = {Sundaram, Shobhita and Fu, Stephanie and Muttenthaler, Lukas and Tamir, Netanel Y. and Chai, Lucy and Kornblith, Simon and Darrell, Trevor and Isola, Phillip},
  year = {2024},
  month = oct,
  number = {arXiv:2410.10817},
  eprint = {2410.10817},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.10817},
  urldate = {2025-01-26},
  abstract = {Humans judge perceptual similarity according to diverse visual attributes, including scene layout, subject location, and camera pose. Existing vision models understand a wide range of semantic abstractions but improperly weigh these attributes and thus make inferences misaligned with human perception. While vision representations have previously benefited from alignment in contexts like image generation, the utility of perceptually aligned representations in more general-purpose settings remains unclear. Here, we investigate how aligning vision model representations to human perceptual judgments impacts their usability across diverse computer vision tasks. We finetune state-of-the-art models on human similarity judgments for image triplets and evaluate them across standard vision benchmarks. We find that aligning models to perceptual judgments yields representations that improve upon the original backbones across many downstream tasks, including counting, segmentation, depth estimation, instance retrieval, and retrieval-augmented generation. In addition, we find that performance is widely preserved on other tasks, including specialized out-of-distribution domains such as in medical imaging and 3D environment frames. Our results suggest that injecting an inductive bias about human perceptual knowledge into vision models can contribute to better representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/MDRY67XY/Sundaram et al. - 2024 - When Does Perceptual Alignment Benefit Vision Repr.pdf;/Users/matt/Zotero/storage/MGJST7G6/2410.html}
}

@article{takagiHighResolutionImageReconstruction,
  title = {High-{{Resolution Image Reconstruction With Latent Diffusion Models From Human Brain Activity}}},
  author = {Takagi, Yu and Nishimoto, Shinji},
  langid = {english},
  file = {/Users/matt/Zotero/storage/GQQKPUKB/Takagi and Nishimoto - High-Resolution Image Reconstruction With Latent D.pdf}
}

@article{ungerleiderWhatWhereHuman1994,
  title = {'{{What}}' and 'where' in the Human Brain},
  author = {Ungerleider, L},
  year = {1994},
  journal = {Current Opinion in Neurobiology},
  volume = {4},
  number = {2},
  pages = {157--165},
  issn = {09594388},
  doi = {10.1016/0959-4388(94)90066-3},
  urldate = {2025-04-02},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@misc{volpiAdversarialFeatureAugmentation2018,
  title = {Adversarial {{Feature Augmentation}} for {{Unsupervised Domain Adaptation}}},
  author = {Volpi, Riccardo and Morerio, Pietro and Savarese, Silvio and Murino, Vittorio},
  year = {2018},
  month = may,
  number = {arXiv:1711.08561},
  eprint = {1711.08561},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.08561},
  urldate = {2025-02-28},
  abstract = {Recent works showed that Generative Adversarial Networks (GANs) can be successfully applied in unsupervised domain adaptation, where, given a labeled source dataset and an unlabeled target dataset, the goal is to train powerful classifiers for the target samples. In particular, it was shown that a GAN objective function can be used to learn target features indistinguishable from the source ones. In this work, we extend this framework by (i) forcing the learned feature extractor to be domain-invariant, and (ii) training it through data augmentation in the feature space, namely performing feature augmentation. While data augmentation in the image space is a well established technique in deep learning, feature augmentation has not yet received the same level of attention. We accomplish it by means of a feature generator trained by playing the GAN minimax game against source features. Results show that both enforcing domain-invariance and performing feature augmentation lead to superior or comparable performance to state-of-the-art results in several unsupervised domain adaptation benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/ZCU3CEGN/Volpi et al. - 2018 - Adversarial Feature Augmentation for Unsupervised .pdf;/Users/matt/Zotero/storage/8YPPE5S9/1711.html}
}

@inproceedings{wangDataDropoutOptimizing2018,
  title = {Data {{Dropout}}: {{Optimizing Training Data}} for {{Convolutional Neural Networks}}},
  shorttitle = {Data {{Dropout}}},
  booktitle = {2018 {{IEEE}} 30th {{International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}})},
  author = {Wang, Tianyang and Huan, Jun and Li, Bo},
  year = {2018},
  month = nov,
  pages = {39--46},
  publisher = {IEEE},
  address = {Volos},
  doi = {10.1109/ICTAI.2018.00017},
  urldate = {2024-11-15},
  isbn = {978-1-5386-7449-9},
  file = {/Users/matt/Zotero/storage/P4FI5G93/Wang et al. - 2018 - Data Dropout Optimizing Training Data for Convolu.pdf}
}

@misc{wangDatasetDistillation2018,
  title = {Dataset {{Distillation}}},
  author = {Wang, Tongzhou and Zhu, Jun-Yan and Torralba, Antonio and Efros, Alexei A.},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1811.10959},
  urldate = {2025-03-05},
  abstract = {Model distillation aims to distill the knowledge of a complex model into a simpler one. In this paper, we consider an alternative formulation called dataset distillation: we keep the model fixed and instead attempt to distill the knowledge from a large training dataset into a small one. The idea is to synthesize a small number of data points that do not need to come from the correct data distribution, but will, when given to the learning algorithm as training data, approximate the model trained on the original data. For example, we show that it is possible to compress 60,000 MNIST training images into just 10 synthetic distilled images (one per class) and achieve close to original performance with only a few gradient descent steps, given a fixed network initialization. We evaluate our method in various initialization settings and with different learning objectives. Experiments on multiple datasets show the advantage of our approach compared to alternative methods.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/Users/matt/Zotero/storage/8Q7VV242/Wang et al. - 2018 - Dataset Distillation.pdf}
}

@article{wangDiversityImageCaptioning2022,
  title = {On {{Diversity}} in {{Image Captioning}}: {{Metrics}} and {{Methods}}},
  shorttitle = {On {{Diversity}} in {{Image Captioning}}},
  author = {Wang, Qingzhong and Wan, Jia and Chan, Antoni B.},
  year = {2022},
  month = feb,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {2},
  pages = {1035--1049},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.3013834},
  urldate = {2025-04-01},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {/Users/matt/Zotero/storage/WLP27RJR/Wang et al. - 2022 - On Diversity in Image Captioning Metrics and Meth.pdf}
}

@article{wangLessBetterUnweighted2020,
  title = {Less {{Is Better}}: {{Unweighted Data Subsampling}} via {{Influence Function}}},
  shorttitle = {Less {{Is Better}}},
  author = {Wang, Zifeng and Zhu, Hong and Dong, Zhenhua and He, Xiuqiang and Huang, Shao-Lun},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {6340--6347},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i04.6103},
  urldate = {2024-11-15},
  abstract = {In the time of Big Data, training complex models on large-scale data sets is challenging, making it appealing to reduce data volume for saving computation resources by subsampling. Most previous works in subsampling are weighted methods designed to help the performance of subset-model approach the full-set-model, hence the weighted methods have no chance to acquire a subset-model that is better than the full-set-model. However, we question that how can we achieve better model with less data? In this work, we propose a novel Unweighted Influence Data Subsampling (UIDS) method, and prove that the subset-model acquired through our method can outperform the full-set-model. Besides, we show that overly confident on a given test set for sampling is common in Influence-based subsampling methods, which can eventually cause our subset-model's failure in out-of-sample test. To mitigate it, we develop a probabilistic sampling scheme to control the worst-case risk over all distributions close to the empirical distribution. The experiment results demonstrate our methods superiority over existed subsampling methods in diverse tasks, such as text classification, image classification, click-through prediction, etc.},
  copyright = {https://www.aaai.org},
  file = {/Users/matt/Zotero/storage/CVDA772J/Wang et al. - 2020 - Less Is Better Unweighted Data Subsampling via In.pdf}
}

@misc{wenHardPromptsMade2023,
  title = {Hard {{Prompts Made Easy}}: {{Gradient-Based Discrete Optimization}} for {{Prompt Tuning}} and {{Discovery}}},
  shorttitle = {Hard {{Prompts Made Easy}}},
  author = {Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2302.03668},
  urldate = {2025-03-04},
  abstract = {The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical "hard" prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also "soft" prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface. We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effective in tuning LMs for classification.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/Users/matt/Zotero/storage/VFPQSI5X/Wen et al. - 2023 - Hard Prompts Made Easy Gradient-Based Discrete Op.pdf}
}

@misc{witteveenInvestigatingPromptEngineering2022,
  title = {Investigating {{Prompt Engineering}} in {{Diffusion Models}}},
  author = {Witteveen, Sam and Andrews, Martin},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2211.15462},
  urldate = {2025-03-04},
  abstract = {With the spread of the use of Text2Img diffusion models such as DALL-E 2, Imagen, Mid Journey and Stable Diffusion, one challenge that artists face is selecting the right prompts to achieve the desired artistic output. We present techniques for measuring the effect that specific words and phrases in prompts have, and (in the Appendix) present guidance on the selection of prompts to produce desired effects.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{wolfeFiveFactorsThat2017,
  title = {Five Factors That Guide Attention in Visual Search},
  author = {Wolfe, Jeremy M. and Horowitz, Todd S.},
  year = {2017},
  month = mar,
  journal = {Nature Human Behaviour},
  volume = {1},
  number = {3},
  pages = {0058},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0058},
  urldate = {2025-03-03},
  langid = {english},
  file = {/Users/matt/Zotero/storage/KILYZH33/Wolfe and Horowitz - 2017 - Five factors that guide attention in visual search.pdf}
}

@incollection{wolfeVisualAttention2000,
  title = {Visual {{Attention}}},
  booktitle = {Seeing},
  author = {Wolfe, Jeremy M.},
  year = {2000},
  pages = {335--386},
  publisher = {Elsevier},
  doi = {10.1016/B978-012443760-9/50010-6},
  urldate = {2025-03-30},
  isbn = {978-0-12-443760-9},
  langid = {english},
  file = {/Users/matt/Zotero/storage/D4I8KNSN/Wolfe - 2000 - Visual Attention.pdf}
}

@misc{xieAdversarialExamplesImprove2020,
  title = {Adversarial {{Examples Improve Image Recognition}}},
  author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan and Le, Quoc V.},
  year = {2020},
  month = apr,
  number = {arXiv:1911.09665},
  eprint = {1911.09665},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.09665},
  urldate = {2025-02-28},
  abstract = {Adversarial examples are commonly viewed as a threat to ConvNets. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose AdvProp, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. We show that AdvProp improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying AdvProp to the latest EfficientNet-B7 [28] on ImageNet, we achieve significant improvements on ImageNet (+0.7\%), ImageNet-C (+6.5\%), ImageNet-A (+7.0\%), Stylized-ImageNet (+4.8\%). With an enhanced EfficientNet-B8, our method achieves the state-of-the-art 85.5\% ImageNet top-1 accuracy without extra data. This result even surpasses the best model in [20] which is trained with 3.5B Instagram images ({\textasciitilde}3000X more than ImageNet) and {\textasciitilde}9.4X more parameters. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/F5UPTHUH/Xie et al. - 2020 - Adversarial Examples Improve Image Recognition.pdf;/Users/matt/Zotero/storage/NISUKJY3/1911.html}
}

@inproceedings{xuPromptFreeDiffusionTaking2024,
  title = {Prompt-{{Free Diffusion}}: {{Taking}} ``{{Text}}'' {{Out}} of {{Text-to-Image Diffusion Models}}},
  shorttitle = {Prompt-{{Free Diffusion}}},
  booktitle = {2024 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xu, Xingqian and Guo, Jiayi and Wang, Zhangyang and Huang, Gao and Essa, Irfan and Shi, Humphrey},
  year = {2024},
  month = jun,
  pages = {8682--8692},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR52733.2024.00829},
  urldate = {2025-03-04},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350353006},
  file = {/Users/matt/Zotero/storage/T53P7NZW/Xu et al. - 2024 - Prompt-Free Diffusion Taking “Text” Out of Text-t.pdf}
}

@inproceedings{xuPromptFreeDiffusionTaking2024a,
  title = {Prompt-{{Free Diffusion}}: {{Taking}} ``{{Text}}'' {{Out}} of {{Text-to-Image Diffusion Models}}},
  shorttitle = {Prompt-{{Free Diffusion}}},
  booktitle = {2024 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xu, Xingqian and Guo, Jiayi and Wang, Zhangyang and Huang, Gao and Essa, Irfan and Shi, Humphrey},
  year = {2024},
  month = jun,
  pages = {8682--8692},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR52733.2024.00829},
  urldate = {2025-03-04},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350353006},
  file = {/Users/matt/Zotero/storage/K9JH5ELL/Xu et al. - 2024 - Prompt-Free Diffusion Taking “Text” Out of Text-t.pdf}
}

@article{xuRobustnessGeneralization2012,
  title = {Robustness and Generalization},
  author = {Xu, Huan and Mannor, Shie},
  year = {2012},
  month = mar,
  journal = {Machine Learning},
  volume = {86},
  number = {3},
  pages = {391--423},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-011-5268-1},
  urldate = {2025-04-01},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/matt/Zotero/storage/GSPWH2DQ/Xu and Mannor - 2012 - Robustness and generalization.pdf}
}

@misc{xuVersatileDiffusionText2024,
  title = {Versatile {{Diffusion}}: {{Text}}, {{Images}} and {{Variations All}} in {{One Diffusion Model}}},
  shorttitle = {Versatile {{Diffusion}}},
  author = {Xu, Xingqian and Wang, Zhangyang and Zhang, Eric and Wang, Kai and Shi, Humphrey},
  year = {2024},
  month = jan,
  number = {arXiv:2211.08332},
  eprint = {2211.08332},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.08332},
  urldate = {2025-03-03},
  abstract = {Recent advances in diffusion models have set an impressive milestone in many generation tasks, and trending works such as DALL-E2, Imagen, and Stable Diffusion have attracted great interest. Despite the rapid landscape changes, recent new approaches focus on extensions and performance rather than capacity, thus requiring separate models for separate tasks. In this work, we expand the existing single-flow diffusion pipeline into a multi-task multimodal network, dubbed Versatile Diffusion (VD), that handles multiple flows of text-to-image, image-to-text, and variations in one unified model. The pipeline design of VD instantiates a unified multi-flow diffusion framework, consisting of sharable and swappable layer modules that enable the crossmodal generality beyond images and text. Through extensive experiments, we demonstrate that VD successfully achieves the following: a) VD outperforms the baseline approaches and handles all its base tasks with competitive quality; b) VD enables novel extensions such as disentanglement of style and semantics, dual- and multi-context blending, etc.; c) The success of our multi-flow multimodal framework over images and text may inspire further diffusion-based universal AI research. Our code and models are open-sourced at https://github.com/SHI-Labs/Versatile-Diffusion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/8QHEYZ8P/Xu et al. - 2024 - Versatile Diffusion Text, Images and Variations A.pdf;/Users/matt/Zotero/storage/8A57VHKY/2211.html}
}

@article{yamashitaSparseEstimationAutomatically2008,
  title = {Sparse Estimation Automatically Selects Voxels Relevant for the Decoding of {{fMRI}} Activity Patterns},
  author = {Yamashita, Okito and Sato, Masa-aki and Yoshioka, Taku and Tong, Frank and Kamitani, Yukiyasu},
  year = {2008},
  month = oct,
  journal = {NeuroImage},
  volume = {42},
  number = {4},
  pages = {1414--1429},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2008.05.050},
  urldate = {2025-03-12},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/matt/Zotero/storage/TT5XIQNM/Yamashita et al. - 2008 - Sparse estimation automatically selects voxels rel.pdf;/Users/matt/Zotero/storage/UKR6I6R4/Yamashita et al. - 2008 - Sparse estimation automatically selects voxels rel.pdf}
}

@inproceedings{yanEnhancingClassificationPerformance2023,
  title = {Enhancing {{Classification Performance}} in {{Knee Magnetic Resonance Imaging Using Adversarial Data Augmentation}}},
  booktitle = {2023 {{IEEE}} 14th {{International Conference}} on {{Software Engineering}} and {{Service Science}} ({{ICSESS}})},
  author = {Yan, Zhongbo and Yang, Xu and Chong, Chak Fong and Wang, Yapeng},
  year = {2023},
  month = oct,
  pages = {19--24},
  publisher = {IEEE},
  address = {Beijing, China},
  doi = {10.1109/ICSESS58500.2023.10293076},
  urldate = {2025-02-28},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350336269 9798350336276},
  file = {/Users/matt/Zotero/storage/PF4R9HK2/Yan et al. - 2023 - Enhancing Classification Performance in Knee Magne.pdf}
}

@article{yaoAutomaticConstructionDiverse2017,
  title = {Towards Automatic Construction of Diverse, High-Quality Image Datasets},
  author = {Yao, Yazhou and Zhang, Jian and Shen, Fumin and Zhang, Dongxiang and Tang, Zhenmin and Shen, Heng Tao},
  year = {2017},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {32},
  pages = {1199--1211},
  file = {/Users/matt/Zotero/storage/BR5Y5RJK/Yao et al. - 2017 - Towards automatic construction of diverse, high-qu.pdf}
}

@inproceedings{yaoDomainRobustApproach2016,
  title = {A {{Domain Robust Approach For Image Dataset Construction}}},
  booktitle = {Proceedings of the 24th {{ACM}} International Conference on {{Multimedia}}},
  author = {Yao, Yazhou and Hua, Xian-sheng and Shen, Fumin and Zhang, Jian and Tang, Zhenmin},
  year = {2016},
  month = oct,
  pages = {212--216},
  publisher = {ACM},
  address = {Amsterdam The Netherlands},
  doi = {10.1145/2964284.2967213},
  urldate = {2025-04-01},
  isbn = {978-1-4503-3603-1},
  langid = {english}
}

@article{yuDatasetDistillationComprehensive2024,
  title = {Dataset {{Distillation}}: {{A Comprehensive Review}}},
  shorttitle = {Dataset {{Distillation}}},
  author = {Yu, Ruonan and Liu, Songhua and Wang, Xinchao},
  year = {2024},
  month = jan,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {46},
  number = {1},
  pages = {150--170},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2023.3323376},
  urldate = {2024-11-18},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {/Users/matt/Zotero/storage/JJ9K4VGG/Yu et al. - 2024 - Dataset Distillation A Comprehensive Review.pdf}
}

@misc{zhangLongCLIPUnlockingLongText2024,
  title = {Long-{{CLIP}}: {{Unlocking}} the {{Long-Text Capability}} of {{CLIP}}},
  shorttitle = {Long-{{CLIP}}},
  author = {Zhang, Beichen and Zhang, Pan and Dong, Xiaoyi and Zang, Yuhang and Wang, Jiaqi},
  year = {2024},
  month = jul,
  number = {arXiv:2403.15378},
  eprint = {2403.15378},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.15378},
  urldate = {2024-12-18},
  abstract = {Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for zero-shot classification, text-image retrieval, and text-image generation by aligning image and text modalities. Despite its widespread adoption, a significant limitation of CLIP lies in the inadequate length of text input. The length of the text token is restricted to 77, and an empirical study shows the actual effective length is even less than 20. This prevents CLIP from handling detailed descriptions, limiting its applications for image retrieval and text-to-image generation with extensive prerequisites. To this end, we propose Long-CLIP as a plug-and-play alternative to CLIP that supports long-text input, retains or even surpasses its zero-shot generalizability, and aligns the CLIP latent space, making it readily replace CLIP without any further adaptation in downstream frameworks. Nevertheless, achieving this goal is far from straightforward, as simplistic fine-tuning can result in a significant degradation of CLIP's performance. Moreover, substituting the text encoder with a language model supporting longer contexts necessitates pretraining with vast amounts of data, incurring significant expenses. Accordingly, Long-CLIP introduces an efficient fine-tuning solution on CLIP with two novel strategies designed to maintain the original capabilities, including (1) a knowledge-preserved stretching of positional embedding and (2) a primary component matching of CLIP features. With leveraging just one million extra long text-image pairs, Long-CLIP has shown the superiority to CLIP for about 20\% in long caption text-image retrieval and 6\% in traditional text-image retrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers enhanced capabilities for generating images from detailed text descriptions by replacing CLIP in a plug-and-play manner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/YQRP6DLR/Zhang et al. - 2024 - Long-CLIP Unlocking the Long-Text Capability of C.pdf;/Users/matt/Zotero/storage/WGF9YILB/2403.html}
}

@misc{zhaoNovelCrossPerturbationSingle2024,
  title = {A {{Novel Cross-Perturbation}} for {{Single Domain Generalization}}},
  author = {Zhao, Dongjia and Qi, Lei and Shi, Xiao and Shi, Yinghuan and Geng, Xin},
  year = {2024},
  month = jun,
  number = {arXiv:2308.00918},
  eprint = {2308.00918},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.00918},
  urldate = {2025-02-28},
  abstract = {Single domain generalization aims to enhance the ability of the model to generalize to unknown domains when trained on a single source domain. However, the limited diversity in the training data hampers the learning of domain-invariant features, resulting in compromised generalization performance. To address this, data perturbation (augmentation) has emerged as a crucial method to increase data diversity. Nevertheless, existing perturbation methods often focus on either image-level or feature-level perturbations independently, neglecting their synergistic effects. To overcome these limitations, we propose CPerb, a simple yet effective cross-perturbation method. Specifically, CPerb utilizes both horizontal and vertical operations. Horizontally, it applies image-level and feature-level perturbations to enhance the diversity of the training data, mitigating the issue of limited diversity in single-source domains. Vertically, it introduces multi-route perturbation to learn domain-invariant features from different perspectives of samples with the same semantic category, thereby enhancing the generalization capability of the model. Additionally, we propose MixPatch, a novel feature-level perturbation method that exploits local image style information to further diversify the training data. Extensive experiments on various benchmark datasets validate the effectiveness of our method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/624Y9U9M/Zhao et al. - 2024 - A Novel Cross-Perturbation for Single Domain Gener.pdf;/Users/matt/Zotero/storage/W82V2UBQ/2308.html}
}
