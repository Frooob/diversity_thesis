@article{1056489,
  title = {Least Squares Quantization in {{PCM}}},
  author = {Lloyd, S.},
  year = {1982},
  journal = {IEEE Transactions on Information Theory},
  volume = {28},
  number = {2},
  pages = {129--137},
  doi = {10.1109/TIT.1982.1056489}
}

@article{ahmedRecentReviewImage2015,
  title = {Recent Review on Image Clustering},
  author = {Ahmed, Nasir},
  year = {2015},
  month = nov,
  journal = {IET Image Processing},
  volume = {9},
  number = {11},
  pages = {1020--1032},
  issn = {1751-9667, 1751-9667},
  doi = {10.1049/iet-ipr.2014.0885},
  urldate = {2024-11-26},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {/Users/matt/Zotero/storage/UHHBU5PA/Ahmed - 2015 - Recent review on image clustering.pdf}
}

@article{allenMassive7TFMRI2022,
  title = {A Massive {{7T fMRI}} Dataset to Bridge Cognitive Neuroscience and Artificial Intelligence},
  author = {Allen, Emily J. and {St-Yves}, Ghislain and Wu, Yihan and Breedlove, Jesse L. and Prince, Jacob S. and Dowdle, Logan T. and Nau, Matthias and Caron, Brad and Pestilli, Franco and Charest, Ian and Hutchinson, J. Benjamin and Naselaris, Thomas and Kay, Kendrick},
  year = {2022},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {25},
  number = {1},
  pages = {116--126},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-021-00962-x},
  urldate = {2025-01-21},
  langid = {english},
  file = {/Users/matt/Zotero/storage/VRE8CBJ4/Allen et al. - 2022 - A massive 7T fMRI dataset to bridge cognitive neur.pdf}
}

@misc{birodkarSemanticRedundanciesImageClassification2019,
  title = {Semantic {{Redundancies}} in {{Image-Classification Datasets}}: {{The}} 10\% {{You Don}}'t {{Need}}},
  shorttitle = {Semantic {{Redundancies}} in {{Image-Classification Datasets}}},
  author = {Birodkar, Vighnesh and Mobahi, Hossein and Bengio, Samy},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1901.11409},
  urldate = {2024-11-18},
  abstract = {Large datasets have been crucial to the success of deep learning models in the recent years, which keep performing better as they are trained with more labelled data. While there have been sustained efforts to make these models more data-efficient, the potential benefit of understanding the data itself, is largely untapped. Specifically, focusing on object recognition tasks, we wonder if for common benchmark datasets we can do better than random subsets of the data and find a subset that can generalize on par with the full dataset when trained on. To our knowledge, this is the first result that can find notable redundancies in CIFAR-10 and ImageNet datasets (at least 10\%). Interestingly, we observe semantic correlations between required and redundant images. We hope that our findings can motivate further research into identifying additional redundancies and exploiting them for more efficient training or data-collection.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-31073-2},
  langid = {english},
  lccn = {Q327 .B52 2006},
  keywords = {Machine learning,Pattern perception},
  file = {/Users/matt/Zotero/storage/V6QBMMNE/Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@article{canatarOutofDistributionGeneralizationKernel,
  title = {Out-of-{{Distribution Generalization}} in {{Kernel Regression}}},
  author = {Canatar, Abdulkadir and Bordelon, Blake and Pehlevan, Cengiz},
  abstract = {In real word applications, the data generating process for training a machine learning model often differs from what the model encounters in the test stage. Understanding how and whether machine learning models generalize under such distributional shifts remains a theoretical challenge. Here, we study generalization in kernel regression when the training and test distributions are different using the replica method from statistical physics. We derive an analytical formula for the out-of-distribution generalization error applicable to any kernel and real datasets. We identify an overlap matrix that quantifies the mismatch between distributions for a given kernel as a key determinant of generalization performance under distribution shift. Using our analytical expressions we elucidate various generalization phenomena including possible improvement in generalization when there is a mismatch. We develop procedures for optimizing training and test distributions for a given data budget to find best and worst case generalizations under the shift. We present applications of our theory to real and synthetic datasets and for many kernels. We compare results of our theory applied to Neural Tangent Kernel with simulations of wide networks and show agreement. We analyze linear regression in further depth.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/QM83882D/Canatar et al. - Out-of-Distribution Generalization in Kernel Regre.pdf}
}

@article{canatarSpectralBiasTaskmodel2021,
  title = {Spectral Bias and Task-Model Alignment Explain Generalization in Kernel Regression and Infinitely Wide Neural Networks},
  author = {Canatar, Abdulkadir and Bordelon, Blake and Pehlevan, Cengiz},
  year = {2021},
  month = may,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {2914},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-23103-1},
  urldate = {2024-06-07},
  abstract = {Abstract             A theoretical understanding of generalization remains an open problem for many machine learning models, including deep networks where overparameterization leads to better performance, contradicting the conventional wisdom from classical statistics. Here, we investigate generalization error for kernel regression, which, besides being a popular machine learning method, also describes certain infinitely overparameterized neural networks. We use techniques from statistical mechanics to derive an analytical expression for generalization error applicable to any kernel and data distribution. We present applications of our theory to real and synthetic datasets, and for many kernels including those that arise from training deep networks in the infinite-width limit. We elucidate an inductive bias of kernel regression to explain data with simple functions, characterize whether a kernel is compatible with a learning task, and show that more data may impair generalization when noisy or not expressible by the kernel, leading to non-monotonic learning curves with possibly many peaks.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/3XF74247/Canatar et al. - 2021 - Spectral bias and task-model alignment explain gen.pdf}
}

@misc{childVeryDeepVAEs2020,
  title = {Very {{Deep VAEs Generalize Autoregressive Models}} and {{Can Outperform Them}} on {{Images}}},
  author = {Child, Rewon},
  year = {2020},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2011.10650},
  urldate = {2025-01-22},
  abstract = {We present a hierarchical VAE that, for the first time, generates samples quickly while outperforming the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@misc{chittaTrainingDataSubset2020,
  title = {Training {{Data Subset Search}} with {{Ensemble Active Learning}}},
  author = {Chitta, Kashyap and Alvarez, Jose M. and Haussmann, Elmar and Farabet, Clement},
  year = {2020},
  month = nov,
  number = {arXiv:1905.12737},
  eprint = {1905.12737},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-15},
  abstract = {Deep Neural Networks (DNNs) often rely on very large datasets for training. Given the large size of such datasets, it is conceivable that they contain certain samples that either do not contribute or negatively impact the DNN's performance. If there is a large number of such samples, subsampling the training dataset in a way that removes them could provide an effective solution to both improve performance and reduce training time. In this paper, we propose an approach called Active Dataset Subsampling (ADS), to identify favorable subsets within a dataset for training using ensemble based uncertainty estimation. When applied to three image classification benchmarks (CIFAR-10, CIFAR-100 and ImageNet) we find that there are low uncertainty subsets, which can be as large as 50\% of the full dataset, that negatively impact performance. These subsets are identified and removed with ADS. We demonstrate that datasets obtained using ADS with a lightweight ResNet18 ensemble remain effective when used to train deeper models like ResNet-101. Our results provide strong empirical evidence that using all the available data for training can hurt performance on large scale vision tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/matt/Zotero/storage/3U7UW5SJ/Chitta et al. - 2020 - Training Data Subset Search with Ensemble Active L.pdf}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
  year = {2009},
  month = jun,
  pages = {248--255},
  publisher = {IEEE},
  address = {Miami, FL},
  doi = {10.1109/CVPR.2009.5206848},
  urldate = {2025-01-21},
  isbn = {978-1-4244-3992-8}
}

@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.11929},
  urldate = {2025-02-28},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/5ZL2MQHJ/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf;/Users/matt/Zotero/storage/EU4B83VK/2010.html}
}

@misc{ds001506:1.3.1,
  title = {"{{Deep}} Image Reconstruction"},
  author = {Shen, Guohua and Horikawa, Tomoyasu and Majima, Kei and Kamitani, Yukiyasu},
  year = {2020},
  publisher = {OpenNeuro},
  doi = {10.18112/openneuro.ds001506.v1.3.1}
}

@inproceedings{durdovTrainingDatasetPruning2024,
  title = {Training {{Dataset Pruning Algorithm}} with {{Evaluation}} on {{Medical Datasets}}},
  booktitle = {2024 {{International Conference}} on {{Software}}, {{Telecommunications}} and {{Computer Networks}} ({{SoftCOM}})},
  author = {Durdov, Bo{\v z}o and Prvan, Marina and {\v C}oko, Duje and Musi{\'c}, Josip},
  year = {2024},
  month = sep,
  pages = {1--8},
  publisher = {IEEE},
  address = {Split, Croatia},
  doi = {10.23919/SoftCOM62040.2024.10721725},
  urldate = {2024-11-15},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-953-290-138-2}
}

@article{engelFMRIHumanVisual1994,
  title = {{{fMRI}} of Human Visual Cortex},
  author = {Engel, Stephen A. and Rumelhart, David E. and Wandell, Brian A. and Lee, Adrian T. and Glover, Gary H. and Chichilnisky, Eduardo-Jose and Shadlen, Michael N.},
  year = {1994},
  month = jun,
  journal = {Nature},
  volume = {369},
  number = {6481},
  pages = {525--525},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/369525a0},
  urldate = {2025-01-25},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/matt/Zotero/storage/IW3EQ74J/Engel et al. - 1994 - fMRI of human visual cortex.pdf}
}

@article{epsteinCorticalRepresentationLocal1998,
  title = {A Cortical Representation of the Local Visual Environment},
  author = {Epstein, Russell and Kanwisher, Nancy},
  year = {1998},
  month = apr,
  journal = {Nature},
  volume = {392},
  number = {6676},
  pages = {598--601},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/33402},
  urldate = {2025-01-25},
  copyright = {http://www.springer.com/tdm},
  langid = {english}
}

@misc{Fort2021CLIPadversarialstickers,
  title = {Pixels Still Beat Text: {{Attacking}} the {{OpenAI CLIP}} Model with Text Patches and Adversarial Pixel Perturbations},
  author = {Fort, Stanislav},
  year = {2021},
  month = mar
}

@misc{fuDreamSimLearningNew2023,
  title = {{{DreamSim}}: {{Learning New Dimensions}} of {{Human Visual Similarity}} Using {{Synthetic Data}}},
  shorttitle = {{{DreamSim}}},
  author = {Fu, Stephanie and Tamir, Netanel and Sundaram, Shobhita and Chai, Lucy and Zhang, Richard and Dekel, Tali and Isola, Phillip},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2306.09344},
  urldate = {2025-01-26},
  abstract = {Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic content while also being sensitive to color and layout. Notably, despite being trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval and reconstruction tasks. Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks.},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@misc{goodfellowExplainingHarnessingAdversarial2014,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  year = {2014},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1412.6572},
  urldate = {2025-02-21},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@incollection{guoDeepCoreComprehensiveLibrary2022,
  title = {{{DeepCore}}: {{A Comprehensive Library}} for {{Coreset Selection}} in {{Deep Learning}}},
  shorttitle = {{{DeepCore}}},
  booktitle = {Database and {{Expert Systems Applications}}},
  author = {Guo, Chengcheng and Zhao, Bo and Bai, Yanbing},
  editor = {Strauss, Christine and Cuzzocrea, Alfredo and Kotsis, Gabriele and Tjoa, A Min and Khalil, Ismail},
  year = {2022},
  volume = {13426},
  pages = {181--195},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-12423-5_14},
  urldate = {2024-11-18},
  isbn = {978-3-031-12422-8 978-3-031-12423-5},
  langid = {english},
  file = {/Users/matt/Zotero/storage/FJ9GGYZZ/Guo et al. - 2022 - DeepCore A Comprehensive Library for Coreset Sele.pdf}
}

@article{hebartTHINGSdataMultimodalCollection2023,
  title = {{{THINGS-data}}, a Multimodal Collection of Large-Scale Datasets for Investigating Object Representations in Human Brain and Behavior},
  author = {Hebart, Martin N and Contier, Oliver and Teichmann, Lina and Rockter, Adam H and Zheng, Charles Y and Kidder, Alexis and Corriveau, Anna and {Vaziri-Pashkam}, Maryam and Baker, Chris I},
  year = {2023},
  month = feb,
  journal = {eLife},
  volume = {12},
  pages = {e82580},
  issn = {2050-084X},
  doi = {10.7554/eLife.82580},
  urldate = {2024-10-17},
  abstract = {Understanding object representations requires a broad, comprehensive sampling of the objects in our visual world with dense measurements of brain activity and behavior. Here, we present THINGS-\-data, a multimodal collection of large-\-scale neuroimaging and behavioral datasets in humans, comprising densely sampled functional MRI and magnetoencephalographic recordings, as well as 4.70 million similarity judgments in response to thousands of photographic images for up to 1,854 object concepts. THINGS-d\- ata is unique in its breadth of richly annotated objects, allowing for testing countless hypotheses at scale while assessing the reproducibility of previous findings. Beyond the unique insights promised by each individual dataset, the multimodality of THINGS-\-data allows combining datasets for a much broader view into object processing than previously possible. Our analyses demonstrate the high quality of the datasets and provide five examples of hypothesis-\- driven and data-\-driven applications. THINGS-d\- ata constitutes the core public release of the THINGS initiative (https://things-initiative.org) for bridging the gap between disciplines and the advancement of cognitive neuroscience.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/4GDH4Y6B/Hebart et al. - 2023 - THINGS-data, a multimodal collection of large-scal.pdf}
}

@article{hoerlRidgeRegressionBiased1970,
  title = {Ridge {{Regression}}: {{Biased Estimation}} for {{Nonorthogonal Problems}}},
  shorttitle = {Ridge {{Regression}}},
  author = {Hoerl, Arthur E. and Kennard, Robert W.},
  year = {1970},
  month = feb,
  journal = {Technometrics},
  volume = {12},
  number = {1},
  pages = {55--67},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.1970.10488634},
  urldate = {2025-01-22},
  langid = {english}
}

@article{horikawaGenericDecodingSeen2017,
  title = {Generic Decoding of Seen and Imagined Objects Using Hierarchical Visual Features},
  author = {Horikawa, Tomoyasu and Kamitani, Yukiyasu},
  year = {2017},
  month = may,
  journal = {Nature Communications},
  volume = {8},
  number = {1},
  pages = {15037},
  issn = {2041-1723},
  doi = {10.1038/ncomms15037},
  urldate = {2024-05-15},
  abstract = {Abstract             Object recognition is a key function in both human and machine vision. While brain decoding of seen and imagined objects has been achieved, the prediction is limited to training examples. We present a decoding approach for arbitrary objects using the machine vision principle that an object category is represented by a set of features rendered invariant through hierarchical processing. We show that visual features, including those derived from a deep convolutional neural network, can be predicted from fMRI patterns, and that greater accuracy is achieved for low-/high-level features with lower-/higher-level visual areas, respectively. Predicted features are used to identify seen/imagined object categories (extending beyond decoder training) from a set of computed features for numerous object images. Furthermore, decoding of imagined objects reveals progressive recruitment of higher-to-lower visual representations. Our results demonstrate a homology between human and machine vision and its utility for brain-based information retrieval.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/GJ6CW97F/Horikawa and Kamitani - 2017 - Generic decoding of seen and imagined objects usin.pdf}
}

@article{kamitaniDecodingVisualSubjective2005,
  title = {Decoding the Visual and Subjective Contents of the Human Brain},
  author = {Kamitani, Yukiyasu and Tong, Frank},
  year = {2005},
  month = may,
  journal = {Nature Neuroscience},
  volume = {8},
  number = {5},
  pages = {679--685},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1444},
  urldate = {2024-05-16},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/matt/Zotero/storage/LDCSPWGF/Kamitani and Tong - 2005 - Decoding the visual and subjective contents of the.pdf}
}

@misc{KamitaniLabBdpy2024,
  title = {{{KamitaniLab}}/Bdpy},
  year = {2024},
  month = dec,
  urldate = {2025-01-23},
  abstract = {Python package for brain decoding analysis (BrainDecoderToolbox2 data format, machine learning analysis, functional MRI)},
  copyright = {MIT},
  howpublished = {Kamitani Lab}
}

@article{kanwisherFusiformFaceArea1997,
  title = {The {{Fusiform Face Area}}: {{A Module}} in {{Human Extrastriate Cortex Specialized}} for {{Face Perception}}},
  shorttitle = {The {{Fusiform Face Area}}},
  author = {Kanwisher, Nancy and McDermott, Josh and Chun, Marvin M.},
  year = {1997},
  month = jun,
  journal = {The Journal of Neuroscience},
  volume = {17},
  number = {11},
  pages = {4302--4311},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.17-11-04302.1997},
  urldate = {2025-01-25},
  abstract = {Using functional magnetic resonance imaging (fMRI), we found an area in the fusiform gyrus in 12 of the 15 subjects tested that was significantly more active when the subjects viewed faces than when they viewed assorted common objects. This face activation was used to define a specific region of interest individually for each subject, within which several new tests of face specificity were run. In each of five subjects tested, the predefined candidate ``face area'' also responded significantly more strongly to passive viewing of (1) intact than scrambled two-tone faces, (2) full front-view face photos than front-view photos of houses, and (in a different set of five subjects) (3) three-quarter-view face photos (with hair concealed) than photos of human hands; it also responded more strongly during (4) a consecutive matching task performed on three-quarter-view faces versus hands. Our technique of running multiple tests applied to the same region defined functionally within individual subjects provides a solution to two common problems in functional imaging: (1) the requirement to correct for multiple statistical comparisons and (2) the inevitable ambiguity in the interpretation of any study in which only two or three conditions are compared. Our data allow us to reject alternative accounts of the function of the fusiform face area (area ``FF'') that appeal to visual attention, subordinate-level classification, or general processing of any animate or human forms, demonstrating that this region is               selectively               involved in the perception of faces.},
  copyright = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
  langid = {english},
  file = {/Users/matt/Zotero/storage/BSFA7PSC/Kanwisher et al. - 1997 - The Fusiform Face Area A Module in Human Extrastr.pdf}
}

@article{kapoorREFORMSConsensusbasedRecommendations2024,
  title = {{{REFORMS}}: {{Consensus-based Recommendations}} for {{Machine-learning-based Science}}},
  shorttitle = {{{REFORMS}}},
  author = {Kapoor, Sayash and Cantrell, Emily M. and Peng, Kenny and Pham, Thanh Hien and Bail, Christopher A. and Gundersen, Odd Erik and Hofman, Jake M. and Hullman, Jessica and Lones, Michael A. and Malik, Momin M. and Nanayakkara, Priyanka and Poldrack, Russell A. and Raji, Inioluwa Deborah and Roberts, Michael and Salganik, Matthew J. and {Serra-Garcia}, Marta and Stewart, Brandon M. and Vandewiele, Gilles and Narayanan, Arvind},
  year = {2024},
  month = may,
  journal = {Science Advances},
  volume = {10},
  number = {18},
  pages = {eadk3452},
  issn = {2375-2548},
  doi = {10.1126/sciadv.adk3452},
  urldate = {2024-06-07},
  abstract = {Machine learning (ML) methods are proliferating in scientific research. However, the adoption of these methods has been accompanied by failures of validity, reproducibility, and generalizability. These failures can hinder scientific progress, lead to false consensus around invalid claims, and undermine the credibility of ML-based science. ML methods are often applied and fail in similar ways across disciplines. Motivated by this observation, our goal is to provide clear recommendations for conducting and reporting ML-based science. Drawing from an extensive review of past literature, we present the REFORMS checklist (recommendations for machine-learning-based science). It consists of 32 questions and a paired set of guidelines. REFORMS was developed on the basis of a consensus of 19 researchers across computer science, data science, mathematics, social sciences, and biomedical sciences. REFORMS can serve as a resource for researchers when designing and implementing a study, for referees when reviewing papers, and for journals when enforcing standards for transparency and reproducibility.           ,              We provide a checklist to improve reporting practices in ML-based science based on a review of best practices and common errors.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/LDE43MAR/Kapoor et al. - 2024 - REFORMS Consensus-based Recommendations for Machi.pdf}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.6980},
  urldate = {2025-03-03},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/HNZPD73H/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/Users/matt/Zotero/storage/LIZCHNG3/1412.html}
}

@article{kolluruLearningFewerImages2021,
  title = {Learning {{With Fewer Images}} via {{Image Clustering}}: {{Application}} to {{Intravascular OCT Image Segmentation}}},
  shorttitle = {Learning {{With Fewer Images}} via {{Image Clustering}}},
  author = {Kolluru, Chaitanya and Lee, Juhwan and Gharaibeh, Yazan and Bezerra, Hiram G. and Wilson, David L.},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {37273--37280},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3058890},
  urldate = {2024-11-15},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  file = {/Users/matt/Zotero/storage/S4JZIZHQ/Kolluru et al. - 2021 - Learning With Fewer Images via Image Clustering A.pdf}
}

@article{kourtziCorticalRegionsInvolved2000,
  title = {Cortical {{Regions Involved}} in {{Perceiving Object Shape}}},
  author = {Kourtzi, Zoe and Kanwisher, Nancy},
  year = {2000},
  month = may,
  journal = {The Journal of Neuroscience},
  volume = {20},
  number = {9},
  pages = {3310--3318},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.20-09-03310.2000},
  urldate = {2025-01-25},
  abstract = {The studies described here use functional magnetic resonance imaging to test whether common or distinct cognitive and/or neural mechanisms are involved in extracting object structure from the different image cues defining an object's shape, such as contours, shading, and monocular depth cues. We found overlapping activations in the lateral and ventral occipital cortex [known as the lateral occipital complex (LOC)] for objects defined by different visual cues (e.g., grayscale photographs and line drawings) when each was compared with its own scrambled-object control. In a second experiment we found a reduced response when objects were repeated, independent of whether they appeared in the same or a different format (i.e., grayscale images vs line drawings). A third experiment showed that activation in the LOC was no stronger for three-dimensional shapes defined by contours or monocular depth cues, such as occlusion, than for two-dimensional shapes, suggesting that these regions are not selectively involved in processing three-dimensional shape information. These results suggest that common regions in the LOC are involved in extracting and/or representing information about object structure from different image cues.},
  copyright = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
  langid = {english},
  file = {/Users/matt/Zotero/storage/FA4TIHVA/Kourtzi and Kanwisher - 2000 - Cortical Regions Involved in Perceiving Object Sha.pdf}
}

@misc{kurakinAdversarialMachineLearning2017,
  title = {Adversarial {{Machine Learning}} at {{Scale}}},
  author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  year = {2017},
  month = feb,
  number = {arXiv:1611.01236},
  eprint = {1611.01236},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1611.01236},
  urldate = {2025-02-28},
  abstract = {Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a "label leaking" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/matt/Zotero/storage/RXH968FT/Kurakin et al. - 2017 - Adversarial Machine Learning at Scale.pdf;/Users/matt/Zotero/storage/MYQUU5C4/1611.html}
}

@article{liDivergenceAgnosticUnsupervisedDomain2022,
  title = {Divergence-{{Agnostic Unsupervised Domain Adaptation}} by {{Adversarial Attacks}}},
  author = {Li, Jingjing and Du, Zhekai and Zhu, Lei and Ding, Zhengming and Lu, Ke and Shen, Heng Tao},
  year = {2022},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {11},
  pages = {8196--8211},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3109287},
  urldate = {2025-02-28},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@misc{lipplWhenDoesCompositional2024,
  title = {When Does Compositional Structure Yield Compositional Generalization? {{A}} Kernel Theory},
  shorttitle = {When Does Compositional Structure Yield Compositional Generalization?},
  author = {Lippl, Samuel and Stachenfeld, Kim},
  year = {2024},
  month = may,
  number = {arXiv:2405.16391},
  eprint = {2405.16391},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-06-12},
  abstract = {Compositional generalization (the ability to respond correctly to novel combinations of familiar components) is thought to be a cornerstone of intelligent behavior. Compositionally structured (e.g. disentangled) representations are essential for this; however, the conditions under which they yield compositional generalization remain unclear. To address this gap, we present a general theory of compositional generalization in kernel models with fixed, potentially nonlinear representations (which also applies to neural networks in the ``lazy regime''). We prove that these models are functionally limited to adding up values assigned to conjunctions/combinations of components that have been seen during training (``conjunction-wise additivity''), and identify novel compositionality failure modes that arise from the data and model structure, even for disentangled inputs. For models in the representation learning (or ``rich'') regime, we show that networks can generalize on an important non-additive task (associative inference), and give a mechanistic explanation for why. Finally, we validate our theory empirically, showing that it captures the behavior of deep neural networks trained on a set of compositional tasks. In sum, our theory characterizes the principles giving rise to compositional generalization in kernel models and shows how representation learning can overcome their limitations. We further provide a formally grounded, novel generalization class for compositional tasks that highlights fundamental differences in the required learning mechanisms (conjunction-wise additivity).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/Users/matt/Zotero/storage/WBLG3HP7/Lippl and Stachenfeld - 2024 - When does compositional structure yield compositio.pdf}
}

@misc{madryDeepLearningModels2019,
  title = {Towards {{Deep Learning Models Resistant}} to {{Adversarial Attacks}}},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  year = {2019},
  month = sep,
  number = {arXiv:1706.06083},
  eprint = {1706.06083},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.06083},
  urldate = {2025-02-28},
  abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/matt/Zotero/storage/R3MUW6DL/Madry et al. - 2019 - Towards Deep Learning Models Resistant to Adversar.pdf;/Users/matt/Zotero/storage/G4B8FYFH/1706.html}
}

@inproceedings{maheshwariImageClusteringUsing2009,
  title = {Image {{Clustering Using Color}} and {{Texture}}},
  booktitle = {2009 {{First International Conference}} on {{Computational Intelligence}}, {{Communication Systems}} and {{Networks}}},
  author = {Maheshwari, Manish and Silakari, Sanjay and Motwani, Mahesh},
  year = {2009},
  month = jul,
  pages = {403--408},
  publisher = {IEEE},
  address = {Indore, India},
  doi = {10.1109/CICSYN.2009.69},
  urldate = {2024-11-26},
  isbn = {978-0-7695-3743-6}
}

@misc{mcinnesUMAPUniformManifold2018,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1802.03426},
  urldate = {2025-02-10},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computational Geometry (cs.CG),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@misc{mildenbergerKamitaniLabBrain_diffuser,
  title = {{{KamitaniLab}}/Brain\_diffuser},
  author = {Mildenberger, Matthias},
  journal = {GitHub},
  urldate = {2025-01-22},
  howpublished = {https://github.com/KamitaniLab/brain\_diffuser},
  langid = {english},
  file = {/Users/matt/Zotero/storage/Y4NG7IGB/brain_diffuser.html}
}

@article{miyawakiVisualImageReconstruction2008,
  title = {Visual {{Image Reconstruction}} from {{Human Brain Activity}} Using a {{Combination}} of {{Multiscale Local Image Decoders}}},
  author = {Miyawaki, Yoichi and Uchida, Hajime and Yamashita, Okito and Sato, Masa-aki and Morito, Yusuke and Tanabe, Hiroki C. and Sadato, Norihiro and Kamitani, Yukiyasu},
  year = {2008},
  month = dec,
  journal = {Neuron},
  volume = {60},
  number = {5},
  pages = {915--929},
  issn = {08966273},
  doi = {10.1016/j.neuron.2008.11.004},
  urldate = {2024-05-16},
  abstract = {Perceptual experience consists of an enormous number of possible states. Previous fMRI studies have predicted a perceptual state by classifying brain activity into prespecified categories. Constraint-free visual image reconstruction is more challenging, as it is impractical to specify brain activity for all possible images. In this study, we reconstructed visual images by combining local image bases of multiple scales, whose contrasts were independently decoded from fMRI activity by automatically selecting relevant voxels and exploiting their correlated patterns. Binarycontrast, 10 3 10-patch images (2100 possible states) were accurately reconstructed without any image prior on a single trial or volume basis by measuring brain activity only for several hundred random images. Reconstruction was also used to identify the presented image among millions of candidates. The results suggest that our approach provides an effective means to read out complex perceptual states from brain activity while discovering information representation in multivoxel patterns.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/G28LT8YB/Miyawaki et al. - 2008 - Visual Image Reconstruction from Human Brain Activ.pdf}
}

@misc{ModelZoo,
  title = {Model {{Zoo}}},
  journal = {GitHub},
  urldate = {2025-01-23},
  abstract = {Caffe: a fast open framework for deep learning. Contribute to BVLC/caffe development by creating an account on GitHub.},
  howpublished = {https://github.com/BVLC/caffe/wiki/Model-Zoo},
  langid = {english},
  file = {/Users/matt/Zotero/storage/CC3AYAS5/Model-Zoo.html}
}

@inproceedings{moosavi-dezfooliUniversalAdversarialPerturbations2017,
  title = {Universal {{Adversarial Perturbations}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {{Moosavi-Dezfooli}, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
  year = {2017},
  month = jul,
  pages = {86--94},
  publisher = {IEEE},
  address = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.17},
  urldate = {2024-10-21},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/Users/matt/Zotero/storage/KGNP6RRK/Moosavi-Dezfooli et al. - 2017 - Universal Adversarial Perturbations.pdf}
}

@misc{naseerIntriguingPropertiesVision2021,
  title = {Intriguing {{Properties}} of {{Vision Transformers}}},
  author = {Naseer, Muzammal and Ranasinghe, Kanchana and Khan, Salman and Hayat, Munawar and Khan, Fahad Shahbaz and Yang, Ming-Hsuan},
  year = {2021},
  month = nov,
  number = {arXiv:2105.10497},
  eprint = {2105.10497},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.10497},
  urldate = {2025-02-28},
  abstract = {Vision transformers (ViT) have demonstrated impressive performance across various machine vision problems. These models are based on multi-head self-attention mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility in attending image-wide context conditioned on a given patch can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a) Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60\% top-1 accuracy on ImageNet even after randomly occluding 80\% of the image content. (b) The robust performance to occlusions is not due to a bias towards local textures, and ViTs are significantly less biased towards textures compared to CNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c) Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d) Off-the-shelf features from a single ViT model can be combined to create a feature ensemble, leading to high accuracy rates across a range of classification datasets in both traditional and few-shot learning paradigms. We show effective features of ViTs are due to flexible and dynamic receptive fields possible via the self-attention mechanism.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/M3H45YP7/Naseer et al. - 2021 - Intriguing Properties of Vision Transformers.pdf;/Users/matt/Zotero/storage/QXUFB87S/2105.html}
}

@misc{OpenAI_ChatGPT_2024,
  title = {{{ChatGPT}} ({{GPT-4o}})},
  author = {{OpenAI}},
  year = {2024}
}

@misc{ozcelikNaturalSceneReconstruction2023,
  title = {Natural Scene Reconstruction from {{fMRI}} Signals Using Generative Latent Diffusion},
  author = {Ozcelik, Furkan and VanRullen, Rufin},
  year = {2023},
  month = jun,
  number = {arXiv:2303.05334},
  eprint = {2303.05334},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-06-12},
  abstract = {In neural decoding research, one of the most intriguing topics is the reconstruction of perceived natural images based on fMRI signals. Previous studies have succeeded in re-creating different aspects of the visuals, such as low-level properties (shape, texture, layout) or high-level features (category of objects, descriptive semantics of scenes) but have typically failed to reconstruct these properties together for complex scene images. Generative AI has recently made a leap forward with latent diffusion models capable of generating high-complexity images. Here, we investigate how to take advantage of this innovative technology for brain decoding. We present a two-stage scene reconstruction framework called ``Brain-Diffuser''. In the first stage, starting from fMRI signals, we reconstruct images that capture low-level properties and overall layout using a VDVAE (Very Deep Variational Autoencoder) model. In the second stage, we use the image-to-image framework of a latent diffusion model (Versatile Diffusion) conditioned on predicted multimodal (text and visual) features, to generate final reconstructed images. On the publicly available Natural Scenes Dataset benchmark, our method outperforms previous models both qualitatively and quantitatively. When applied to synthetic fMRI patterns generated from individual ROI (region-of-interest) masks, our trained model creates compelling ``ROI-optimal'' scenes consistent with neuroscientific knowledge. Thus, the proposed methodology can have an impact on both applied (e.g. brain-computer interface) and fundamental neuroscience.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Neurons and Cognition},
  file = {/Users/matt/Zotero/storage/CA7ZXH5T/Ozcelik and VanRullen - 2023 - Natural scene reconstruction from fMRI signals usi.pdf}
}

@misc{ozcelikOzcelikfuBraindiffuser2025,
  title = {Ozcelikfu/Brain-Diffuser},
  author = {Ozcelik, Furkan},
  year = {2025},
  month = jan,
  urldate = {2025-01-22},
  abstract = {Official repository for the paper "Brain-Diffuser: Natural scene reconstruction from fMRI signals using generative latent diffusion" by Furkan Ozcelik and Rufin VanRullen.},
  copyright = {MIT}
}

@misc{papernotPracticalBlackBoxAttacks2017,
  title = {Practical {{Black-Box Attacks}} against {{Machine Learning}}},
  author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
  year = {2017},
  month = mar,
  number = {arXiv:1602.02697},
  eprint = {1602.02697},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1602.02697},
  urldate = {2025-02-28},
  abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24\% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19\% and 88.94\%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/KXDQLMS8/Papernot et al. - 2017 - Practical Black-Box Attacks against Machine Learni.pdf;/Users/matt/Zotero/storage/QBV547UX/1602.html}
}

@inproceedings{poursaeedGenerativeAdversarialPerturbations2018,
  title = {Generative {{Adversarial Perturbations}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Poursaeed, Omid and Katsman, Isay and Gao, Bicheng and Belongie, Serge},
  year = {2018},
  month = jun,
  pages = {4422--4431},
  publisher = {IEEE},
  address = {Salt Lake City, UT},
  doi = {10.1109/CVPR.2018.00465},
  urldate = {2024-10-21},
  abstract = {In this paper, we propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models. We present trainable deep neural networks for transforming images to adversarial perturbations. Our proposed models can produce image-agnostic and image-dependent perturbations for targeted and nontargeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling both classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. Using extensive experiments on challenging high-resolution datasets such as ImageNet and Cityscapes, we show that our perturbations achieve high fooling rates with small perturbation norms. Moreover, our attacks are considerably faster than current iterative methods at inference time.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {/Users/matt/Zotero/storage/W8EV6KUJ/Poursaeed et al. - 2018 - Generative Adversarial Perturbations.pdf}
}

@article{radfordLearningTransferableVisual,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  abstract = {SOTA computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study performance on over 30 different computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers nontrivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/BTPJMQXI/Radford et al. - Learning Transferable Visual Models From Natural L.pdf}
}

@misc{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.00020},
  urldate = {2025-01-22},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/UL9GKCHF/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf;/Users/matt/Zotero/storage/AFDS68XK/2103.html}
}

@article{rensinkVisualSensingSeeing2004,
  title = {Visual {{Sensing Without Seeing}}},
  author = {Rensink, Ronald A.},
  year = {2004},
  month = jan,
  journal = {Psychological Science},
  volume = {15},
  number = {1},
  pages = {27--32},
  issn = {0956-7976, 1467-9280},
  doi = {10.1111/j.0963-7214.2004.01501005.x},
  urldate = {2025-03-03},
  abstract = {It has often been assumed that when we use vision to become aware of an object or event in our surroundings, this must be accompanied by a corresponding visual experience (i.e., seeing). The studies reported here show that this assumption is incorrect. When observers view a sequence of displays alternating between an image of a scene and the same image changed in some way, they often feel (or sense) the change even though they have no visual experience of it. The subjective difference between sensing and seeing is mirrored in several behavioral differences, suggesting that these are two distinct modes of conscious visual perception.},
  copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
  langid = {english},
  file = {/Users/matt/Zotero/storage/MDZIFSSP/Rensink - 2004 - Visual Sensing Without Seeing.pdf}
}

@article{saraImageQualityAssessment2019,
  title = {Image {{Quality Assessment}} through {{FSIM}}, {{SSIM}}, {{MSE}} and {{PSNR}}---{{A Comparative Study}}},
  author = {Sara, Umme and Akter, Morium and Uddin, Mohammad Shorif},
  year = {2019},
  journal = {Journal of Computer and Communications},
  volume = {07},
  number = {03},
  pages = {8--18},
  issn = {2327-5219, 2327-5227},
  doi = {10.4236/jcc.2019.73002},
  urldate = {2024-10-25},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  file = {/Users/matt/Zotero/storage/INN6JNJ8/Sara et al. - 2019 - Image Quality Assessment through FSIM, SSIM, MSE a.pdf}
}

@misc{schuhmannLAION400MOpenDataset2021,
  title = {{{LAION-400M}}: {{Open Dataset}} of {{CLIP-Filtered}} 400 {{Million Image-Text Pairs}}},
  shorttitle = {{{LAION-400M}}},
  author = {Schuhmann, Christoph and Vencu, Richard and Beaumont, Romain and Kaczmarczyk, Robert and Mullis, Clayton and Katta, Aarush and Coombes, Theo and Jitsev, Jenia and Komatsuzaki, Aran},
  year = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2111.02114},
  urldate = {2025-01-22},
  abstract = {Multi-modal language-vision models trained on hundreds of millions of image-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable capability to perform zero- or few-shot learning and transfer even in absence of per-sample labels on target image data. Despite this trend, to date there has been no publicly available datasets of sufficient scale for training such models from scratch. To address this issue, in a community effort we build and release for public LAION-400M, a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices that allow efficient similarity search.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computation and Language (cs.CL),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@misc{senerActiveLearningConvolutional2018,
  title = {Active {{Learning}} for {{Convolutional Neural Networks}}: {{A Core-Set Approach}}},
  shorttitle = {Active {{Learning}} for {{Convolutional Neural Networks}}},
  author = {Sener, Ozan and Savarese, Silvio},
  year = {2018},
  month = jun,
  number = {arXiv:1708.00489},
  eprint = {1708.00489},
  primaryclass = {stat},
  publisher = {arXiv},
  urldate = {2024-11-15},
  abstract = {Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (ie. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, ie. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/matt/Zotero/storage/W6WLE8HI/Sener and Savarese - 2018 - Active Learning for Convolutional Neural Networks.pdf;/Users/matt/Zotero/storage/Q5LRKUGC/1708.html}
}

@article{serenoBordersMultipleVisual1995,
  title = {Borders of {{Multiple Visual Areas}} in {{Humans Revealed}} by {{Functional Magnetic Resonance Imaging}}},
  author = {Sereno, M. I. and Dale, A. M. and Reppas, J. B. and Kwong, K. K. and Belliveau, J. W. and Brady, T. J. and Rosen, B. R. and Tootell, R. B. H.},
  year = {1995},
  month = may,
  journal = {Science},
  volume = {268},
  number = {5212},
  pages = {889--893},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.7754376},
  urldate = {2025-01-25},
  abstract = {The borders of human visual areas V1, V2, VP, V3, and V4 were precisely and noninvasively determined. Functional magnetic resonance images were recorded during phase-encoded retinal stimulation. This volume data set was then sampled with a cortical surface reconstruction, making it possible to calculate the local visual field sign (mirror image versus non-mirror image representation). This method automatically and objectively outlines area borders because adjacent areas often have the opposite field sign. Cortical magnification factor curves for striate and extrastriate cortical areas were determined, which showed that human visual areas have a greater emphasis on the center-of-gaze than their counterparts in monkeys. Retinotopically organized visual areas in humans extend anteriorly to overlap several areas previously shown to be activated by written words.},
  langid = {english}
}

@article{shenDeepImageReconstruction2019,
  title = {Deep Image Reconstruction from Human Brain Activity},
  author = {Shen, Guohua and Horikawa, Tomoyasu and Majima, Kei and Kamitani, Yukiyasu},
  editor = {O'Reilly, Jill},
  year = {2019},
  month = jan,
  journal = {PLOS Computational Biology},
  volume = {15},
  number = {1},
  pages = {e1006633},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006633},
  urldate = {2024-05-15},
  abstract = {The mental contents of perception and imagery are thought to be encoded in hierarchical representations in the brain, but previous attempts to visualize perceptual contents have failed to capitalize on multiple levels of the hierarchy, leaving it challenging to reconstruct internal imagery. Recent work showed that visual cortical activity measured by functional magnetic resonance imaging (fMRI) can be decoded (translated) into the hierarchical features of a pre-trained deep neural network (DNN) for the same input image, providing a way to make use of the information from hierarchical visual features. Here, we present a novel image reconstruction method, in which the pixel values of an image are optimized to make its DNN features similar to those decoded from human brain activity at multiple layers. We found that our method was able to reliably produce reconstructions that resembled the viewed natural images. A natural image prior introduced by a deep generator neural network effectively rendered semantically meaningful details to the reconstructions. Human judgment of the reconstructions supported the effectiveness of combining multiple DNN layers to enhance the visual quality of generated images. While our model was solely trained with natural images, it successfully generalized to artificial shapes, indicating that our model was not simply matching to exemplars. The same analysis applied to mental imagery demonstrated rudimentary reconstructions of the subjective content. Our results suggest that our method can effectively combine hierarchical neural representations to reconstruct perceptual and subjective images, providing a new window into the internal contents of the brain.},
  langid = {english},
  file = {/Users/matt/Zotero/storage/GLQ76TPU/Shen et al. - 2019 - Deep image reconstruction from human brain activit.pdf}
}

@article{shirakawaCriticalAssessmentGenerative2023,
  title = {Critical Assessment of Generative {{AI}} Methods and Natural Image Datasets for Visual Image Reconstruction from Brain Activity},
  author = {Shirakawa, Ken and Majima, Kei and Kamitani, Yukiyasu and Tanaka, Misato and Aoki, Shuntaro},
  year = {2023},
  publisher = {OSF},
  doi = {10.17605/OSF.IO/NMFC5},
  urldate = {2024-06-12},
  abstract = {Ongoing progress reports detailing the evaluation and characterization of text-to-image diffusion methods and natural image datasets used in recent image reconstruction studies},
  collaborator = {{Center For Open Science}},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {brain decoding,Deep neural network,generative AI,neuroscience,reconstruction}
}

@misc{shirakawaSpuriousReconstructionBrain2024,
  title = {Spurious Reconstruction from Brain Activity},
  author = {Shirakawa, Ken and Nagano, Yoshihiro and Tanaka, Misato and Aoki, Shuntaro C. and Majima, Kei and Muraki, Yusuke and Kamitani, Yukiyasu},
  year = {2024},
  month = may,
  number = {arXiv:2405.10078},
  eprint = {2405.10078},
  primaryclass = {q-bio},
  publisher = {arXiv},
  urldate = {2024-06-05},
  abstract = {The rapid advances in brain decoding, particularly visual image reconstruction, have sparked discussions about the potential societal implications and ethical considerations surrounding neurotechnology. As these methods aim to recover perceived images from brain activity and achieve prediction over diverse images beyond training samples (zero-shot prediction), it is crucial to critically assess their capabilities and limitations to prevent misguided public expectations and inform future regulations. Our case study of recent text-guided reconstruction methods, which leverage a large-scale dataset (the Natural Scene Dataset, NSD) and text-to-image diffusion models, reveals significant limitations in their generalizability. We found a notable decrease in performance when applying these methods to a different dataset, which was designed to prevent category overlaps between training and test sets. UMAP visualization of the text features with NSD images showed a limited diversity of distinct semantic and visual clusters, with substantial overlap between training and test sets. Formal analysis and simulations demonstrated that clustered training samples can lead to ``output dimension collapse,'' restricting the output feature dimensions predictable from brain activity. Diversifying the training set to ensure a broader feature distribution improved generalizability beyond the trained clusters. However, text features alone are insufficient for a complete mapping to the visual space, even if perfectly predicted from brain activity. We argue that recent photo-like reconstructions may primarily be a blend of classification into trained categories and the generation of convincing yet inauthentic images through text-to-image diffusion (hallucination). To achieve genuine zero-shot prediction, diverse datasets and compositional representations spanning the image space are essential. As neurotechnology advances, engaging in interdisciplinary discussions involving neuroscientists, ethicists, policymakers, and the public is crucial to ensure responsible development and application of these techniques. These discussions should be grounded in a clear understanding of the current capabilities and limitations of the technology, as well as a careful consideration of the potential ethical and societal impacts.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {/Users/matt/Zotero/storage/BP33X6SV/Shirakawa et al. - 2024 - Spurious reconstruction from brain activity.10078}
}

@misc{simonyanVeryDeepConvolutional2014,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2014},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1409.1556},
  urldate = {2025-01-23},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@misc{sundaramWhenDoesPerceptual2024,
  title = {When {{Does Perceptual Alignment Benefit Vision Representations}}?},
  author = {Sundaram, Shobhita and Fu, Stephanie and Muttenthaler, Lukas and Tamir, Netanel Y. and Chai, Lucy and Kornblith, Simon and Darrell, Trevor and Isola, Phillip},
  year = {2024},
  month = oct,
  number = {arXiv:2410.10817},
  eprint = {2410.10817},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.10817},
  urldate = {2025-01-26},
  abstract = {Humans judge perceptual similarity according to diverse visual attributes, including scene layout, subject location, and camera pose. Existing vision models understand a wide range of semantic abstractions but improperly weigh these attributes and thus make inferences misaligned with human perception. While vision representations have previously benefited from alignment in contexts like image generation, the utility of perceptually aligned representations in more general-purpose settings remains unclear. Here, we investigate how aligning vision model representations to human perceptual judgments impacts their usability across diverse computer vision tasks. We finetune state-of-the-art models on human similarity judgments for image triplets and evaluate them across standard vision benchmarks. We find that aligning models to perceptual judgments yields representations that improve upon the original backbones across many downstream tasks, including counting, segmentation, depth estimation, instance retrieval, and retrieval-augmented generation. In addition, we find that performance is widely preserved on other tasks, including specialized out-of-distribution domains such as in medical imaging and 3D environment frames. Our results suggest that injecting an inductive bias about human perceptual knowledge into vision models can contribute to better representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/matt/Zotero/storage/MDRY67XY/Sundaram et al. - 2024 - When Does Perceptual Alignment Benefit Vision Repr.pdf;/Users/matt/Zotero/storage/MGJST7G6/2410.html}
}

@article{takagiHighResolutionImageReconstruction,
  title = {High-{{Resolution Image Reconstruction With Latent Diffusion Models From Human Brain Activity}}},
  author = {Takagi, Yu and Nishimoto, Shinji},
  langid = {english},
  file = {/Users/matt/Zotero/storage/GQQKPUKB/Takagi and Nishimoto - High-Resolution Image Reconstruction With Latent D.pdf}
}

@misc{volpiAdversarialFeatureAugmentation2018,
  title = {Adversarial {{Feature Augmentation}} for {{Unsupervised Domain Adaptation}}},
  author = {Volpi, Riccardo and Morerio, Pietro and Savarese, Silvio and Murino, Vittorio},
  year = {2018},
  month = may,
  number = {arXiv:1711.08561},
  eprint = {1711.08561},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.08561},
  urldate = {2025-02-28},
  abstract = {Recent works showed that Generative Adversarial Networks (GANs) can be successfully applied in unsupervised domain adaptation, where, given a labeled source dataset and an unlabeled target dataset, the goal is to train powerful classifiers for the target samples. In particular, it was shown that a GAN objective function can be used to learn target features indistinguishable from the source ones. In this work, we extend this framework by (i) forcing the learned feature extractor to be domain-invariant, and (ii) training it through data augmentation in the feature space, namely performing feature augmentation. While data augmentation in the image space is a well established technique in deep learning, feature augmentation has not yet received the same level of attention. We accomplish it by means of a feature generator trained by playing the GAN minimax game against source features. Results show that both enforcing domain-invariance and performing feature augmentation lead to superior or comparable performance to state-of-the-art results in several unsupervised domain adaptation benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/ZCU3CEGN/Volpi et al. - 2018 - Adversarial Feature Augmentation for Unsupervised .pdf;/Users/matt/Zotero/storage/8YPPE5S9/1711.html}
}

@inproceedings{wangDataDropoutOptimizing2018,
  title = {Data {{Dropout}}: {{Optimizing Training Data}} for {{Convolutional Neural Networks}}},
  shorttitle = {Data {{Dropout}}},
  booktitle = {2018 {{IEEE}} 30th {{International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}})},
  author = {Wang, Tianyang and Huan, Jun and Li, Bo},
  year = {2018},
  month = nov,
  pages = {39--46},
  publisher = {IEEE},
  address = {Volos},
  doi = {10.1109/ICTAI.2018.00017},
  urldate = {2024-11-15},
  isbn = {978-1-5386-7449-9},
  file = {/Users/matt/Zotero/storage/P4FI5G93/Wang et al. - 2018 - Data Dropout Optimizing Training Data for Convolu.pdf}
}

@article{wangLessBetterUnweighted2020,
  title = {Less {{Is Better}}: {{Unweighted Data Subsampling}} via {{Influence Function}}},
  shorttitle = {Less {{Is Better}}},
  author = {Wang, Zifeng and Zhu, Hong and Dong, Zhenhua and He, Xiuqiang and Huang, Shao-Lun},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {6340--6347},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i04.6103},
  urldate = {2024-11-15},
  abstract = {In the time of Big Data, training complex models on large-scale data sets is challenging, making it appealing to reduce data volume for saving computation resources by subsampling. Most previous works in subsampling are weighted methods designed to help the performance of subset-model approach the full-set-model, hence the weighted methods have no chance to acquire a subset-model that is better than the full-set-model. However, we question that how can we achieve better model with less data? In this work, we propose a novel Unweighted Influence Data Subsampling (UIDS) method, and prove that the subset-model acquired through our method can outperform the full-set-model. Besides, we show that overly confident on a given test set for sampling is common in Influence-based subsampling methods, which can eventually cause our subset-model's failure in out-of-sample test. To mitigate it, we develop a probabilistic sampling scheme to control the worst-case risk over all distributions close to the empirical distribution. The experiment results demonstrate our methods superiority over existed subsampling methods in diverse tasks, such as text classification, image classification, click-through prediction, etc.},
  copyright = {https://www.aaai.org},
  file = {/Users/matt/Zotero/storage/CVDA772J/Wang et al. - 2020 - Less Is Better Unweighted Data Subsampling via In.pdf}
}

@article{wolfeFiveFactorsThat2017,
  title = {Five Factors That Guide Attention in Visual Search},
  author = {Wolfe, Jeremy M. and Horowitz, Todd S.},
  year = {2017},
  month = mar,
  journal = {Nature Human Behaviour},
  volume = {1},
  number = {3},
  pages = {0058},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0058},
  urldate = {2025-03-03},
  langid = {english},
  file = {/Users/matt/Zotero/storage/KILYZH33/Wolfe and Horowitz - 2017 - Five factors that guide attention in visual search.pdf}
}

@misc{xieAdversarialExamplesImprove2020,
  title = {Adversarial {{Examples Improve Image Recognition}}},
  author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan and Le, Quoc V.},
  year = {2020},
  month = apr,
  number = {arXiv:1911.09665},
  eprint = {1911.09665},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.09665},
  urldate = {2025-02-28},
  abstract = {Adversarial examples are commonly viewed as a threat to ConvNets. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose AdvProp, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. We show that AdvProp improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying AdvProp to the latest EfficientNet-B7 [28] on ImageNet, we achieve significant improvements on ImageNet (+0.7\%), ImageNet-C (+6.5\%), ImageNet-A (+7.0\%), Stylized-ImageNet (+4.8\%). With an enhanced EfficientNet-B8, our method achieves the state-of-the-art 85.5\% ImageNet top-1 accuracy without extra data. This result even surpasses the best model in [20] which is trained with 3.5B Instagram images ({\textasciitilde}3000X more than ImageNet) and {\textasciitilde}9.4X more parameters. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/F5UPTHUH/Xie et al. - 2020 - Adversarial Examples Improve Image Recognition.pdf;/Users/matt/Zotero/storage/NISUKJY3/1911.html}
}

@misc{xuVersatileDiffusionText2024,
  title = {Versatile {{Diffusion}}: {{Text}}, {{Images}} and {{Variations All}} in {{One Diffusion Model}}},
  shorttitle = {Versatile {{Diffusion}}},
  author = {Xu, Xingqian and Wang, Zhangyang and Zhang, Eric and Wang, Kai and Shi, Humphrey},
  year = {2024},
  month = jan,
  number = {arXiv:2211.08332},
  eprint = {2211.08332},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.08332},
  urldate = {2025-03-03},
  abstract = {Recent advances in diffusion models have set an impressive milestone in many generation tasks, and trending works such as DALL-E2, Imagen, and Stable Diffusion have attracted great interest. Despite the rapid landscape changes, recent new approaches focus on extensions and performance rather than capacity, thus requiring separate models for separate tasks. In this work, we expand the existing single-flow diffusion pipeline into a multi-task multimodal network, dubbed Versatile Diffusion (VD), that handles multiple flows of text-to-image, image-to-text, and variations in one unified model. The pipeline design of VD instantiates a unified multi-flow diffusion framework, consisting of sharable and swappable layer modules that enable the crossmodal generality beyond images and text. Through extensive experiments, we demonstrate that VD successfully achieves the following: a) VD outperforms the baseline approaches and handles all its base tasks with competitive quality; b) VD enables novel extensions such as disentanglement of style and semantics, dual- and multi-context blending, etc.; c) The success of our multi-flow multimodal framework over images and text may inspire further diffusion-based universal AI research. Our code and models are open-sourced at https://github.com/SHI-Labs/Versatile-Diffusion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/8QHEYZ8P/Xu et al. - 2024 - Versatile Diffusion Text, Images and Variations A.pdf;/Users/matt/Zotero/storage/8A57VHKY/2211.html}
}

@inproceedings{yanEnhancingClassificationPerformance2023,
  title = {Enhancing {{Classification Performance}} in {{Knee Magnetic Resonance Imaging Using Adversarial Data Augmentation}}},
  booktitle = {2023 {{IEEE}} 14th {{International Conference}} on {{Software Engineering}} and {{Service Science}} ({{ICSESS}})},
  author = {Yan, Zhongbo and Yang, Xu and Chong, Chak Fong and Wang, Yapeng},
  year = {2023},
  month = oct,
  pages = {19--24},
  publisher = {IEEE},
  address = {Beijing, China},
  doi = {10.1109/ICSESS58500.2023.10293076},
  urldate = {2025-02-28},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350336269 9798350336276},
  file = {/Users/matt/Zotero/storage/PF4R9HK2/Yan et al. - 2023 - Enhancing Classification Performance in Knee Magne.pdf}
}

@article{yuDatasetDistillationComprehensive2024,
  title = {Dataset {{Distillation}}: {{A Comprehensive Review}}},
  shorttitle = {Dataset {{Distillation}}},
  author = {Yu, Ruonan and Liu, Songhua and Wang, Xinchao},
  year = {2024},
  month = jan,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {46},
  number = {1},
  pages = {150--170},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2023.3323376},
  urldate = {2024-11-18},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {/Users/matt/Zotero/storage/JJ9K4VGG/Yu et al. - 2024 - Dataset Distillation A Comprehensive Review.pdf}
}

@misc{zhangLongCLIPUnlockingLongText2024,
  title = {Long-{{CLIP}}: {{Unlocking}} the {{Long-Text Capability}} of {{CLIP}}},
  shorttitle = {Long-{{CLIP}}},
  author = {Zhang, Beichen and Zhang, Pan and Dong, Xiaoyi and Zang, Yuhang and Wang, Jiaqi},
  year = {2024},
  month = jul,
  number = {arXiv:2403.15378},
  eprint = {2403.15378},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.15378},
  urldate = {2024-12-18},
  abstract = {Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for zero-shot classification, text-image retrieval, and text-image generation by aligning image and text modalities. Despite its widespread adoption, a significant limitation of CLIP lies in the inadequate length of text input. The length of the text token is restricted to 77, and an empirical study shows the actual effective length is even less than 20. This prevents CLIP from handling detailed descriptions, limiting its applications for image retrieval and text-to-image generation with extensive prerequisites. To this end, we propose Long-CLIP as a plug-and-play alternative to CLIP that supports long-text input, retains or even surpasses its zero-shot generalizability, and aligns the CLIP latent space, making it readily replace CLIP without any further adaptation in downstream frameworks. Nevertheless, achieving this goal is far from straightforward, as simplistic fine-tuning can result in a significant degradation of CLIP's performance. Moreover, substituting the text encoder with a language model supporting longer contexts necessitates pretraining with vast amounts of data, incurring significant expenses. Accordingly, Long-CLIP introduces an efficient fine-tuning solution on CLIP with two novel strategies designed to maintain the original capabilities, including (1) a knowledge-preserved stretching of positional embedding and (2) a primary component matching of CLIP features. With leveraging just one million extra long text-image pairs, Long-CLIP has shown the superiority to CLIP for about 20\% in long caption text-image retrieval and 6\% in traditional text-image retrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers enhanced capabilities for generating images from detailed text descriptions by replacing CLIP in a plug-and-play manner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/YQRP6DLR/Zhang et al. - 2024 - Long-CLIP Unlocking the Long-Text Capability of C.pdf;/Users/matt/Zotero/storage/WGF9YILB/2403.html}
}

@misc{zhaoNovelCrossPerturbationSingle2024,
  title = {A {{Novel Cross-Perturbation}} for {{Single Domain Generalization}}},
  author = {Zhao, Dongjia and Qi, Lei and Shi, Xiao and Shi, Yinghuan and Geng, Xin},
  year = {2024},
  month = jun,
  number = {arXiv:2308.00918},
  eprint = {2308.00918},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.00918},
  urldate = {2025-02-28},
  abstract = {Single domain generalization aims to enhance the ability of the model to generalize to unknown domains when trained on a single source domain. However, the limited diversity in the training data hampers the learning of domain-invariant features, resulting in compromised generalization performance. To address this, data perturbation (augmentation) has emerged as a crucial method to increase data diversity. Nevertheless, existing perturbation methods often focus on either image-level or feature-level perturbations independently, neglecting their synergistic effects. To overcome these limitations, we propose CPerb, a simple yet effective cross-perturbation method. Specifically, CPerb utilizes both horizontal and vertical operations. Horizontally, it applies image-level and feature-level perturbations to enhance the diversity of the training data, mitigating the issue of limited diversity in single-source domains. Vertically, it introduces multi-route perturbation to learn domain-invariant features from different perspectives of samples with the same semantic category, thereby enhancing the generalization capability of the model. Additionally, we propose MixPatch, a novel feature-level perturbation method that exploits local image style information to further diversify the training data. Extensive experiments on various benchmark datasets validate the effectiveness of our method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/matt/Zotero/storage/624Y9U9M/Zhao et al. - 2024 - A Novel Cross-Perturbation for Single Domain Gener.pdf;/Users/matt/Zotero/storage/W82V2UBQ/2308.html}
}
